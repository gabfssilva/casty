{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Meet Casty","text":"<p> Typed, clustered actor framework for Python </p> <p> </p> <p>Casty is a typed, clustered actor framework for Python built on asyncio. Pure Python, zero dependencies. Actors communicate exclusively through immutable messages, from a single process to a distributed cluster.</p> <ul> <li>Behaviors are values, not classes. No <code>Actor</code> base class. Behaviors are frozen dataclasses produced by factory functions, composed like any other value.</li> <li>State lives in closures. State transitions happen by returning a new behavior that closes over the new state. No mutable fields, no <code>self.balance = ...</code>.</li> <li>Immutability by default. All messages, behaviors, events, and configurations are frozen dataclasses.</li> <li>Zero external dependencies. Pure Python, stdlib only.</li> <li>Type-safe end-to-end. <code>ActorRef[M]</code>, <code>Behavior[M]</code>, and PEP 695 type aliases catch message mismatches at development time.</li> <li>Cluster-ready. Distribute actors across nodes with gossip-based membership, phi accrual failure detection, and automatic shard rebalancing.</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>@dataclass(frozen=True)\nclass Greet:\n    name: str\n\ndef greeter() -&gt; Behavior[Greet]:\n    async def receive(ctx: ActorContext[Greet], msg: Greet) -&gt; Behavior[Greet]:\n        print(f\"Hello, {msg.name}!\")\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        ref = system.spawn(greeter(), \"greeter\")\n        ref.tell(Greet(\"Alice\"))\n        await asyncio.sleep(0.1)\n\nasyncio.run(main())\n# Hello, Alice!\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> Getting Started \u2014 Installation, first actor, core concepts</li> <li> Concepts \u2014 Actors, behaviors, state, supervision, and more</li> <li> Persistence \u2014 Event sourcing for durable actor state</li> <li> Clustering \u2014 Distribute actors across multiple nodes</li> <li> Configuration \u2014 TOML-based configuration for all parameters</li> <li> API Reference \u2014 Full autodoc of all public types</li> </ul>"},{"location":"acknowledgments/","title":"Acknowledgments","text":"<p>Casty didn't emerge from a vacuum. Every feature builds on research and engineering that, without it, wouldn't be possible to implement. This page maps each feature to the ideas and papers that shaped it.</p>"},{"location":"acknowledgments/#actors-and-message-passing","title":"Actors and Message Passing","text":"<p>Actors are the primitive unit of computation in Casty: they receive messages, make local decisions, create other actors, and send messages. No shared state, no locks. This model was introduced by Hewitt, Bishop, and Steiger in 1973.</p> <p>Hewitt, C., Bishop, P., &amp; Steiger, R. (1973). A Universal Modular ACTOR Formalism for Artificial Intelligence. IJCAI.</p>"},{"location":"acknowledgments/#behaviors-as-values","title":"Behaviors as Values","text":"<p>In Casty, behaviors are frozen dataclasses produced by factory functions (<code>receive</code>, <code>setup</code>, <code>same</code>, <code>stopped</code>), not classes with a <code>receive</code> method to override. State transitions happen by returning a new behavior with new closed-over values. This functional, compositional approach to actor definition comes directly from Akka Typed.</p> <p>Akka Typed. Lightbend, Inc.</p>"},{"location":"acknowledgments/#supervision","title":"Supervision","text":"<p>When an actor fails, its supervisor decides what happens next: restart, stop, or escalate. Actors don't defend themselves with try/except around every operation. They fail fast and let the supervision hierarchy handle recovery. This \"let it crash\" philosophy was pioneered by Erlang/OTP and articulated in Armstrong's thesis on building reliable systems through isolation and message passing.</p> <p>Armstrong, J. (2003). Making reliable distributed systems in the presence of software errors. PhD thesis, Royal Institute of Technology, Stockholm.</p>"},{"location":"acknowledgments/#event-sourcing","title":"Event Sourcing","text":"<p>Event-sourced actors persist the sequence of events that produced their state, not the state itself. On recovery, events are replayed to reconstruct the actor. Snapshots periodically checkpoint the state to bound replay time. This pattern, popularized by Greg Young and described by Martin Fowler, provides a complete audit trail and the ability to rebuild state from any point in history.</p>"},{"location":"acknowledgments/#cluster-membership-via-gossip","title":"Cluster Membership via Gossip","text":"<p>Cluster membership is disseminated via gossip: each node periodically picks random peers and exchanges its view of the cluster. This epidemic approach guarantees eventual consistency without a central coordinator or consensus protocol. The protocol in Casty follows the epidemic dissemination model introduced by Demers et al.</p> <p>Demers, A., Greene, D., Hauser, C., et al. (1987). Epidemic Algorithms for Replicated Database Maintenance. ACM PODC.</p>"},{"location":"acknowledgments/#failure-detection","title":"Failure Detection","text":"<p>Casty uses the phi accrual failure detector for heartbeat monitoring. Unlike traditional binary detectors (alive or dead), this one outputs a continuous suspicion level. Each consumer chooses its own threshold, making the system adaptable to varying network conditions without retuning.</p> <p>Hayashibara, N., D\u00e9fago, X., Yared, R., &amp; Katayama, T. (2004). The Phi Accrual Failure Detector. IEEE Symposium on Reliable Distributed Systems (SRDS).</p>"},{"location":"acknowledgments/#cluster-state-convergence","title":"Cluster State Convergence","text":"<p>The cluster state that nodes gossip to each other uses CRDT merge semantics. When two nodes have divergent views, they merge deterministically: member status advances along a lattice (joining \u2192 up \u2192 leaving \u2192 down \u2192 removed), and vector clocks are merged component-wise. Convergence is guaranteed regardless of message ordering or duplication.</p> <p>Shapiro, M., Pregui\u00e7a, N., Baquero, C., &amp; Zawirski, M. (2011). Conflict-free Replicated Data Types. Symposium on Self-Stabilizing Systems (SSS).</p> <p>State versions are tracked with vector clocks to establish causal ordering across nodes. Each node increments its own component on state changes, and during gossip the clocks merge to determine which state is newer or whether a CRDT join is needed.</p> <p>Lamport, L. (1978). Time, Clocks, and the Ordering of Events in a Distributed System. Communications of the ACM.</p> <p>Fidge, C. J. (1988). Timestamps in Message-Passing Systems That Preserve the Partial Ordering. Australian Computer Science Conference.</p> <p>Mattern, F. (1989). Virtual Time and Global States of Distributed Systems. Parallel and Distributed Algorithms.</p>"},{"location":"acknowledgments/#cluster-sharding","title":"Cluster Sharding","text":"<p>Sharding distributes actors across nodes using a coordinator, region, and proxy architecture. The coordinator assigns shards to nodes using allocation strategies; regions manage the local entities for their assigned shards; proxies route messages transparently to the correct region, wherever it lives. This three-layer architecture follows Akka Cluster Sharding's design.</p> <p>Akka Cluster Sharding. Lightbend, Inc.</p>"},{"location":"clustering/cluster-backend/","title":"Casty as a Cluster Backend","text":"<p>Most Python libraries that need distributed coordination depend on external services. Celery requires Redis or RabbitMQ. Distributed lock managers need etcd or ZooKeeper. Service meshes depend on Consul. Each external dependency adds operational complexity \u2014 deployment, monitoring, version compatibility, network configuration.</p> <p>Casty provides the same cluster capabilities as an embeddable Python library with zero external dependencies. No broker to deploy, no configuration server to manage, no serialization protocol to learn. A library author can add <code>casty</code> to their dependencies and get cluster membership, leader election, failure detection, work distribution, and state replication out of the box. An application developer can add clustering to an existing system without introducing new infrastructure.</p> <p>The following sections illustrate these capabilities through the lens of building a distributed task queue \u2014 a scenario where Celery would typically require Redis and a separate worker process.</p>"},{"location":"clustering/cluster-backend/#cluster-formation","title":"Cluster Formation","text":"<p><code>ClusteredActorSystem</code> establishes a cluster through seed nodes. Once started, nodes discover each other automatically through the topology actor \u2014 a single actor that owns cluster state, runs gossip protocol and phi accrual failure detection, and pushes <code>TopologySnapshot</code> updates to all subscribers. The event stream publishes membership events as nodes join and leave:</p> <pre><code>async with ClusteredActorSystem(\n    name=\"task-queue\",\n    host=\"10.0.0.1\",\n    port=25520,\n    node_id=\"worker-1\",\n    seed_nodes=[(\"10.0.0.2\", 25520), (\"10.0.0.3\", 25520)],\n    required_quorum=3,  # startup blocks until 3 nodes are UP\n) as system:\n    def membership_logger() -&gt; Behavior[MemberUp | MemberLeft]:\n        async def receive(ctx, msg):\n            match msg:\n                case MemberUp(member=m):\n                    log.info(f\"Worker joined: {m.address}\")\n                case MemberLeft(member=m):\n                    log.info(f\"Worker left: {m.address}\")\n            return Behaviors.same()\n        return Behaviors.receive(receive)\n\n    monitor = system.spawn(membership_logger(), \"membership-monitor\")\n    system.event_stream.tell(EventStreamSubscribe(event_type=MemberUp, handler=monitor))\n    system.event_stream.tell(EventStreamSubscribe(event_type=MemberLeft, handler=monitor))\n</code></pre> <p>Every node in the cluster runs the same code. There is no distinction between \"broker\" and \"worker\" \u2014 every node is both. The cluster forms automatically as nodes start and discover each other through the seed list. The <code>required_quorum</code> parameter blocks startup inside <code>__aenter__</code> until the specified number of nodes have status <code>up</code> \u2014 no more guessing with <code>asyncio.sleep()</code>. When omitted, startup completes immediately after the local node initializes.</p>"},{"location":"clustering/cluster-backend/#work-distribution","title":"Work Distribution","text":"<p><code>Behaviors.sharded()</code> partitions work across nodes by entity ID. For a task queue, each task type or queue name can be an entity, and the cluster handles routing transparently:</p> <pre><code>def task_worker(entity_id: str) -&gt; Behavior[TaskMsg]:\n    def idle() -&gt; Behavior[TaskMsg]:\n        async def receive(ctx, msg):\n            match msg:\n                case SubmitTask(payload, reply_to):\n                    result = await execute(payload)\n                    reply_to.tell(TaskResult(result))\n                    return idle()\n        return Behaviors.receive(receive)\n    return idle()\n\ntasks = system.spawn(Behaviors.sharded(task_worker, num_shards=256), \"tasks\")\n\n# Submit from any node \u2014 routing is transparent\ntasks.tell(ShardEnvelope(\"queue:emails\", SubmitTask(send_welcome, reply_to)))\ntasks.tell(ShardEnvelope(\"queue:reports\", SubmitTask(generate_pdf, reply_to)))\n</code></pre> <p>The entity ID determines which node processes the task. Tasks with the same queue name always land on the same node, providing natural ordering guarantees. Tasks with different queue names are distributed across the cluster.</p>"},{"location":"clustering/cluster-backend/#failure-handling","title":"Failure Handling","text":"<p>When a node becomes unreachable, the phi accrual failure detector identifies it and the coordinator reallocates its shards to surviving nodes. No manual intervention, no external health check service:</p> <pre><code>def unreachable_logger() -&gt; Behavior[UnreachableMember]:\n    async def receive(ctx, msg):\n        match msg:\n            case UnreachableMember(member=m):\n                log.warning(f\"Node unreachable: {m.address}, shards will be reallocated\")\n        return Behaviors.same()\n    return Behaviors.receive(receive)\n\nalert = system.spawn(unreachable_logger(), \"unreachable-alert\")\nsystem.event_stream.tell(EventStreamSubscribe(event_type=UnreachableMember, handler=alert))\n</code></pre> <p>If the task worker uses event sourcing, reallocated entities replay their journal on the new node and resume exactly where they left off. Tasks in progress at the time of failure are recovered automatically \u2014 the new primary replays persisted events and the worker continues from its last known state.</p>"},{"location":"clustering/cluster-backend/#distributed-coordination","title":"Distributed Coordination","text":"<p><code>system.barrier(name, n)</code> blocks until <code>n</code> nodes reach the same named barrier, then releases all simultaneously:</p> <pre><code>await system.barrier(\"work-complete\", num_nodes)\n# all nodes have reached this point before any proceeds\nawait system.barrier(\"shutdown\", num_nodes)\n</code></pre> <p>Barriers, locks, and semaphores are also available through the <code>Distributed</code> facade (see Distributed Data Structures). All coordination primitives are backed by sharded entities \u2014 no external coordination service.</p>"},{"location":"clustering/cluster-backend/#state-without-external-storage","title":"State Without External Storage","text":"<p>Event sourcing combined with replication provides durable distributed state without a database. The primary persists events to its journal and pushes them to replicas on other nodes. If the primary fails, a replica is promoted with its full event history intact:</p> <pre><code>journal = InMemoryJournal()  # replace with a database-backed journal in production\n\ntasks = system.spawn(\n    Behaviors.sharded(\n        lambda eid: Behaviors.event_sourced(\n            entity_id=eid,\n            journal=journal,\n            initial_state=QueueState(pending=(), completed=()),\n            on_event=apply_event,\n            on_command=handle_command,\n        ),\n        num_shards=256,\n        replication=ReplicationConfig(replicas=1, min_acks=1),\n    ),\n    \"tasks\",\n)\n</code></pre> <p>No Redis. No RabbitMQ. No ZooKeeper. The cluster stack is the library \u2014 membership, leader election, failure detection, sharding, replication, distributed data structures, locks, semaphores, and barriers, all in pure Python, all in a single <code>pip install</code>.</p>"},{"location":"clustering/cluster-backend/#external-clients","title":"External Clients","text":"<p>Not every process needs to be a full cluster member. <code>ClusterClient</code> connects to the cluster as an external observer \u2014 it receives topology updates (membership + shard allocations) and routes <code>ShardEnvelope</code> messages directly to the owning node with zero hops, no proxy overhead, no cluster participation:</p> <pre><code>async with ClusterClient(\n    cluster_host=\"10.0.0.1\",\n    cluster_port=25520,\n    system_name=\"task-queue\",\n) as client:\n    ref = await client.shard_ref(\"tasks\", num_shards=256)\n    ref.tell(ShardEnvelope(\"queue:emails\", SubmitTask(send_welcome, reply_to)))\n</code></pre> <p>The client subscribes to <code>TopologySnapshot</code> from the cluster's topology actor, caches shard-to-node mappings locally, and includes a TCP circuit breaker that blacklists failed nodes and retries on topology changes. Ideal for web servers, API gateways, or any process that sends work to the cluster without hosting entities itself.</p> <p>Next: Configuration</p>"},{"location":"clustering/cluster-broadcast/","title":"Cluster Broadcast","text":"<p>Sharding routes messages to a single entity via its ID. But some scenarios require sending a message to all nodes \u2014 configuration updates, cache invalidation, system-wide announcements. <code>Behaviors.broadcasted()</code> wraps any behavior so that <code>tell()</code> automatically fans out to every cluster member and <code>ask()</code> collects all responses:</p> <pre><code>@dataclass(frozen=True)\nclass Announcement:\n    text: str\n    reply_to: ActorRef[Ack]\n\n@dataclass(frozen=True)\nclass Ack:\n    from_node: str\n\ndef listener() -&gt; Behavior[Announcement]:\n    node = socket.gethostname()\n\n    async def receive(_ctx, msg: Announcement) -&gt; Behavior[Announcement]:\n        msg.reply_to.tell(Ack(from_node=node))\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ClusteredActorSystem(\n        name=\"demo\", host=\"127.0.0.1\", port=25520,\n        node_id=\"node-1\",\n    ) as system:\n        # BroadcastRef \u2014 tell/ask fan out to ALL nodes\n        ref: BroadcastRef[Announcement] = system.spawn(\n            Behaviors.broadcasted(listener()), \"listener\"\n        )\n\n        # ask() returns tuple[Ack, ...] \u2014 one per node\n        acks: tuple[Ack, ...] = await system.ask(\n            ref,\n            lambda r: Announcement(text=\"Hello cluster!\", reply_to=r),\n            timeout=5.0,\n        )\n        for ack in acks:\n            print(f\"Ack from {ack.from_node}\")\n\nasyncio.run(main())\n</code></pre> <p>Under the hood, each node spawns a local copy of the actor at <code>/_bcast-{name}</code> and a proxy at <code>/{name}</code>. The proxy tracks cluster membership via gossip and fans out messages to all <code>up</code> members \u2014 locally or over TCP. The <code>BroadcastRef[M]</code> subclass of <code>ActorRef[M]</code> enables typesafe overloads: <code>ask(BroadcastRef, ...)</code> returns <code>tuple[R, ...]</code> instead of <code>R</code>.</p> <p>Next: Cluster Singleton</p>"},{"location":"clustering/cluster-client/","title":"Cluster Client","text":"<p>Not every process needs to join the cluster. A web server, an API gateway, or a CLI tool might need to talk to cluster actors without hosting any itself. ClusterClient connects to a Casty cluster as an external observer \u2014 it receives topology updates and gives you three capabilities:</p> <ul> <li>Route messages to sharded entities \u2014 zero hops, no proxy overhead.</li> <li>Discover services \u2014 <code>lookup()</code> reads from the locally cached topology, no network round-trip.</li> <li>Request-reply \u2014 <code>ask()</code> creates a temporary ref so the cluster can respond directly.</li> </ul> <p>No cluster membership, no gossip participation, no failure detection overhead.</p> <pre><code>async with ClusterClient(\n    contact_points=[(\"10.0.1.10\", 25520), (\"10.0.1.11\", 25520)],\n    system_name=\"my-cluster\",\n) as client:\n    # Sharded entities\n    counter = client.entity_ref(\"counter\", num_shards=100)\n    counter.tell(ShardEnvelope(\"user-42\", Increment(1)))\n\n    # Request-reply\n    count = await client.ask(\n        counter,\n        lambda r: ShardEnvelope(\"user-42\", GetCount(reply_to=r)),\n    )\n\n    # Service discovery\n    listing = client.lookup(PAYMENT_KEY)\n    for instance in listing.instances:\n        instance.ref.tell(ProcessPayment(amount=100))\n</code></pre>"},{"location":"clustering/cluster-client/#how-it-works","title":"How It Works","text":"<p>On startup, <code>ClusterClient</code> opens a TCP connection (with <code>client_only=True</code> \u2014 no server socket) and spawns a topology subscriber actor that sends <code>SubscribeTopology</code> to the first reachable contact point. From that point on, the cluster pushes <code>TopologySnapshot</code> updates whenever membership, shard allocations, or the service registry changes.</p> <p>Replies travel back through the same bidirectional TCP connection \u2014 the client doesn't need to be reachable from the cluster.</p>"},{"location":"clustering/cluster-client/#sharded-entities","title":"Sharded Entities","text":"<p>Each call to <code>entity_ref(shard_type, num_shards=N)</code> creates a local proxy actor for that shard type. The proxy:</p> <ol> <li>Receives <code>TopologySnapshot</code> updates from the subscriber and caches shard-to-node mappings.</li> <li>When a <code>ShardEnvelope</code> arrives, hashes the entity ID to a shard and routes directly to the owning node's region actor over TCP.</li> <li>On cache miss, queries the coordinator on the leader node via <code>GetShardLocation</code> and buffers messages until the response arrives.</li> </ol> <p>Subsequent calls with the same <code>shard_type</code> return the cached proxy \u2014 no duplicate actors.</p> <pre><code>counter = client.entity_ref(\"counter\", num_shards=100)\ncounter.tell(ShardEnvelope(\"user-42\", Increment(1)))\ncounter.tell(ShardEnvelope(\"user-99\", Increment(5)))\n</code></pre>"},{"location":"clustering/cluster-client/#service-discovery","title":"Service Discovery","text":"<p><code>ClusterClient</code> can discover services registered with <code>Behaviors.discoverable()</code>. The <code>TopologySnapshot</code> carries the full service registry, so <code>lookup()</code> is a synchronous local read:</p> <pre><code>PAYMENT_KEY: ServiceKey[PaymentMsg] = ServiceKey(\"payment\")\n\nlisting = client.lookup(PAYMENT_KEY)\nfor instance in listing.instances:\n    instance.ref.tell(ProcessPayment(amount=100))\n</code></pre> <p>If no topology has been received yet (e.g. called right after startup), <code>lookup()</code> returns an empty <code>Listing</code>. Once the first snapshot arrives, all registered services become visible.</p> <p>You can also use <code>ask()</code> through discovered services:</p> <pre><code>result = await client.ask(\n    next(iter(listing.instances)).ref,\n    lambda r: GetStatus(reply_to=r),\n)\n</code></pre> <p>See Service Discovery for how <code>Behaviors.discoverable()</code> and the receptionist work.</p>"},{"location":"clustering/cluster-client/#request-reply","title":"Request-Reply","text":"<p><code>client.ask()</code> creates a temporary remotely-addressable ref so the cluster can respond directly via the existing TCP connection:</p> <pre><code>balance = await client.ask(\n    accounts,\n    lambda r: ShardEnvelope(\"alice\", GetBalance(reply_to=r)),\n    timeout=5.0,\n)\n</code></pre> <p>The temporary ref is cleaned up after the response or timeout. Works with both sharded entities and discovered services.</p>"},{"location":"clustering/cluster-client/#fault-tolerance","title":"Fault Tolerance","text":"<p>The client includes a TCP circuit breaker. When a send to a node fails:</p> <ol> <li>The proxy evicts all shard allocations routed to that node.</li> <li>Failed shards are retried after a short delay \u2014 the proxy re-queries the coordinator for updated locations.</li> <li>If no topology update arrives within the liveness window (10 seconds by default), the subscriber reconnects to the next contact point.</li> </ol> <p>The subscriber also enriches its contact list from <code>TopologySnapshot</code> \u2014 if a seed node goes down, it can still reconnect through any other cluster member it has seen.</p>"},{"location":"clustering/cluster-client/#ssh-tunnels","title":"SSH Tunnels","text":"<p>When the cluster lives in a private network (e.g. AWS VPC), the client can reach it through SSH forward tunnels. Reverse tunnels are not needed \u2014 replies travel back through the same TCP connection the client opened.</p>"},{"location":"clustering/cluster-client/#setup","title":"Setup","text":"<p>Set up forward tunnels to each cluster node:</p> <pre><code>ssh -N \\\n  -L 25520:10.0.1.10:25520 \\\n  -L 25521:10.0.1.11:25520 \\\n  -L 25522:10.0.1.12:25520 \\\n  -i ~/.ssh/key ec2-user@bastion.example.com\n</code></pre> <p>Then configure <code>ClusterClient</code> with an <code>address_map</code> that translates cluster-internal addresses to local tunnel endpoints:</p> <pre><code>async with ClusterClient(\n    contact_points=[(\"10.0.1.10\", 25520), (\"10.0.1.11\", 25520)],\n    system_name=\"my-cluster\",\n    client_host=\"127.0.0.1\",\n    client_port=0,\n    address_map={\n        (\"10.0.1.10\", 25520): (\"127.0.0.1\", 25520),\n        (\"10.0.1.11\", 25520): (\"127.0.0.1\", 25521),\n        (\"10.0.1.12\", 25520): (\"127.0.0.1\", 25522),\n    },\n) as client:\n    counter = client.entity_ref(\"counter\", num_shards=100)\n    count = await client.ask(\n        counter,\n        lambda r: ShardEnvelope(\"user-42\", GetCount(reply_to=r)),\n    )\n</code></pre>"},{"location":"clustering/cluster-client/#how-it-works_1","title":"How it works","text":"<ul> <li><code>address_map</code> translates logical cluster addresses to local tunnel endpoints. When the client sends to <code>10.0.1.10:25520</code>, the TCP layer connects to <code>127.0.0.1:25520</code> instead \u2014 the SSH forward tunnel delivers it to the actual node.</li> <li><code>client_only=True</code> (internal default) means the client opens outbound connections only. No server socket, no inbound port needed.</li> <li>Replies arrive on the same TCP connection, so no reverse tunnel or <code>advertised_host</code>/<code>advertised_port</code> is required.</li> <li>If <code>address_map</code> doesn't contain an address, the client connects directly (identity fallback).</li> </ul>"},{"location":"clustering/cluster-client/#configuration","title":"Configuration","text":"Parameter Default Description <code>contact_points</code> (required) List of <code>(host, port)</code> for cluster nodes <code>system_name</code> (required) Must match the cluster's actor system name <code>client_host</code> <code>\"127.0.0.1\"</code> Local bind address <code>client_port</code> <code>0</code> Local bind port (<code>0</code> for auto-assignment) <code>address_map</code> <code>None</code> <code>dict[tuple[str, int], tuple[str, int]]</code> \u2014 logical-to-tunnel address mapping <code>advertised_host</code> <code>None</code> Override host in reply addresses (for advanced network setups) <code>advertised_port</code> <code>None</code> Override port in reply addresses (for advanced network setups)"},{"location":"clustering/cluster-client/#when-to-use","title":"When to Use","text":"Scenario Use Process hosts sharded entities or needs leader election <code>ClusteredActorSystem</code> Process sends work to sharded entities from outside <code>ClusterClient</code> Process needs to discover services without joining the cluster <code>ClusterClient</code> Process needs <code>Subscribe</code> for live service updates <code>ClusteredActorSystem</code> Process needs request-reply with cluster actors <code>ClusterClient</code> <p>Next: Cluster Backend</p>"},{"location":"clustering/cluster-sharding/","title":"Cluster Sharding","text":"<p>A single process has limits \u2014 memory, CPU, network connections. When a system manages millions of entities (bank accounts, user sessions, IoT sensors), no single machine can host them all. Cluster sharding solves this by partitioning entities across multiple nodes and routing messages transparently.</p> <p>Consider a network of 100,000 temperature sensors reporting readings every second. Each sensor is modeled as an actor that aggregates its readings. No single machine can host all 100,000 actors \u2014 but if they are partitioned into 256 shards distributed across 8 nodes, each node manages roughly 12,500 sensors. When a sensor reports a reading, the cluster routes the message to whichever node owns that sensor's shard. If nodes are added or removed, shards are rebalanced automatically.</p> <p>Cluster sharding has three components:</p> <ul> <li>Shards. A logical partition of the entity space. Entity IDs are hashed deterministically to a shard number (Casty uses MD5). The number of shards is fixed at creation time and should be significantly larger than the expected number of nodes \u2014 this ensures rebalancing is granular (individual shards move between nodes, not entire ranges).</li> <li>Coordinator. Decides which node owns which shard. Uses a least-shard-first allocation strategy: when a shard is accessed for the first time, it is assigned to the node currently hosting the fewest shards. The coordinator is replicated across the cluster with a leader/follower topology \u2014 the leader makes allocation decisions, followers cache allocations and forward unknown shards to the leader.</li> <li>Region. Each node runs a region that manages local entities. When a message arrives for a shard owned by this node, the region spawns the entity actor (if it doesn't exist yet) and delivers the message. Messages for shards owned by other nodes are forwarded over the network.</li> </ul> <p>Underneath the sharding layer, a single topology actor owns all cluster membership \u2014 it runs the gossip protocol for state propagation, a phi accrual failure detector (Hayashibara et al.) for identifying unresponsive nodes, and vector clocks for resolving conflicting state during network partitions. Cluster consumers (coordinator, receptionist, singleton managers) self-subscribe via <code>SubscribeTopology</code> and receive <code>TopologySnapshot</code> pushes whenever the cluster state changes.</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | GetBalance\n\ndef account_entity(entity_id: str) -&gt; Behavior[AccountMsg]:\n    def active(balance: int) -&gt; Behavior[AccountMsg]:\n        async def receive(ctx: ActorContext[AccountMsg], msg: AccountMsg) -&gt; Behavior[AccountMsg]:\n            match msg:\n                case Deposit(amount):\n                    return active(balance + amount)\n                case GetBalance(reply_to):\n                    reply_to.tell(balance)\n                    return Behaviors.same()\n\n        return Behaviors.receive(receive)\n\n    return active(0)\n\nasync def main() -&gt; None:\n    async with (\n        ClusteredActorSystem(\n            name=\"bank\", host=\"127.0.0.1\", port=25520,\n            node_id=\"node-1\", seed_nodes=[(\"127.0.0.1\", 25521)],\n        ) as node1,\n        ClusteredActorSystem(\n            name=\"bank\", host=\"127.0.0.1\", port=25521,\n            node_id=\"node-2\", seed_nodes=[(\"127.0.0.1\", 25520)],\n        ) as node2,\n    ):\n        accounts1 = node1.spawn(Behaviors.sharded(account_entity, num_shards=100), \"accounts\")\n        accounts2 = node2.spawn(Behaviors.sharded(account_entity, num_shards=100), \"accounts\")\n        await asyncio.sleep(0.3)\n\n        # Deposit via node 1\n        accounts1.tell(ShardEnvelope(\"alice\", Deposit(100)))\n        accounts1.tell(ShardEnvelope(\"bob\", Deposit(200)))\n        await asyncio.sleep(0.3)\n\n        # Deposit via node 2 \u2014 routes to the correct node transparently\n        accounts2.tell(ShardEnvelope(\"alice\", Deposit(50)))\n        await asyncio.sleep(0.3)\n\n        # Query from either node\n        balance = await node1.ask(\n            accounts1,\n            lambda r: ShardEnvelope(\"alice\", GetBalance(reply_to=r)),\n            timeout=2.0,\n        )\n        print(f\"Alice's balance: {balance}\")  # Alice's balance: 150\n\nasyncio.run(main())\n</code></pre> <p>Every node in the cluster runs the same code \u2014 only <code>host</code>, <code>port</code>, and <code>node_id</code> differ. Each node has a human-readable <code>node_id</code> (e.g. <code>\"node-1\"</code>, <code>\"worker-east\"</code>) that is propagated through the topology actor and can be used with <code>system.lookup(\"/actor\", node=\"node-1\")</code> to address actors on specific nodes. <code>ShardEnvelope(entity_id, message)</code> wraps a message with the entity ID for routing. The proxy actor on each node caches shard-to-node mappings and forwards messages to the correct region, whether local or remote.</p> <p><code>ClusteredActorSystem</code> extends <code>ActorSystem</code>. Spawning a <code>ShardedBehavior</code> (produced by <code>Behaviors.sharded()</code>) returns an <code>ActorRef[ShardEnvelope[M]]</code>. All other spawns work identically to the local <code>ActorSystem</code>. This means existing actors that don't need distribution require no changes when moving to a clustered deployment.</p> <p>Next: Cluster Broadcast</p>"},{"location":"clustering/cluster-singleton/","title":"Cluster Singleton","text":"<p>Some actors must exist exactly once in the entire cluster. A cluster-wide lock manager, a coordinator that assigns work, a single pipeline that ingests an external feed \u2014 spawning two would cause conflicts or duplicate processing. Cluster Singleton guarantees that exactly one instance of an actor runs across all nodes, always on the current leader.</p> <p>Sharding could technically achieve this with <code>num_shards=1</code> and a fixed entity ID, but it brings coordinator/region/proxy overhead and forces the <code>ShardEnvelope</code> wrapper on every message. Singleton is the direct solution: one actor, one location, automatic failover, typed ref with no envelope.</p> <pre><code>@dataclass(frozen=True)\nclass ProcessJob:\n    job_id: str\n\n@dataclass(frozen=True)\nclass GetStatus:\n    reply_to: ActorRef[str]\n\ntype CoordinatorMsg = ProcessJob | GetStatus\n\ndef job_coordinator() -&gt; Behavior[CoordinatorMsg]:\n    def active(pending: tuple[str, ...] = ()) -&gt; Behavior[CoordinatorMsg]:\n        async def receive(ctx: ActorContext[CoordinatorMsg], msg: CoordinatorMsg) -&gt; Behavior[CoordinatorMsg]:\n            match msg:\n                case ProcessJob(job_id):\n                    return active((*pending, job_id))\n                case GetStatus(reply_to):\n                    reply_to.tell(f\"{len(pending)} jobs pending\")\n                    return Behaviors.same()\n\n        return Behaviors.receive(receive)\n\n    return active()\n\nasync def main() -&gt; None:\n    async with (\n        ClusteredActorSystem(\n            name=\"cluster\", host=\"127.0.0.1\", port=25520,\n            node_id=\"node-1\", seed_nodes=[(\"127.0.0.1\", 25521)],\n        ) as node1,\n        ClusteredActorSystem(\n            name=\"cluster\", host=\"127.0.0.1\", port=25521,\n            node_id=\"node-2\", seed_nodes=[(\"127.0.0.1\", 25520)],\n        ) as node2,\n    ):\n        ref1 = node1.spawn(Behaviors.singleton(job_coordinator), \"coordinator\")\n        ref2 = node2.spawn(Behaviors.singleton(job_coordinator), \"coordinator\")\n        await asyncio.sleep(0.5)\n\n        # Send from either node \u2014 messages reach the single instance\n        ref1.tell(ProcessJob(\"job-1\"))\n        ref2.tell(ProcessJob(\"job-2\"))\n\n        status = await node1.ask(\n            ref1,\n            lambda r: GetStatus(reply_to=r),\n            timeout=2.0,\n        )\n        print(status)  # 2 jobs pending\n\nasyncio.run(main())\n</code></pre> <p>The ref returned by <code>spawn()</code> is <code>ActorRef[CoordinatorMsg]</code> \u2014 typed, no wrapper. Every node calls <code>spawn()</code> with the same name and factory, but only the leader node runs the actual actor. Non-leader nodes forward messages transparently.</p>"},{"location":"clustering/cluster-singleton/#how-it-works","title":"How It Works","text":"<p>Each call to <code>system.spawn(Behaviors.singleton(factory), name)</code> creates a singleton manager actor on that node. The manager has three modes:</p> <ul> <li>Pending. No topology received yet. Buffers incoming messages until the first <code>TopologySnapshot</code> arrives with leader information.</li> <li>Active. This node is the leader. The manager spawns the actual singleton actor as a child and forwards all messages to it.</li> <li>Standby. Another node is the leader. The manager forwards messages to the leader's manager over TCP.</li> </ul> <p>On startup, the manager subscribes to the topology actor via <code>SubscribeTopology</code> and receives <code>TopologySnapshot</code> pushes whenever the cluster state changes. When a leadership change occurs (a node goes down, a new leader is elected), every singleton manager receives an updated <code>TopologySnapshot</code>. The old leader's manager stops its child. The new leader's manager spawns a fresh instance from the factory.</p>"},{"location":"clustering/cluster-singleton/#failover","title":"Failover","text":"<p>The singleton tracks leadership, not individual node health. When the leader node dies:</p> <ol> <li>The phi accrual failure detector marks the node as unreachable.</li> <li>The topology actor transitions the member to <code>down</code> status via <code>DownMember</code>.</li> <li>Leader election (deterministic \u2014 lowest address among <code>up</code> members) produces a new leader.</li> <li>The topology actor pushes a new <code>TopologySnapshot</code> to all subscribers, and the new leader's singleton manager spawns the singleton.</li> </ol> <p>Messages sent during the transition are buffered by the manager on the sending node and delivered once the new singleton is active.</p>"},{"location":"clustering/cluster-singleton/#event-sourced-singleton","title":"Event-Sourced Singleton","text":"<p>If the singleton needs to survive failover with its state intact, wrap it with event sourcing. The journal persists events, and the new instance replays them on startup:</p> <pre><code>@dataclass(frozen=True)\nclass Incremented:\n    pass\n\ndef on_event(state: int, _event: Incremented) -&gt; int:\n    return state + 1\n\nasync def on_command(\n    _ctx: ActorContext[CounterMsg], state: int, msg: CounterMsg,\n) -&gt; Behavior[CounterMsg]:\n    match msg:\n        case Increment():\n            return Behaviors.persisted([Incremented()])\n        case GetCount(reply_to):\n            reply_to.tell(state)\n            return Behaviors.same()\n\nref = system.spawn(\n    Behaviors.singleton(lambda: Behaviors.event_sourced(\n        entity_id=\"my-singleton\",\n        journal=journal,\n        initial_state=0,\n        on_event=on_event,\n        on_command=on_command,\n    )),\n    \"counter\",\n)\n</code></pre> <p>Without event sourcing, the singleton restarts from scratch on the new leader \u2014 the factory is called with no prior state. Choose based on whether state loss is acceptable during failover.</p> <p>Next: Service Discovery</p>"},{"location":"clustering/distributed-data-structures/","title":"Distributed Data Structures","text":"<p>For common patterns that don't require custom entity actors, Casty provides higher-level data structures and coordination primitives built on top of cluster sharding. Each structure is backed by sharded actors, optionally persistent via event sourcing.</p> <p>Every concept from the previous sections \u2014 actors, functional state, event sourcing, cluster sharding \u2014 converges here. A <code>Counter</code>, for example, is a sharded actor with a predefined message protocol, distributed transparently across the cluster.</p> <pre><code>@dataclass(frozen=True)\nclass User:\n    name: str\n    email: str\n\nasync def main() -&gt; None:\n    async with ClusteredActorSystem(\n        name=\"my-app\", host=\"127.0.0.1\", port=25520,\n        node_id=\"node-1\", seed_nodes=[(\"127.0.0.1\", 25521)],\n    ) as system:\n        d = system.distributed()\n\n        # Counter\n        views = d.counter(\"page-views\", shards=50)\n        await views.increment(100)\n        await views.decrement(10)\n        value = await views.get()        # 90\n\n        # Dict \u2014 one sharded entity per key\n        users = d.map[str, User](\"users\", shards=50)\n        await users.put(\"alice\", User(\"Alice\", \"alice@example.com\"))\n        user = await users.get(\"alice\")  # User(name='Alice', email='alice@example.com')\n        await users.contains(\"alice\")    # True\n        await users.delete(\"alice\")      # True (existed)\n\n        # Set\n        tags = d.set[str](\"active-tags\", shards=50)\n        await tags.add(\"python\")         # True (added)\n        await tags.add(\"python\")         # False (already present)\n        await tags.size()                # 1\n\n        # Queue (FIFO)\n        jobs = d.queue[str](\"work-queue\", shards=50)\n        await jobs.enqueue(\"task-1\")\n        await jobs.peek()                # \"task-1\" (does not remove)\n        await jobs.dequeue()             # \"task-1\" (removes)\n        await jobs.dequeue()             # None (empty)\n\n        # Lock (distributed mutex)\n        lock = d.lock(\"deploy-lock\", shards=10)\n        await lock.acquire()             # blocks until granted\n        # ... critical section ...\n        await lock.release()             # True (released)\n\n        # Lock \u2014 non-blocking try\n        got = await lock.try_acquire()   # True if free, False if held\n\n        # Semaphore (bounded concurrency)\n        sem = d.semaphore(\"db-pool\", permits=5, shards=10)\n        await sem.acquire()              # blocks until a permit is available\n        # ... use connection ...\n        await sem.release()              # True (released)\n\n        # Barrier (distributed rendezvous)\n        barrier = d.barrier(\"phase-1\")\n        await barrier.arrive(expected=3) # blocks until 3 nodes arrive\n\nasyncio.run(main())\n</code></pre> <p>All structures accept <code>shards</code> (default <code>100</code>) for distribution granularity and <code>timeout</code> (default <code>5.0s</code>) for operation timeouts. The same structure can be accessed from any node in the cluster \u2014 use the same <code>name</code> and <code>shards</code>.</p> <p>Lock provides mutual exclusion across the cluster. Each <code>Lock</code> instance has a unique owner (auto-generated UUID). <code>acquire()</code> blocks until the lock is granted (FIFO ordering among waiters). <code>try_acquire()</code> returns immediately with <code>True</code>/<code>False</code>. <code>release()</code> returns <code>False</code> if the caller is not the current holder \u2014 only the owner can release.</p> <p>Semaphore generalizes Lock to bounded concurrency. <code>semaphore(\"name\", permits=N)</code> allows up to N concurrent holders. The same owner can acquire multiple permits. When all permits are taken, <code>acquire()</code> blocks and <code>try_acquire()</code> returns <code>False</code>. Released permits are granted to waiters in FIFO order.</p> <p>Barrier provides a distributed rendezvous point. <code>arrive(expected=N)</code> blocks until N participants have reached the barrier, then releases all simultaneously. Barriers are reusable \u2014 the same name can be used for multiple rounds. The <code>node_id</code> defaults to the system's <code>host:port</code> but can be overridden for custom identification.</p> <p>To make data structures persistent, pass an <code>EventJournal</code> to the <code>distributed()</code> facade. Every mutation is persisted as an event and replayed on recovery:</p> <pre><code>d = system.distributed(journal=InMemoryJournal())\ncounter = d.counter(\"hits\")\nawait counter.increment(10)\n# Process restarts \u2192 replays Incremented(10) \u2192 recovers value=10\n</code></pre> <p>These structures are intentionally simple. For domain-specific entities \u2014 bank accounts, user sessions, IoT sensor aggregators \u2014 define custom behaviors using <code>Behaviors.sharded()</code> and <code>Behaviors.event_sourced()</code>. The distributed data structures exist for the common cases that don't warrant a custom actor.</p> <p>Next: TLS</p>"},{"location":"clustering/service-discovery/","title":"Service Discovery","text":"<p>In a cluster, knowing which nodes exist isn't enough. You need to know which services are running and where. A payment processor on node-2, three chat handlers spread across the cluster, a monitoring agent that just came online \u2014 without discovery, every caller needs hardcoded paths and node addresses.</p> <p>The receptionist solves this. Actors register themselves under typed <code>ServiceKey</code>s, and other actors discover them via <code>Find</code> (one-shot) or <code>Subscribe</code> (continuous). The registry propagates through the cluster's topology actor \u2014 no extra round-trips, no external service registry.</p> <pre><code>PAYMENT_KEY: ServiceKey[PaymentMsg] = ServiceKey(\"payment\")\n\nasync with ClusteredActorSystem(...) as system:\n    ref = system.spawn(\n        Behaviors.discoverable(payment_actor(), key=PAYMENT_KEY),\n        \"payment\",\n    )\n\n    listing = await system.lookup(PAYMENT_KEY)\n    for instance in listing.instances:\n        instance.ref.tell(ProcessPayment(amount=100))\n</code></pre>"},{"location":"clustering/service-discovery/#behaviorsdiscoverable","title":"Behaviors.discoverable()","text":"<p>The most common pattern \u2014 spawn an actor and register it \u2014 is a single call with <code>Behaviors.discoverable()</code>:</p> <pre><code>ref = system.spawn(\n    Behaviors.discoverable(my_behavior, key=MY_KEY),\n    \"my-service\",\n)\n</code></pre> <p>This is equivalent to:</p> <pre><code>ref = system.spawn(my_behavior, \"my-service\")\nsystem.receptionist.tell(Register(key=MY_KEY, ref=ref))\n</code></pre> <p><code>discoverable()</code> composes with other wrappers:</p> <pre><code>ref = system.spawn(\n    Behaviors.discoverable(\n        Behaviors.supervise(my_behavior, OneForOneStrategy()),\n        key=MY_KEY,\n    ),\n    \"supervised-service\",\n)\n</code></pre> <p>Deregistration is automatic \u2014 when the actor stops, the receptionist removes it from the registry and notifies all subscribers.</p>"},{"location":"clustering/service-discovery/#register-and-find","title":"Register and Find","text":"<p>For cases where you need manual control \u2014 registering an existing actor, or registering via <code>ctx.spawn()</code> \u2014 use <code>Register</code> directly. <code>Find</code> performs a one-shot query that returns the current <code>Listing</code> \u2014 a frozen set of all known instances across all nodes.</p> <pre><code>@dataclass(frozen=True)\nclass Ping:\n    reply_to: ActorRef[str]\n\nPING_KEY: ServiceKey[Ping] = ServiceKey(\"ping\")\n\nasync with ClusteredActorSystem(...) as system:\n    ref = system.spawn(ping_actor(), \"ping-service\")\n    system.receptionist.tell(Register(key=PING_KEY, ref=ref))\n\n    listing: Listing[Ping] = await system.ask(\n        system.receptionist,\n        lambda r: Find(key=PING_KEY, reply_to=r),\n        timeout=3.0,\n    )\n\n    for instance in listing.instances:\n        reply = await system.ask(\n            instance.ref,\n            lambda r: Ping(reply_to=r),\n            timeout=2.0,\n        )\n</code></pre> <p><code>system.lookup(ServiceKey(...))</code> is a convenience that wraps <code>Find</code> + <code>ask</code> in a single call.</p>"},{"location":"clustering/service-discovery/#subscribe-for-continuous-updates","title":"Subscribe for Continuous Updates","text":"<p><code>Find</code> gives you a snapshot. <code>Subscribe</code> gives you a live stream \u2014 the subscriber immediately receives the current <code>Listing</code>, then gets notified every time an instance is added or removed.</p> <pre><code>def presence_monitor() -&gt; Behavior[Listing[ChatMsg]]:\n    def active(prev: frozenset[str]) -&gt; Behavior[Listing[ChatMsg]]:\n        async def receive(\n            ctx: ActorContext[Listing[ChatMsg]], msg: Listing[ChatMsg],\n        ) -&gt; Behavior[Listing[ChatMsg]]:\n            current = frozenset(i.ref.address.path for i in msg.instances)\n            for path in current - prev:\n                print(f\"joined: {path}\")\n            for path in prev - current:\n                print(f\"left: {path}\")\n            return active(current)\n\n        return Behaviors.receive(receive)\n\n    return active(frozenset())\n\nmonitor = system.spawn(presence_monitor(), \"monitor\")\nsystem.receptionist.tell(Subscribe(key=CHAT_KEY, reply_to=monitor))\n</code></pre> <p>The monitor reacts to changes \u2014 no polling, no timers. When a user actor registers on any node in the cluster, the monitor receives an updated <code>Listing</code> within one topology push cycle.</p>"},{"location":"clustering/service-discovery/#auto-deregister-on-stop","title":"Auto-Deregister on Stop","text":"<p>When a registered actor stops, it is automatically deregistered. The receptionist subscribes to <code>ActorStopped</code> events on the EventStream and removes the entry. Subscribers are notified with an updated <code>Listing</code> that no longer includes the stopped actor.</p> <pre><code>ref = system.spawn(\n    Behaviors.discoverable(user_actor(\"Alice\"), key=USER_KEY),\n    \"alice\",\n)\n\nref.tell(Leave())\n</code></pre> <p>No explicit <code>Deregister</code> needed \u2014 stopping the actor is enough. <code>Deregister</code> exists for cases where you want to remove an actor from the registry while keeping it alive.</p>"},{"location":"clustering/service-discovery/#how-it-works","title":"How It Works","text":"<p>The receptionist is a regular actor spawned by <code>ClusteredActorSystem</code> at startup, accessible via <code>system.receptionist</code>. It maintains two sets of entries:</p> <ul> <li>Local entries \u2014 actors registered on this node via <code>Register</code>.</li> <li>Cluster entries \u2014 actors on remote nodes, received via gossip.</li> </ul> <p>When a <code>Register</code> arrives, the receptionist adds a <code>ServiceEntry</code> to its local set and forwards it to the topology actor for cluster-wide propagation. The topology actor includes registry entries in its gossip rounds, and remote nodes receive them through normal CRDT merge.</p> <p>The registry is part of <code>TopologySnapshot</code>. <code>ClusterState</code> carries a <code>registry: frozenset[ServiceEntry]</code> field that merges with the same CRDT rules as membership \u2014 union of entries, pruning of entries from <code>down</code> nodes. The receptionist subscribes to topology updates and refreshes its remote entries whenever a new snapshot arrives. No additional network messages, no extra protocol.</p>"},{"location":"clustering/service-discovery/#cross-node-discovery","title":"Cross-Node Discovery","text":"<p>A service registered on node A becomes discoverable from node B after gossip convergence:</p> <pre><code># Node A\nref = system_a.spawn(\n    Behaviors.discoverable(echo_actor(), key=ECHO_KEY),\n    \"echo\",\n)\n\n# Node B (after topology propagation)\nlisting = await system_b.lookup(ECHO_KEY)\nfor instance in listing.instances:\n    instance.ref.tell(Echo(\"hello\"))\n</code></pre> <p>The <code>ActorRef</code> in the <code>Listing</code> is a remote ref \u2014 <code>tell()</code> transparently serializes the message and sends it over TCP. No special handling needed by the caller.</p>"},{"location":"clustering/service-discovery/#discovery-from-clusterclient","title":"Discovery from ClusterClient","text":"<p>External processes that don't join the cluster can still discover services via <code>ClusterClient.lookup()</code>. The topology snapshot pushed to the client includes the service registry, so lookups are local reads with no extra round-trip:</p> <pre><code>async with ClusterClient(\n    contact_points=[(\"10.0.1.10\", 25520)],\n    system_name=\"my-cluster\",\n) as client:\n    listing = client.lookup(ECHO_KEY)\n    for instance in listing.instances:\n        instance.ref.tell(Echo(\"hello from outside\"))\n</code></pre> <p>See Cluster Client for the full API.</p> <p>Next: Shard Replication</p>"},{"location":"clustering/shard-replication/","title":"Shard Replication","text":"<p>Sharding solves the capacity problem but introduces a new failure mode: if a node goes down, all entities hosted on that node become unavailable. Replication addresses this by maintaining passive copies of each entity on other nodes.</p> <p>The primary processes commands and pushes persisted events to its replicas after each command. Replicas maintain their own copy of the event journal and can be promoted to primary if the original node fails. This requires the entity to use event sourcing \u2014 replication operates on the event stream, not on raw state.</p> <p>The <code>min_acks</code> parameter controls the consistency/latency trade-off:</p> <code>min_acks</code> Behavior <code>0</code> Fire-and-forget replication. Lowest latency, but recent events may be lost if the primary fails before replication completes. <code>1+</code> The primary waits for N replica acknowledgments before confirming the command. Stronger durability at the cost of increased latency. <pre><code>accounts = node.spawn(\n    Behaviors.sharded(\n        account_entity,\n        num_shards=100,\n        replication=ReplicationConfig(\n            replicas=2,        # 2 passive replicas per entity\n            min_acks=1,        # wait for 1 replica ack before confirming\n            ack_timeout=5.0,   # seconds to wait for acks\n        ),\n    ),\n    \"accounts\",\n)\n</code></pre> <p>The coordinator allocates primary and replicas on different nodes. When a node fails, the failure detector triggers a <code>NodeDown</code> event, and the coordinator promotes the replica with the highest event sequence number to primary. With three nodes and <code>replicas=2</code>, every entity has a primary on one node and replicas on the other two \u2014 the cluster tolerates the loss of any single node without data loss.</p> <p>Next: Distributed Data Structures</p>"},{"location":"clustering/tls/","title":"TLS","text":"<p>All inter-node communication in a Casty cluster travels over TCP. By default, traffic is unencrypted. Enabling TLS encrypts gossip, heartbeats, shard routing, and event replication \u2014 every TCP connection the cluster opens.</p>"},{"location":"clustering/tls/#enabling-tls","title":"Enabling TLS","text":"<p>Pass a <code>Config</code> with paths to your certificate, CA, and (optionally) private key:</p> <pre><code>async with ClusteredActorSystem(\n    name=\"my-cluster\",\n    host=\"10.0.0.1\",\n    port=25520,\n    node_id=\"node-1\",\n    seed_nodes=[(\"10.0.0.2\", 25520)],\n    tls=Config.from_paths(certfile=\"certs/node.pem\", cafile=\"certs/ca.pem\"),\n) as system:\n    ...  # all inter-node traffic is TLS-encrypted\n</code></pre> <p>Every node needs a certificate signed by the same CA. The certificate's Subject Alternative Name (SAN) must include the IP or hostname other nodes use to reach it.</p> <p>If the private key is in a separate file, pass <code>keyfile</code>:</p> <pre><code>tls=Config.from_paths(\n    certfile=\"certs/node.crt\",\n    keyfile=\"certs/node.key\",\n    cafile=\"certs/ca.pem\",\n)\n</code></pre> <p>When <code>tls</code> is omitted, the cluster runs in plaintext.</p>"},{"location":"clustering/tls/#mutual-tls","title":"Mutual TLS","text":"<p>TLS in Casty is always mutual. Every connection authenticates both sides: the client verifies the server's certificate, and the server verifies the client's certificate. A node that presents an invalid or missing certificate is rejected at the TCP level \u2014 it never reaches the actor system.</p> <p>Since every Casty node is both a server and a client, the same certificate and CA are used for both roles. No additional configuration is needed per direction.</p> <p>This is secure by default. Unlike web servers that accept anonymous clients, a cluster should only allow authorized nodes. One-way TLS would protect against eavesdropping but not against an impostor joining the cluster.</p>"},{"location":"clustering/tls/#custom-sslcontext","title":"Custom SSLContext","text":"<p><code>Config.from_paths</code> covers the common case. For advanced scenarios \u2014 custom cipher suites, OCSP stapling, hardware-backed keys \u2014 build <code>ssl.SSLContext</code> objects and pass them directly:</p> <pre><code>server_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\nserver_ctx.load_cert_chain(\"certs/node.pem\", \"certs/node.key\")\nserver_ctx.verify_mode = ssl.CERT_REQUIRED\nserver_ctx.load_verify_locations(\"certs/ca.pem\")\n\nclient_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\nclient_ctx.load_cert_chain(\"certs/node.pem\", \"certs/node.key\")\nclient_ctx.load_verify_locations(\"certs/ca.pem\")\n\ntls = Config(server_context=server_ctx, client_context=client_ctx)\n\nasync with ClusteredActorSystem(..., tls=tls) as system:\n    ...\n</code></pre> <p>When using <code>Config</code> directly, you are responsible for enabling mutual TLS (<code>verify_mode = ssl.CERT_REQUIRED</code> on the server context). <code>from_paths</code> does this automatically.</p>"},{"location":"clustering/tls/#generating-certificates","title":"Generating Certificates","text":"<p>For development and testing, generate a CA and node certificate with OpenSSL:</p> <pre><code># Create a CA\nopenssl req -x509 -newkey rsa:2048 -keyout ca.key -out ca.pem \\\n    -days 365 -nodes -subj \"/CN=CastyCA\"\n\n# Create a node certificate signed by the CA\nopenssl req -newkey rsa:2048 -keyout node.key -out node.csr \\\n    -nodes -subj \"/CN=10.0.0.1\"\n\nopenssl x509 -req -in node.csr -CA ca.pem -CAkey ca.key \\\n    -CAcreateserial -out node.pem -days 365 \\\n    -extfile &lt;(echo \"subjectAltName=IP:10.0.0.1\")\n</code></pre> <p>The SAN must match the address other nodes use to connect. For multiple nodes, repeat the second step with each node's IP or hostname.</p> <p>For production, use your organization's PKI or a tool like step-ca to automate certificate issuance and rotation.</p> <p>For tests, use trustme to generate throwaway CAs and certificates in memory \u2014 no OpenSSL binary, no temp files:</p> <pre><code>ca = trustme.CA()\nserver_cert = ca.issue_cert(\"127.0.0.1\")\n\nserver_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\nserver_cert.configure_cert(server_ctx)\nserver_ctx.verify_mode = ssl.CERT_REQUIRED\nca.configure_trust(server_ctx)\n\nclient_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\nclient_cert = ca.issue_cert(\"127.0.0.1\")\nclient_cert.configure_cert(client_ctx)\nca.configure_trust(client_ctx)\n\ntls = Config(server_context=server_ctx, client_context=client_ctx)\n</code></pre> <p>Next: Casty as a Cluster Backend</p>"},{"location":"concepts/actor-hierarchies/","title":"Actor Hierarchies","text":"<p>Actors form a tree. When an actor spawns a child via <code>ctx.spawn()</code>, it becomes the parent. This hierarchy is not merely organizational \u2014 it is the foundation of fault tolerance. A parent is responsible for the lifecycle of its children: it can stop them, watch them for termination, and define supervision strategies for their failures.</p> <p><code>Behaviors.setup()</code> provides access to the <code>ActorContext</code> at initialization time, which is where children are typically spawned:</p> <pre><code>@dataclass(frozen=True)\nclass Task:\n    description: str\n\n@dataclass(frozen=True)\nclass Hire:\n    name: str\n\n@dataclass(frozen=True)\nclass Assign:\n    worker: str\n    task: str\n\ntype ManagerMsg = Hire | Assign | Terminated\n\ndef worker(name: str) -&gt; Behavior[Task]:\n    async def receive(ctx: ActorContext[Task], msg: Task) -&gt; Behavior[Task]:\n        print(f\"[{name}] working on: {msg.description}\")\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\ndef manager() -&gt; Behavior[ManagerMsg]:\n    async def setup(ctx: ActorContext[ManagerMsg]) -&gt; Behavior[ManagerMsg]:\n        workers: dict[str, ActorRef[Task]] = {}\n\n        async def receive(ctx: ActorContext[ManagerMsg], msg: ManagerMsg) -&gt; Behavior[ManagerMsg]:\n            match msg:\n                case Hire(name):\n                    ref = ctx.spawn(worker(name), name)\n                    ctx.watch(ref)\n                    workers[name] = ref\n                    return Behaviors.same()\n                case Assign(worker_name, task):\n                    if worker_name in workers:\n                        workers[worker_name].tell(Task(task))\n                    return Behaviors.same()\n                case Terminated(ref):\n                    print(f\"Worker stopped: {ref.address}\")\n                    return Behaviors.same()\n\n        return Behaviors.receive(receive)\n\n    return Behaviors.setup(setup)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        mgr = system.spawn(manager(), \"manager\")\n        mgr.tell(Hire(\"alice\"))\n        mgr.tell(Hire(\"bob\"))\n        await asyncio.sleep(0.1)\n\n        mgr.tell(Assign(\"alice\", \"implement login\"))\n        mgr.tell(Assign(\"bob\", \"write tests\"))\n        await asyncio.sleep(0.1)\n\nasyncio.run(main())\n</code></pre> <p>Death watch is the mechanism by which an actor observes the termination of another actor. Calling <code>ctx.watch(ref)</code> registers interest; when the watched actor stops \u2014 whether gracefully or due to failure \u2014 the watcher receives a <code>Terminated</code> message containing the stopped actor's ref.</p> <p>Next: Supervision</p>"},{"location":"concepts/actor-runtime/","title":"Actor Runtime","text":"<p>The following concepts control the operational behavior of actors. They are grouped here because each is important but self-contained \u2014 none changes the fundamental mental model established in previous sections.</p>"},{"location":"concepts/actor-runtime/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p>Actors transition through a defined lifecycle: start, stop, and (if supervised) restart. Lifecycle hooks allow executing side effects at each boundary \u2014 acquiring resources on start, releasing them on stop, logging on restart:</p> <pre><code>def my_actor() -&gt; Behavior[str]:\n    async def pre_start(ctx: ActorContext[str]) -&gt; None:\n        ctx.log.info(\"Actor starting\")\n\n    async def post_stop(ctx: ActorContext[str]) -&gt; None:\n        ctx.log.info(\"Actor stopped\")\n\n    async def receive(ctx: ActorContext[str], msg: str) -&gt; Behavior[str]:\n        return Behaviors.same()\n\n    return Behaviors.with_lifecycle(\n        Behaviors.receive(receive),\n        pre_start=pre_start,\n        post_stop=post_stop,\n    )\n</code></pre> <p>Available hooks: <code>pre_start</code> (before first message), <code>post_stop</code> (after final message), <code>pre_restart</code> (before restart), <code>post_restart</code> (after restart, before first message of new incarnation).</p>"},{"location":"concepts/actor-runtime/#event-stream","title":"Event Stream","text":"<p>The event stream is a system-wide publish/subscribe bus for observability, implemented as an actor. <code>system.event_stream</code> returns an <code>ActorRef[EventStreamMsg]</code> \u2014 you subscribe by spawning a handler actor and sending <code>EventStreamSubscribe</code>:</p> <pre><code>from casty import (\n    ActorStarted, ActorStopped, DeadLetter,\n    EventStreamSubscribe, Behaviors,\n)\n\ndef lifecycle_logger() -&gt; Behavior[ActorStarted | ActorStopped | DeadLetter]:\n    async def receive(ctx, msg):\n        match msg:\n            case ActorStarted(ref=ref):\n                print(f\"Started: {ref}\")\n            case ActorStopped(ref=ref):\n                print(f\"Stopped: {ref}\")\n            case DeadLetter(message=message):\n                print(f\"Dead letter: {message}\")\n        return Behaviors.same()\n    return Behaviors.receive(receive)\n\nlogger = system.spawn(lifecycle_logger(), \"lifecycle-logger\")\nsystem.event_stream.tell(EventStreamSubscribe(event_type=ActorStarted, handler=logger))\nsystem.event_stream.tell(EventStreamSubscribe(event_type=ActorStopped, handler=logger))\nsystem.event_stream.tell(EventStreamSubscribe(event_type=DeadLetter, handler=logger))\n</code></pre> <p>A <code>DeadLetter</code> is published when a message is sent to an actor that has already stopped. This is valuable for debugging message routing issues.</p> <p>Available events: <code>ActorStarted</code>, <code>ActorStopped</code>, <code>ActorRestarted</code>, <code>DeadLetter</code>, <code>UnhandledMessage</code>. In clustered mode, additional events are published: <code>MemberUp</code>, <code>MemberLeft</code>, <code>UnreachableMember</code>, <code>ReachableMember</code>.</p>"},{"location":"concepts/actor-runtime/#mailbox-configuration","title":"Mailbox Configuration","text":"<p>Each actor has a mailbox \u2014 a bounded queue that buffers incoming messages. When messages arrive faster than the actor can process them, the overflow strategy determines what happens:</p> <pre><code>ref = system.spawn(\n    my_behavior(),\n    \"bounded-actor\",\n    mailbox=Mailbox(capacity=100, overflow=MailboxOverflowStrategy.drop_oldest),\n)\n</code></pre> Strategy Behavior <code>drop_new</code> (default) Discard the incoming message when the mailbox is full <code>drop_oldest</code> Discard the oldest message in the mailbox to make room for the new one <code>reject</code> Raise <code>asyncio.QueueFull</code>, rejecting the message"},{"location":"concepts/actor-runtime/#scheduling","title":"Scheduling","text":"<p>The scheduler is an actor (spawned lazily by the system) that manages timed message delivery. Two patterns are supported: periodic ticks and one-shot delays.</p> <pre><code># Send a BalanceReport to the account actor every 30 seconds\nsystem.tick(\"report\", account_ref, BalanceReport(), interval=30.0)\n\n# Send a Timeout message after 5 seconds\nsystem.schedule(\"timeout\", account_ref, Timeout(), delay=5.0)\n\n# Cancel a scheduled task\nsystem.cancel_schedule(\"report\")\n</code></pre> <p>Scheduled tasks are identified by a string key. Scheduling a new task with the same key cancels the previous one.</p>"},{"location":"concepts/actor-runtime/#task-runner","title":"Task Runner","text":"<p>The task runner is a system-level actor (<code>_task_runner</code>) that centralizes fire-and-forget coroutine execution. Instead of scattering bare <code>asyncio.create_task</code> calls \u2014 which produce orphan tasks that generate warnings on shutdown \u2014 all fire-and-forget work is routed through the task runner. It tracks every running task and cancels them all when it stops.</p> <p>The system spawns the task runner automatically on the first <code>spawn()</code> call. You can also use it directly for your own fire-and-forget work:</p> <pre><code>from casty import RunTask, TaskCompleted, TaskResult\n\n# Fire and forget \u2014 no notification\nsystem.lookup(\"/_task_runner\").tell(RunTask(some_coroutine()))\n\n# With completion notification\n@dataclass(frozen=True)\nclass MyMsg:\n    result: TaskResult\n\ntask_runner_ref.tell(RunTask(\n    coro=some_coroutine(),\n    reply_to=my_actor_ref,\n    key=\"my-job\",\n))\n# my_actor_ref will receive TaskCompleted(\"my-job\"),\n# TaskFailed(\"my-job\", exc), or TaskCancelled(\"my-job\")\n</code></pre> <p>The task runner is stopped last during shutdown, ensuring that fire-and-forget tasks from other actors are properly cancelled rather than orphaned.</p> <p>Next: Spy</p>"},{"location":"concepts/actors-and-messages/","title":"Actors and Messages","text":"<p>In the actor model, an actor is the fundamental unit of computation. When an actor receives a message, it can do exactly three things:</p> <ol> <li>Send messages to other actors it knows about.</li> <li>Create new actors (its children).</li> <li>Designate the behavior for the next message it receives.</li> </ol> <p>That's the entire model. There is no shared memory between actors, no synchronization primitives, no mutex. Concurrency emerges naturally: each actor processes one message at a time from its mailbox, and multiple actors run concurrently on the asyncio event loop.</p> <p>A message is any value sent to an actor. In Casty, messages are frozen dataclasses \u2014 immutable by construction. Related messages are grouped using PEP 695 type aliases, which enables exhaustive pattern matching via <code>match</code> statements:</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass Withdraw:\n    amount: int\n    reply_to: ActorRef[str]\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | Withdraw | GetBalance\n</code></pre> <p>The type alias <code>AccountMsg</code> is a union of all messages the actor accepts. When handling messages with <code>match</code>, the type checker verifies that all variants are covered. If a new message type is added to the union but not handled, Pyright reports an error at development time.</p> <p>Next: Behaviors as Values</p>"},{"location":"concepts/behaviors/","title":"Behaviors as Values","text":"<p>In most actor frameworks, an actor is defined by subclassing a base class and overriding a <code>receive</code> method. Casty takes a different approach: behaviors are values, not classes.</p> <p>A behavior is a frozen dataclass that describes how an actor processes messages. You compose behaviors using the <code>Behaviors</code> factory:</p> Factory Purpose <code>Behaviors.receive(handler)</code> Create a behavior from an async message handler <code>(ctx, msg) -&gt; Behavior</code> <code>Behaviors.setup(factory)</code> Run initialization logic with access to <code>ActorContext</code>, then return the real behavior <code>Behaviors.ignore()</code> Accept any message and keep the current behavior \u2014 useful for terminal states <code>Behaviors.same()</code> Keep the current behavior unchanged (returned from a message handler) <code>Behaviors.stopped()</code> Stop the actor gracefully <code>Behaviors.unhandled()</code> Signal that the message was not handled <code>Behaviors.restart()</code> Explicitly restart the actor <code>Behaviors.supervise(behavior, strategy)</code> Wrap a behavior with a supervision strategy <code>Behaviors.with_lifecycle(behavior, ...)</code> Attach lifecycle hooks (pre_start, post_stop, etc.) <code>Behaviors.event_sourced(...)</code> Persist actor state as a sequence of events <code>Behaviors.persisted(events)</code> Return from a command handler to persist events and update state <code>Behaviors.sharded(entity_factory, ...)</code> Distribute entities across cluster nodes via sharding <code>Behaviors.singleton(factory)</code> Run exactly one actor instance across the cluster <code>Behaviors.broadcasted(behavior)</code> Enable broadcasting messages to all instances across the cluster <code>Behaviors.discoverable(behavior, key=...)</code> Auto-register the actor with the cluster receptionist on spawn <code>Behaviors.spy(behavior, observer)</code> Observe all messages an actor processes <p>Because behaviors are values, they compose naturally. A behavior can be wrapped with supervision, decorated with lifecycle hooks, and backed by event sourcing \u2014 all through function composition, not class inheritance.</p> <p>Next: Functional State</p>"},{"location":"concepts/functional-state/","title":"Functional State","text":"<p>The actor model requires each actor to designate the behavior for its next message. This is the mechanism for state transitions. In Casty, state is captured in closures: the message handler closes over the current state, and returning a new behavior with different closed-over values constitutes a state transition.</p> <p>Consider a bank account actor that tracks a balance:</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | GetBalance\n\ndef bank_account(balance: int = 0) -&gt; Behavior[AccountMsg]:\n    async def receive(ctx: ActorContext[AccountMsg], msg: AccountMsg) -&gt; Behavior[AccountMsg]:\n        match msg:\n            case Deposit(amount):\n                return bank_account(balance + amount)\n            case GetBalance(reply_to):\n                reply_to.tell(balance)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        account = system.spawn(bank_account(), \"account\")\n        account.tell(Deposit(100))\n        account.tell(Deposit(50))\n        await asyncio.sleep(0.1)\n\nasyncio.run(main())\n</code></pre> <p>The line <code>return bank_account(balance + amount)</code> is the state transition. It creates a new <code>ReceiveBehavior</code> whose handler closes over <code>balance + amount</code>. There is no mutable field, no <code>self.balance = ...</code>, no <code>nonlocal</code>. The function call is the state transition.</p> <p>This approach \u2014 called behavior recursion \u2014 makes state transitions explicit, traceable, and impossible to corrupt through accidental sharing. Each invocation of <code>bank_account(n)</code> produces a completely independent behavior value.</p> <p>Next: Request-Reply</p>"},{"location":"concepts/pipe-to-self/","title":"Pipe to Self","text":"<p>Actors process one message at a time. When an actor needs to call an async function \u2014 an HTTP request, a database query, file I/O \u2014 awaiting it inside the handler blocks the mailbox. No other messages can be processed until the coroutine completes.</p> <p>Request-reply solves actor-to-actor communication, but what about plain async functions that aren't actors?</p> <p><code>ctx.pipe_to_self()</code> dispatches a coroutine as a background <code>asyncio.Task</code> and sends the mapped result back to the actor's mailbox, preserving the sequential message-processing guarantee without blocking.</p> <pre><code>@dataclass(frozen=True)\nclass FetchUser:\n    user_id: str\n\n@dataclass(frozen=True)\nclass UserFound:\n    name: str\n\n@dataclass(frozen=True)\nclass UserFetchFailed:\n    error: str\n\n@dataclass(frozen=True)\nclass GetResult:\n    reply_to: ActorRef[str]\n\ntype FetcherMsg = FetchUser | UserFound | UserFetchFailed | GetResult\n\n\nasync def fetch_user_from_api(user_id: str) -&gt; str:\n    \"\"\"Simulate an async API call.\"\"\"\n    await asyncio.sleep(0.1)\n    return f\"User-{user_id}\"\n\n\ndef fetcher(result: str = \"\") -&gt; Behavior[FetcherMsg]:\n    async def receive(ctx: ActorContext[FetcherMsg], msg: FetcherMsg) -&gt; Behavior[FetcherMsg]:\n        match msg:\n            case FetchUser(user_id=uid):\n                ctx.pipe_to_self(\n                    fetch_user_from_api(uid),\n                    lambda name: UserFound(name=name),\n                    on_failure=lambda exc: UserFetchFailed(error=str(exc)),\n                )\n                return Behaviors.same()\n            case UserFound(name=name):\n                return fetcher(result=name)\n            case UserFetchFailed(error=error):\n                return fetcher(result=f\"error: {error}\")\n            case GetResult(reply_to=reply_to):\n                reply_to.tell(result)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n\nasync def main() -&gt; None:\n    async with ActorSystem(\"pipe-demo\") as system:\n        ref = system.spawn(fetcher(), \"fetcher\")\n\n        ref.tell(FetchUser(user_id=\"42\"))\n        await asyncio.sleep(0.2)\n\n        result = await system.ask(ref, lambda r: GetResult(reply_to=r), timeout=2.0)\n        print(f\"Result: {result}\")  # Result: User-42\n\n\nasyncio.run(main())\n</code></pre> <p>The actor receives <code>FetchUser</code>, kicks off the async operation in the background, and immediately returns <code>Behaviors.same()</code> \u2014 the mailbox stays open. When the coroutine completes, the <code>mapper</code> wraps the result into a <code>UserFound</code> message that arrives through the normal mailbox, processed in order with everything else.</p>"},{"location":"concepts/pipe-to-self/#error-handling","title":"Error handling","text":"<p>The optional <code>on_failure</code> parameter maps exceptions to messages:</p> <pre><code>ctx.pipe_to_self(\n    fetch_user_from_api(user_id),\n    lambda name: UserFound(name=name),\n    on_failure=lambda exc: UserFetchFailed(error=str(exc)),\n)\n</code></pre> <p>If <code>on_failure</code> is omitted and the coroutine raises, a warning is logged and the exception is discarded. The actor continues processing other messages normally.</p>"},{"location":"concepts/pipe-to-self/#signature","title":"Signature","text":"<pre><code>def pipe_to_self[T](\n    self,\n    coro: Awaitable[T],\n    mapper: Callable[[T], M],\n    on_failure: Callable[[Exception], M] | None = None,\n) -&gt; None\n</code></pre> Parameter Type Description <code>coro</code> <code>Awaitable[T]</code> The async operation to run in background <code>mapper</code> <code>Callable[[T], M]</code> Maps the successful result to a message for this actor <code>on_failure</code> <code>Callable[[Exception], M] \\| None</code> Maps a failure to a message. If <code>None</code>, failures are logged and discarded <p>Warning</p> <p>The coroutine runs outside the actor's sequential message processing. Do not capture mutable state from the closure \u2014 by the time the coroutine completes, the actor may have moved to a different behavior with different state. The result should arrive as a message and be handled like any other.</p> <p>Next: Actor Hierarchies</p>"},{"location":"concepts/request-reply/","title":"Request-Reply","text":"<p>Actors communicate via fire-and-forget <code>tell()</code> by default. When a response is needed, the actor model uses the reply-to pattern: the sender includes its own <code>ActorRef</code> in the message so the receiver can send a response back.</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass Withdraw:\n    amount: int\n    reply_to: ActorRef[str]\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | Withdraw | GetBalance\n\ndef bank_account(balance: int = 0) -&gt; Behavior[AccountMsg]:\n    async def receive(ctx: ActorContext[AccountMsg], msg: AccountMsg) -&gt; Behavior[AccountMsg]:\n        match msg:\n            case Deposit(amount):\n                return bank_account(balance + amount)\n            case Withdraw(amount, reply_to) if balance &gt;= amount:\n                reply_to.tell(\"ok\")\n                return bank_account(balance - amount)\n            case Withdraw(_, reply_to):\n                reply_to.tell(f\"insufficient funds (balance={balance})\")\n                return Behaviors.same()\n            case GetBalance(reply_to):\n                reply_to.tell(balance)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        account = system.spawn(bank_account(), \"account\")\n        account.tell(Deposit(100))\n\n        balance = await system.ask(\n            account,\n            lambda reply_to: GetBalance(reply_to=reply_to),\n            timeout=5.0,\n        )\n        print(f\"Balance: {balance}\")  # Balance: 100\n\n        result = await system.ask(\n            account,\n            lambda reply_to: Withdraw(200, reply_to=reply_to),\n            timeout=5.0,\n        )\n        print(f\"Withdraw: {result}\")  # Withdraw: insufficient funds (balance=100)\n\nasyncio.run(main())\n</code></pre> <p><code>system.ask()</code> is a convenience that creates a temporary actor behind the scenes, passes its <code>ActorRef</code> as the <code>reply_to</code> field, and awaits the response with a timeout. The underlying mechanism is still <code>tell</code> \u2014 <code>ask</code> simply wraps the reply-to pattern into a coroutine.</p> <p>Warning</p> <p><code>system.ask()</code> is meant for use outside actors \u2014 from <code>main()</code>, HTTP handlers, or other external code. If called inside an actor's receive handler, it blocks that actor's mailbox until the response arrives (actors process one message at a time), which can lead to deadlocks.</p> <p>Inside actors, pass <code>ctx.self</code> as <code>reply_to</code> and handle the response as a regular message:</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | GetBalance\n\ndef bank_account(balance: int = 0) -&gt; Behavior[AccountMsg]:\n    async def receive(ctx: ActorContext[AccountMsg], msg: AccountMsg) -&gt; Behavior[AccountMsg]:\n        match msg:\n            case Deposit(amount):\n                return bank_account(balance + amount)\n            case GetBalance(reply_to):\n                reply_to.tell(balance)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n@dataclass(frozen=True)\nclass CheckBalance:\n    account: ActorRef[AccountMsg]\n\ntype MonitorMsg = CheckBalance | int\n\ndef monitor() -&gt; Behavior[MonitorMsg]:\n    async def receive(ctx: ActorContext[MonitorMsg], msg: MonitorMsg) -&gt; Behavior[MonitorMsg]:\n        match msg:\n            case CheckBalance(account):\n                # Non-blocking: sends the request and keeps processing\n                account.tell(GetBalance(reply_to=ctx.self))\n                return Behaviors.same()\n            case int() as balance:\n                print(f\"Balance: {balance}\")\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        acc = system.spawn(bank_account(), \"account\")\n        acc.tell(Deposit(100))\n\n        mon = system.spawn(monitor(), \"monitor\")\n        mon.tell(CheckBalance(account=acc))  # prints \"Balance: 100\"\n</code></pre> <p>Next: Pipe to Self</p>"},{"location":"concepts/spy/","title":"Spy","text":"<p>Actors are opaque by design \u2014 you send messages in and observe effects out, but you can't see what an actor received, in what order, or when. This makes it difficult to trace message flows, verify that routing is correct, or understand why an actor reached a particular state.</p> <p><code>Behaviors.spy()</code> wraps any behavior with a transparent observer. The spy is a cell-level wrapper \u2014 like <code>SupervisedBehavior</code> or <code>LifecycleBehavior</code> \u2014 so the cell itself emits a <code>SpyEvent</code> after processing each message. This means all messages are captured, including self-tells (<code>ctx.self.tell(...)</code>).</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\ntype AccountMsg = Deposit | GetBalance\n\n\ndef account(balance: int = 0) -&gt; Behavior[AccountMsg]:\n    async def receive(ctx: ActorContext[AccountMsg], msg: AccountMsg) -&gt; Behavior[AccountMsg]:\n        match msg:\n            case Deposit(amount):\n                return account(balance + amount)\n            case GetBalance(reply_to=reply_to):\n                reply_to.tell(balance)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n\ndef log_observer() -&gt; Behavior[SpyEvent[AccountMsg]]:\n    async def receive(\n        ctx: ActorContext[SpyEvent[AccountMsg]], event: SpyEvent[AccountMsg]\n    ) -&gt; Behavior[SpyEvent[AccountMsg]]:\n        print(f\"[spy] {event.actor_path} received {event.event} at t={event.timestamp:.4f}\")\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n\nasync def main() -&gt; None:\n    async with ActorSystem(\"bank\") as system:\n        observer = system.spawn(log_observer(), \"observer\")\n\n        ref = system.spawn(\n            Behaviors.spy(account(), observer),\n            \"account\",\n        )\n\n        ref.tell(Deposit(100))\n        ref.tell(Deposit(50))\n        balance = await system.ask(ref, lambda r: GetBalance(reply_to=r), timeout=2.0)\n        print(f\"Balance: {balance}\")\n\n        await asyncio.sleep(0.1)\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>[spy] /account received Deposit(amount=100) at t=12345.0001\n[spy] /account received Deposit(amount=50) at t=12345.0002\n[spy] /account received GetBalance(reply_to=...) at t=12345.0003\nBalance: 150\n</code></pre> <p>The spy is completely transparent: the target actor processes messages normally, replies reach their destination, and supervision works as expected. Because the spy is a cell-level behavior wrapper, it captures every message the actor actually processes \u2014 including self-tells that a forwarding proxy would miss.</p> <p><code>SpyEvent</code> contains three fields:</p> Field Type Description <code>actor_path</code> <code>str</code> Path of the spied actor (e.g. <code>\"/account\"</code>) <code>event</code> <code>M \\| Terminated</code> The message received by the actor <code>timestamp</code> <code>float</code> Monotonic timestamp via <code>time.monotonic()</code> <p>The spy composes freely with other behavior wrappers. Spy a supervised actor to observe crash-restart cycles. Spy a lifecycle-wrapped actor to see the messages that arrive between start and stop:</p> <pre><code>strategy = OneForOneStrategy(max_restarts=3, within=60.0)\n\nref = system.spawn(\n    Behaviors.spy(\n        Behaviors.supervise(my_behavior(), strategy),\n        observer,\n    ),\n    \"resilient\",\n)\n</code></pre> <p>The observer itself is an ordinary actor \u2014 you can write it to log, collect metrics, filter events, or forward to an external system.</p>"},{"location":"concepts/spy/#spying-on-children","title":"Spying on children","text":"<p>By default, only the wrapped actor is observed. Pass <code>spy_children=True</code> to automatically spy on every child the actor spawns \u2014 recursively. All children (and their children) report to the same observer:</p> <pre><code>ref = system.spawn(\n    Behaviors.spy(parent_behavior(), observer, spy_children=True),\n    \"parent\",\n)\n</code></pre> <p>When the parent spawns a child via <code>ctx.spawn(...)</code>, the child is transparently wrapped with the same spy observer and <code>spy_children=True</code>, so the propagation continues down the entire subtree. The <code>actor_path</code> field in each <code>SpyEvent</code> distinguishes which actor produced the event (e.g. <code>\"/parent\"</code> vs <code>\"/parent/worker-1\"</code>).</p> <p>Next: Event Sourcing</p>"},{"location":"concepts/state-machines/","title":"State Machines","text":"<p>Because behaviors are values and state transitions are function calls, finite state machines emerge as a natural pattern. Each state is a behavior function, each transition is a return value. No enum, no conditional dispatch on a status field \u2014 the behavior is the state.</p> <pre><code>@dataclass(frozen=True)\nclass Item:\n    name: str\n    price: float\n\n@dataclass(frozen=True)\nclass Receipt:\n    items: tuple[Item, ...]\n    total: float\n\n@dataclass(frozen=True)\nclass AddItem:\n    item: Item\n\n@dataclass(frozen=True)\nclass Checkout:\n    reply_to: ActorRef[Receipt]\n\n@dataclass(frozen=True)\nclass GetTotal:\n    reply_to: ActorRef[float]\n\ntype CartMsg = AddItem | Checkout | GetTotal\n\ndef empty_cart() -&gt; Behavior[CartMsg]:\n    async def receive(ctx: ActorContext[CartMsg], msg: CartMsg) -&gt; Behavior[CartMsg]:\n        match msg:\n            case AddItem(item):\n                return active_cart({item.name: item})\n            case Checkout():\n                return Behaviors.same()  # cannot checkout empty cart\n            case GetTotal(reply_to):\n                reply_to.tell(0.0)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\ndef active_cart(items: dict[str, Item]) -&gt; Behavior[CartMsg]:\n    async def receive(ctx: ActorContext[CartMsg], msg: CartMsg) -&gt; Behavior[CartMsg]:\n        match msg:\n            case AddItem(item):\n                return active_cart({**items, item.name: item})\n            case Checkout(reply_to):\n                all_items = tuple(items.values())\n                total = sum(i.price for i in all_items)\n                reply_to.tell(Receipt(items=all_items, total=total))\n                return checked_out()\n            case GetTotal(reply_to):\n                reply_to.tell(sum(i.price for i in items.values()))\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\ndef checked_out() -&gt; Behavior[CartMsg]:\n    async def receive(ctx: ActorContext[CartMsg], msg: CartMsg) -&gt; Behavior[CartMsg]:\n        match msg:\n            case GetTotal(reply_to):\n                reply_to.tell(0.0)\n                return Behaviors.same()\n            case _:\n                return Behaviors.same()  # terminal state, ignore modifications\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        cart = system.spawn(empty_cart(), \"cart\")\n\n        cart.tell(AddItem(Item(\"keyboard\", 75.0)))\n        cart.tell(AddItem(Item(\"mouse\", 25.0)))\n        await asyncio.sleep(0.1)\n\n        total = await system.ask(cart, lambda r: GetTotal(reply_to=r), timeout=5.0)\n        print(f\"Total: ${total:.2f}\")  # Total: $100.00\n\n        receipt = await system.ask(cart, lambda r: Checkout(reply_to=r), timeout=5.0)\n        print(f\"Receipt: {len(receipt.items)} items, ${receipt.total:.2f}\")\n        # Receipt: 2 items, $100.00\n\n        # Further modifications are ignored \u2014 checked_out is a terminal state\n        cart.tell(AddItem(Item(\"monitor\", 300.0)))\n\nasyncio.run(main())\n</code></pre> <p>Three functions, three states: <code>empty_cart</code>, <code>active_cart</code>, <code>checked_out</code>. The transitions are explicit in the return values \u2014 <code>empty_cart</code> transitions to <code>active_cart</code> on the first <code>AddItem</code>, <code>active_cart</code> transitions to <code>checked_out</code> on <code>Checkout</code>. A <code>checked_out</code> actor ignores further modifications. The type checker ensures every state handles the full <code>CartMsg</code> union.</p> <p>Next: Actor Runtime</p>"},{"location":"concepts/supervision/","title":"Supervision","text":"<p>In traditional programming, errors propagate upward through the call stack via exceptions. In the actor model, errors propagate to the supervisor. This is the \"let it crash\" philosophy pioneered by Erlang/OTP: instead of writing defensive code within the actor to handle every possible failure, let the actor fail fast and let its supervisor decide the recovery strategy.</p> <p>A supervisor is any actor that has spawned children. The supervision strategy defines what happens when a child fails:</p> Directive Effect <code>Directive.restart</code> Restart the actor with its initial behavior, resetting state <code>Directive.stop</code> Stop the actor permanently <code>Directive.escalate</code> Propagate the failure to the next supervisor up the hierarchy <p><code>OneForOneStrategy</code> supervises each child independently. It tracks restart counts within a configurable time window \u2014 if a child exceeds the limit, it is stopped instead of restarted:</p> <pre><code>def unreliable_worker() -&gt; Behavior[str]:\n    async def receive(ctx: ActorContext[str], msg: str) -&gt; Behavior[str]:\n        if msg == \"crash\":\n            raise RuntimeError(\"something went wrong\")\n        print(f\"Processed: {msg}\")\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    strategy = OneForOneStrategy(\n        max_restarts=3,\n        within=60.0,\n        decider=lambda exc: Directive.restart,\n    )\n\n    async with ActorSystem() as system:\n        ref = system.spawn(\n            Behaviors.supervise(unreliable_worker(), strategy),\n            \"worker\",\n        )\n\n        ref.tell(\"crash\")          # fails, supervisor restarts\n        await asyncio.sleep(0.2)\n\n        ref.tell(\"hello\")          # succeeds \u2014 actor recovered\n        await asyncio.sleep(0.1)\n\nasyncio.run(main())\n</code></pre> <p>The <code>decider</code> function receives the exception and returns a directive. This allows fine-grained control: restart on transient errors, stop on fatal ones, escalate on unknown failures.</p> <p>An important interaction to note: when a supervised actor without event sourcing is restarted, its state is reset to the initial behavior. This means the bank account from previous sections would lose its balance on restart. Event sourcing (covered in Persistence) solves this by replaying persisted events to reconstruct state after a restart.</p> <p>Next: State Machines</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Every parameter shown in previous sections \u2014 mailbox capacity, supervision strategy, gossip interval, failure detector threshold \u2014 can be set programmatically. But when deploying the same codebase across environments (dev, staging, production), hardcoding these values becomes impractical. Casty supports TOML-based configuration via a <code>casty.toml</code> file.</p> <p>Configuration is always optional. When absent, all parameters use the same defaults as the programmatic API. When present, the TOML file produces the existing dataclasses \u2014 <code>ClusterConfig</code>, <code>ReplicationConfig</code>, etc. \u2014 through a pure loader function. No global state, no singletons, no magic.</p>"},{"location":"configuration/#file-structure","title":"File Structure","text":"<p>A complete <code>casty.toml</code> covering all configurable aspects:</p> <pre><code>[system]\nname = \"my-app\"\n\n# --- Global defaults (apply to all actors) ---\n[defaults.mailbox]\ncapacity = 1000\nstrategy = \"drop_new\"         # drop_new | drop_oldest | reject\n\n[defaults.supervision]\nstrategy = \"restart\"          # restart | stop | escalate\nmax_restarts = 3\nwithin_seconds = 60.0\n\n[defaults.sharding]\nnum_shards = 256\n\n[defaults.replication]\nreplicas = 2\nmin_acks = 1\nack_timeout = 5.0\n\n# --- Cluster ---\n[cluster]\nhost = \"0.0.0.0\"\nport = 25520\nseed_nodes = [\"node1:25520\", \"node2:25520\"]\nroles = [\"worker\"]\n\n[cluster.gossip]\ninterval = 1.0\n\n[cluster.heartbeat]\ninterval = 0.5\navailability_check_interval = 2.0\n\n[cluster.tls]\ncertfile = \"certs/node.pem\"\ncafile = \"certs/ca.pem\"\n# keyfile = \"certs/node.key\"  # optional, if key is separate from certfile\n\n[cluster.failure_detector]\nthreshold = 8.0\nmax_sample_size = 200\nmin_std_deviation_ms = 100.0\nacceptable_heartbeat_pause_ms = 0.0\nfirst_heartbeat_estimate_ms = 1000.0\n\n# --- Per-actor overrides ---\n[actors.orders]\nsharding = { num_shards = 512 }\nreplication = { replicas = 3, min_acks = 2 }\n\n[actors.my-worker]\nmailbox = { capacity = 5000, strategy = \"reject\" }\nsupervision = { strategy = \"stop\" }\n\n[actors.\"child-\\\\d+\"]\nmailbox = { capacity = 100, strategy = \"drop_oldest\" }\n</code></pre> <p>Every key is optional. A minimal <code>casty.toml</code> can be just:</p> <pre><code>[system]\nname = \"my-app\"\n</code></pre>"},{"location":"configuration/#loading-configuration","title":"Loading Configuration","text":"<p><code>load_config()</code> accepts an explicit path or discovers <code>casty.toml</code> automatically by walking up from the current working directory (like <code>pyproject.toml</code>):</p> <pre><code># Auto-discovery \u2014 walks up from CWD looking for casty.toml\nconfig = load_config()\n\n# Explicit path\nconfig = load_config(Path(\"infra/casty.toml\"))\n</code></pre> <p>For a local <code>ActorSystem</code>, pass the config to the constructor:</p> <pre><code>config = load_config()\n\nasync with ActorSystem(config=config) as system:\n    ref = system.spawn(my_behavior(), \"my-actor\")\n    # mailbox and supervision come from casty.toml\n</code></pre> <p>For a <code>ClusteredActorSystem</code>, use <code>from_config</code> to derive host, port, seed nodes, and cluster tuning from the <code>[cluster]</code> section:</p> <pre><code>config = load_config()\n\nasync with ClusteredActorSystem.from_config(config) as system:\n    # host, port, seed_nodes, roles, TLS, gossip interval,\n    # heartbeat interval, failure detector \u2014 all from casty.toml\n    ...\n</code></pre> <p>Programmatic overrides take precedence over the file \u2014 useful for dynamic ports in tests or container orchestration:</p> <pre><code>async with ClusteredActorSystem.from_config(\n    config,\n    host=\"10.0.0.5\",\n    port=0,  # OS-assigned port\n) as system:\n    ...\n</code></pre>"},{"location":"configuration/#per-actor-overrides","title":"Per-Actor Overrides","text":"<p>Actor keys under <code>[actors.*]</code> are matched against the actor name at <code>spawn()</code> time using <code>re.fullmatch()</code>. Simple names like <code>orders</code> match exactly. Regex patterns use quoted TOML keys:</p> <pre><code># Exact match \u2014 only the actor named \"orders\"\n[actors.orders]\nmailbox = { capacity = 5000 }\n\n# Regex \u2014 matches \"sensor-001\", \"sensor-042\", etc.\n[actors.\"sensor-\\\\d+\"]\nmailbox = { capacity = 100, strategy = \"drop_oldest\" }\n</code></pre> <p>Resolution order: per-actor override &gt; <code>[defaults.*]</code> &gt; dataclass defaults. First matching pattern wins (definition order in TOML).</p> <pre><code>[defaults.mailbox]\ncapacity = 1000\nstrategy = \"drop_new\"\n\n[actors.orders]\nmailbox = { capacity = 5000 }\n# strategy inherits \"drop_new\" from [defaults.mailbox]\n</code></pre> <p>Internal actors (names starting with <code>_</code>) are never resolved against config \u2014 they always use framework defaults.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install casty\n</code></pre> <p>Casty requires Python 3.12+ and has zero external dependencies \u2014 stdlib only.</p>"},{"location":"getting-started/#your-first-actor","title":"Your First Actor","text":"<pre><code>@dataclass(frozen=True)\nclass Greet:\n    name: str\n\ndef greeter() -&gt; Behavior[Greet]:\n    async def receive(ctx: ActorContext[Greet], msg: Greet) -&gt; Behavior[Greet]:\n        print(f\"Hello, {msg.name}!\")\n        return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\nasync def main() -&gt; None:\n    async with ActorSystem() as system:\n        ref = system.spawn(greeter(), \"greeter\")\n        ref.tell(Greet(\"Alice\"))\n        ref.tell(Greet(\"Bob\"))\n        await asyncio.sleep(0.1)\n\nasyncio.run(main())\n# Hello, Alice!\n# Hello, Bob!\n</code></pre> <p>Four things happened here:</p> <ol> <li><code>Greet</code> is a frozen dataclass. Messages are immutable values \u2014 safe to send between actors without defensive copying.</li> <li><code>greeter()</code> returns a <code>Behavior[Greet]</code>. The behavior is a value produced by <code>Behaviors.receive()</code>, not a class instance. The actor system interprets this value to wire up the message handler.</li> <li><code>Behaviors.same()</code> means \"keep the current behavior.\" Returning a different behavior would transition the actor to a new state. This is the fundamental mechanism for state management.</li> <li><code>tell()</code> is fire-and-forget. Messages are enqueued in the actor's mailbox and processed asynchronously, one at a time.</li> </ol>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>The sections that follow build on these foundations progressively \u2014 from functional state management through fault tolerance, event sourcing, and distributed clustering. Each section introduces one concept, explains why it exists in the actor model, and shows how Casty implements it.</p> <p>Start with Actors and Messages to understand the core building blocks.</p>"},{"location":"guides/cluster-client/","title":"Client + Cluster","text":"<p>In this guide you'll connect to a running cluster from outside using <code>ClusterClient</code>. Along the way you'll learn how the client discovers topology, how <code>entity_ref</code> creates a local proxy for routing, and how <code>ask()</code> enables request-reply from external code.</p>"},{"location":"guides/cluster-client/#when-to-use-clusterclient","title":"When to Use ClusterClient","text":"<p><code>ClusteredActorSystem</code> joins the cluster as a full member \u2014 it runs actors, owns shards, participates in gossip. <code>ClusterClient</code> is for code that uses the cluster without joining it: API servers, CLI tools, monitoring dashboards. It connects via TCP, subscribes to topology updates, and routes messages directly to the node owning each shard.</p>"},{"location":"guides/cluster-client/#messages-and-entity-factory","title":"Messages and Entity Factory","text":"<p>A loyalty points system. Each user is a sharded entity:</p> <pre><code>@dataclass(frozen=True)\nclass AddPoints:\n    amount: int\n\n\n@dataclass(frozen=True)\nclass GetPoints:\n    reply_to: ActorRef[int]\n\n\ntype LoyaltyMsg = AddPoints | GetPoints\n\n\nNUM_SHARDS = 10\n\n\n# \u2500\u2500 Entity factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef loyalty_entity(entity_id: str) -&gt; Behavior[LoyaltyMsg]:\n    def active(points: int = 0) -&gt; Behavior[LoyaltyMsg]:\n        async def receive(_ctx: Any, msg: LoyaltyMsg) -&gt; Behavior[LoyaltyMsg]:\n            match msg:\n                case AddPoints(amount):\n                    print(f\"  [{entity_id}] +{amount} points (total: {points + amount})\")\n                    return active(points + amount)\n                case GetPoints(reply_to):\n                    reply_to.tell(points)\n                    return Behaviors.same()\n\n        return Behaviors.receive(receive)\n\n    return active()\n</code></pre>"},{"location":"guides/cluster-client/#starting-the-cluster","title":"Starting the Cluster","text":"<p>A single-node cluster with a sharded \"loyalty\" entity type:</p> <pre><code>async def main() -&gt; None:\n    # 1. Start a single-node cluster\n    cluster = ClusteredActorSystem(\n        name=\"loyalty-cluster\",\n        host=\"127.0.0.1\",\n        port=25530,\n        node_id=\"node-1\",\n    )\n\n    async with cluster:\n        cluster.spawn(\n            Behaviors.sharded(loyalty_entity, num_shards=NUM_SHARDS),\n            \"loyalty\",\n        )\n        await cluster.wait_for(1)\n        print(\"\u2500\u2500 Cluster running \u2500\u2500\\n\")\n</code></pre>"},{"location":"guides/cluster-client/#connecting-from-outside","title":"Connecting from Outside","text":"<p><code>ClusterClient</code> takes a list of contact points and the cluster's system name:</p> <pre><code># 2. Connect from outside with ClusterClient\nasync with ClusterClient(\n    contact_points=[(\"127.0.0.1\", 25530)],\n    system_name=\"loyalty-cluster\",\n) as client:\n    await asyncio.sleep(1.0)\n</code></pre> <p>The client connects to the contact point, subscribes to <code>TopologySnapshot</code> updates, and caches shard allocations locally. <code>entity_ref(\"loyalty\", num_shards=NUM_SHARDS)</code> creates a local proxy actor that routes <code>ShardEnvelope</code> messages to the cluster.</p>"},{"location":"guides/cluster-client/#fire-and-forget","title":"Fire and Forget","text":"<p><code>tell()</code> works the same as inside the cluster \u2014 wrap the message in <code>ShardEnvelope</code>:</p> <pre><code>async def send_points(loyalty: ActorRef[ShardEnvelope[LoyaltyMsg]]) -&gt; None:\n    print(\"\u2500\u2500 Sending points \u2500\u2500\")\n    loyalty.tell(ShardEnvelope(\"user-1\", AddPoints(100)))\n    loyalty.tell(ShardEnvelope(\"user-1\", AddPoints(50)))\n    loyalty.tell(ShardEnvelope(\"user-2\", AddPoints(200)))\n    await asyncio.sleep(0.5)\n</code></pre>"},{"location":"guides/cluster-client/#request-reply","title":"Request-Reply","text":"<p><code>client.ask()</code> creates a temporary ref, sends the message, and waits for the response:</p> <pre><code>async def query_points(\n    client: ClusterClient, loyalty: ActorRef[ShardEnvelope[LoyaltyMsg]]\n) -&gt; None:\n    print(\"\\n\u2500\u2500 Querying points \u2500\u2500\")\n    for user in (\"user-1\", \"user-2\"):\n        points: int = await client.ask(\n            loyalty,\n            lambda r, uid=user: ShardEnvelope(uid, GetPoints(reply_to=r)),\n            timeout=5.0,\n        )\n        print(f\"  {user}: {points} points\")\n</code></pre> <p>The reply travels back through the existing TCP connection \u2014 no extra server socket or reverse tunnel needed.</p>"},{"location":"guides/cluster-client/#running-it","title":"Running It","text":"<pre><code>async def main() -&gt; None:\n    # 1. Start a single-node cluster\n    cluster = ClusteredActorSystem(\n        name=\"loyalty-cluster\",\n        host=\"127.0.0.1\",\n        port=25530,\n        node_id=\"node-1\",\n    )\n\n    async with cluster:\n        cluster.spawn(\n            Behaviors.sharded(loyalty_entity, num_shards=NUM_SHARDS),\n            \"loyalty\",\n        )\n        await cluster.wait_for(1)\n        print(\"\u2500\u2500 Cluster running \u2500\u2500\\n\")\n\n        # 2. Connect from outside with ClusterClient\n        async with ClusterClient(\n            contact_points=[(\"127.0.0.1\", 25530)],\n            system_name=\"loyalty-cluster\",\n        ) as client:\n            await asyncio.sleep(1.0)\n\n            loyalty = client.entity_ref(\"loyalty\", num_shards=NUM_SHARDS)\n\n            await send_points(loyalty)\n            await query_points(client, loyalty)\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Cluster running \u2500\u2500\n\n\u2500\u2500 Sending points \u2500\u2500\n  [user-1] +100 points (total: 100)\n  [user-1] +50 points (total: 150)\n  [user-2] +200 points (total: 200)\n\n\u2500\u2500 Querying points \u2500\u2500\n  user-1: 150 points\n  user-2: 200 points\n</code></pre>"},{"location":"guides/cluster-client/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/07_cluster_client.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>ClusterClient</code> connects to a cluster without joining it \u2014 topology-aware routing with zero membership.</li> <li><code>entity_ref(shard_type, num_shards=N)</code> returns a cached proxy that routes <code>ShardEnvelope</code> to the correct node.</li> <li><code>client.ask()</code> enables request-reply from outside the cluster using temporary refs.</li> <li>Fault tolerance is built in \u2014 the client rotates contact points on timeout and caches topology locally.</li> </ul>"},{"location":"guides/clustered/","title":"Clustered","text":"<p>In this guide you'll form a two-node cluster with sharded entities distributed across nodes. Along the way you'll learn how nodes discover each other via seed nodes, how <code>ShardEnvelope</code> routes messages to the right node, and how to query entities from anywhere in the cluster.</p>"},{"location":"guides/clustered/#messages-and-entity-factory","title":"Messages and Entity Factory","text":"<p>A simple counter entity. Each <code>entity_id</code> gets its own actor instance, placed on whichever node the shard coordinator assigns:</p> <pre><code>@dataclass(frozen=True)\nclass Increment:\n    amount: int\n\n\n@dataclass(frozen=True)\nclass GetValue:\n    reply_to: ActorRef[int]\n\n\ntype CounterMsg = Increment | GetValue\n\n\n# \u2500\u2500 Entity factory (one instance per entity_id) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef counter_entity(entity_id: str) -&gt; Behavior[CounterMsg]:\n    def active(value: int = 0) -&gt; Behavior[CounterMsg]:\n        async def receive(_ctx: Any, msg: CounterMsg) -&gt; Behavior[CounterMsg]:\n            match msg:\n                case Increment(amount):\n                    print(f\"  [{entity_id}] {value} + {amount} = {value + amount}\")\n                    return active(value + amount)\n                case GetValue(reply_to):\n                    reply_to.tell(value)\n                    return Behaviors.same()\n\n        return Behaviors.receive(receive)\n\n    return active()\n</code></pre> <p>The factory receives the <code>entity_id</code> and returns a <code>Behavior</code>. Casty calls it lazily \u2014 the actor is only created when the first message for that entity arrives.</p>"},{"location":"guides/clustered/#forming-a-cluster","title":"Forming a Cluster","text":"<p>Two <code>ClusteredActorSystem</code> instances. Node 1 starts independently; node 2 joins via <code>seed_nodes</code>:</p> <pre><code>def make_cluster() -&gt; tuple[ClusteredActorSystem, ClusteredActorSystem]:\n    node1 = ClusteredActorSystem(\n        name=\"my-cluster\",\n        host=\"127.0.0.1\",\n        port=25520,\n        node_id=\"node-1\",\n    )\n    node2 = ClusteredActorSystem(\n        name=\"my-cluster\",\n        host=\"127.0.0.1\",\n        port=25521,\n        node_id=\"node-2\",\n        seed_nodes=[(\"127.0.0.1\", 25520)],\n    )\n    return node1, node2\n</code></pre> <p><code>wait_for(2)</code> blocks until both nodes reach <code>MemberStatus.up</code>. In production, each node runs in its own process \u2014 here we run both in one process for simplicity.</p>"},{"location":"guides/clustered/#spawning-sharded-entities","title":"Spawning Sharded Entities","text":"<p>Both nodes spawn the same shard type. The coordinator distributes shards across the cluster:</p> <pre><code>def spawn_sharded(\n    node1: ClusteredActorSystem, node2: ClusteredActorSystem\n) -&gt; tuple[ActorRef[ShardEnvelope[CounterMsg]], ActorRef[ShardEnvelope[CounterMsg]]]:\n    proxy1 = node1.spawn(\n        Behaviors.sharded(counter_entity, num_shards=10), \"counters\"\n    )\n    proxy2 = node2.spawn(\n        Behaviors.sharded(counter_entity, num_shards=10), \"counters\"\n    )\n    return proxy1, proxy2\n</code></pre> <p><code>num_shards=10</code> means entity IDs are hashed into 10 buckets. Each bucket is assigned to one node. You send messages through the local proxy \u2014 routing to the correct node is transparent.</p>"},{"location":"guides/clustered/#routing-and-querying","title":"Routing and Querying","text":"<p>Messages are wrapped in <code>ShardEnvelope(entity_id, message)</code>. The proxy routes based on entity ID:</p> <pre><code>async def send_and_query(\n    node: ClusteredActorSystem, proxy: ActorRef[ShardEnvelope[CounterMsg]]\n) -&gt; None:\n    # Route messages by entity_id \u2014 shards are distributed across nodes\n    print(\"\u2500\u2500 Sending increments \u2500\u2500\")\n    proxy.tell(ShardEnvelope(\"alice\", Increment(10)))\n    proxy.tell(ShardEnvelope(\"alice\", Increment(5)))\n    proxy.tell(ShardEnvelope(\"bob\", Increment(100)))\n    proxy.tell(ShardEnvelope(\"carol\", Increment(42)))\n    await asyncio.sleep(0.5)\n\n    # Query from either node \u2014 routing is transparent\n    print(\"\\n\u2500\u2500 Querying balances \u2500\u2500\")\n    for name in (\"alice\", \"bob\", \"carol\"):\n        value: int = await node.ask(\n            proxy,\n            lambda r, eid=name: ShardEnvelope(eid, GetValue(reply_to=r)),\n            timeout=5.0,\n        )\n        print(f\"  {name}: {value}\")\n</code></pre> <p>Notice: all queries go through <code>proxy1</code> on node 1, but entities may live on node 2. The proxy handles the cross-node routing transparently \u2014 you never need to know which node owns which shard.</p>"},{"location":"guides/clustered/#running-it","title":"Running It","text":"<pre><code>async def main() -&gt; None:\n    node1, node2 = make_cluster()\n\n    async with node1, node2:\n        await node1.wait_for(2)\n        await node2.wait_for(2)\n        print(\"\u2500\u2500 Cluster formed (2 nodes) \u2500\u2500\\n\")\n\n        proxy1, proxy2 = spawn_sharded(node1, node2)\n        await asyncio.sleep(1.0)\n\n        await send_and_query(node1, proxy1)\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Cluster formed (2 nodes) \u2500\u2500\n\n\u2500\u2500 Sending increments \u2500\u2500\n  [alice] 0 + 10 = 10\n  [alice] 10 + 5 = 15\n  [bob] 0 + 100 = 100\n  [carol] 0 + 42 = 42\n\n\u2500\u2500 Querying balances \u2500\u2500\n  alice: 15\n  bob: 100\n  carol: 42\n</code></pre>"},{"location":"guides/clustered/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/06_clustered.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>ClusteredActorSystem</code> extends <code>ActorSystem</code> with cluster membership and shard routing.</li> <li>Seed nodes bootstrap cluster formation \u2014 node 2 connects to node 1's address to join.</li> <li><code>Behaviors.sharded(factory, num_shards=N)</code> distributes entities across nodes by hashing the entity ID.</li> <li><code>ShardEnvelope(entity_id, msg)</code> routes messages to the correct node transparently.</li> <li><code>wait_for(n)</code> blocks until the cluster has enough members \u2014 useful for quorum requirements.</li> </ul>"},{"location":"guides/custom-serialization/","title":"Custom Serialization","text":"<p>In this guide you'll implement a custom serializer using cloudpickle that can handle lambdas and closures. Along the way you'll learn the <code>Serializer</code> protocol, build a working implementation, and prove it roundtrips functions through a real actor.</p>"},{"location":"guides/custom-serialization/#the-serializer-protocol","title":"The Serializer Protocol","text":"<p>Casty uses a <code>Protocol</code>, not an ABC. Any object with these two methods satisfies it:</p> <pre><code>#   class Serializer(Protocol):\n#       def serialize(self, obj: Any) -&gt; bytes: ...\n#       def deserialize(self, data: bytes, *, ref_factory=None) -&gt; Any: ...\n</code></pre> <p><code>serialize</code> converts to bytes. <code>deserialize</code> converts back. The optional <code>ref_factory</code> keyword argument lets the transport layer pass a factory for reconstructing <code>ActorRef</code> objects during deserialization.</p>"},{"location":"guides/custom-serialization/#cloudpickleserializer","title":"CloudpickleSerializer","text":"<p>The implementation is minimal \u2014 cloudpickle does the heavy lifting:</p> <pre><code>class CloudpickleSerializer:\n    \"\"\"Serializer that uses cloudpickle for lambdas and closures.\"\"\"\n\n    def serialize[M](self, obj: M) -&gt; bytes:\n        return cloudpickle.dumps(obj)\n\n    def deserialize[M, R](\n        self,\n        data: bytes,\n        *,\n        ref_factory: Callable[[ActorAddress], ActorRef[R]] | None = None,\n    ) -&gt; M:\n        return cloudpickle.loads(data)  # noqa: S301\n</code></pre> <p>No inheritance, no registration. If it quacks like a <code>Serializer</code>, it is a <code>Serializer</code>.</p>"},{"location":"guides/custom-serialization/#why-cloudpickle","title":"Why Cloudpickle?","text":"<p>Standard pickle can't serialize lambdas defined inside functions. Cloudpickle can:</p> <pre><code>def pickle_vs_cloudpickle() -&gt; None:\n    print(\"\u2500\u2500 Standard pickle vs cloudpickle \u2500\u2500\")\n    try:\n        pickle.dumps(lambda x: x + 1)\n        print(\"  pickle:      OK (unexpected)\")\n    except Exception as e:\n        print(f\"  pickle:      {type(e).__name__}\")\n\n    cloudpickle.dumps(lambda x: x + 1)\n    print(\"  cloudpickle: OK\")\n</code></pre> <p>Output:</p> <pre><code>pickle:      AttributeError\ncloudpickle: OK\n</code></pre>"},{"location":"guides/custom-serialization/#roundtripping-closures","title":"Roundtripping Closures","text":"<p>Cloudpickle captures local variables along with the lambda. <code>multiplier = 3</code> survives the serialize-deserialize roundtrip:</p> <pre><code>def roundtrip_closure(serializer: CloudpickleSerializer) -&gt; None:\n    print(\"\\n\u2500\u2500 Roundtrip a closure \u2500\u2500\")\n    multiplier = 3\n    fn: Callable[[int], int] = lambda x: x * multiplier  # noqa: E731\n\n    data = serializer.serialize(fn)\n    restored_fn = serializer.deserialize(data)\n    print(f\"  restored_fn(10) = {restored_fn(10)}\")\n</code></pre>"},{"location":"guides/custom-serialization/#messages-with-lambdas","title":"Messages with Lambdas","text":"<p>A frozen dataclass carrying a callable roundtrips cleanly:</p> <pre><code>@dataclass(frozen=True)\nclass ApplyFn:\n    fn: Callable[[int], int]\n\n\n@dataclass(frozen=True)\nclass GetValue:\n    reply_to: ActorRef[int]\n\n\ntype ComputeMsg = ApplyFn | GetValue\n</code></pre> <pre><code>def roundtrip_message(serializer: CloudpickleSerializer) -&gt; None:\n    print(\"\\n\u2500\u2500 Roundtrip a message \u2500\u2500\")\n    msg = ApplyFn(fn=lambda x: x + 42)\n    data = serializer.serialize(msg)\n    restored_msg = serializer.deserialize(data)\n    print(f\"  fn(0) = {restored_msg.fn(0)}\")\n</code></pre>"},{"location":"guides/custom-serialization/#actor-applying-serialized-functions","title":"Actor Applying Serialized Functions","text":"<p>The full loop: serialize a message carrying a lambda, deserialize it, and <code>tell()</code> it to an actor that applies the function to its state:</p> <pre><code>def accumulator(value: int = 0) -&gt; Behavior[ComputeMsg]:\n    async def receive(\n        ctx: ActorContext[ComputeMsg], msg: ComputeMsg\n    ) -&gt; Behavior[ComputeMsg]:\n        match msg:\n            case ApplyFn(fn):\n                new_value = fn(value)\n                print(f\"  {value} \u2192 {new_value}\")\n                return accumulator(new_value)\n            case GetValue(reply_to):\n                reply_to.tell(value)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <pre><code>async def actor_with_serialized_fns(serializer: CloudpickleSerializer) -&gt; None:\n    print(\"\\n\u2500\u2500 Actor applying serialized functions \u2500\u2500\")\n    async with ActorSystem() as system:\n        ref: ActorRef[ComputeMsg] = system.spawn(accumulator(), \"calc\")\n\n        fns: list[Callable[[int], int]] = [\n            lambda x: x + 10,\n            lambda x: x * 3,\n            lambda x: x - 5,\n        ]\n        for fn in fns:\n            data = serializer.serialize(ApplyFn(fn=fn))\n            ref.tell(serializer.deserialize(data))\n\n        await asyncio.sleep(0.1)\n\n        value: int = await system.ask(\n            ref, lambda r: GetValue(reply_to=r), timeout=5.0\n        )\n        print(f\"  Final value: {value}\")\n</code></pre> <p>Output:</p> <pre><code>0 \u2192 10\n10 \u2192 30\n30 \u2192 25\nFinal value: 25\n</code></pre>"},{"location":"guides/custom-serialization/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>pip install cloudpickle\ngit clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/08_custom_serialization.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>Serializer</code> is a Protocol \u2014 implement two methods and you're done. No inheritance required.</li> <li>Cloudpickle handles lambdas, closures, and captured variables that standard pickle cannot.</li> <li>Messages carrying callables roundtrip cleanly through cloudpickle serialization.</li> <li>Structural subtyping means your serializer satisfies the protocol without any base class.</li> </ul>"},{"location":"guides/event-sourcing/","title":"Event Sourcing","text":"<p>In this guide you'll build a bank account that persists every transaction as an event. Along the way you'll learn the command-event-state flow, how actors recover from the journal on restart, and how snapshots speed up recovery.</p>"},{"location":"guides/event-sourcing/#the-idea","title":"The Idea","text":"<p>Instead of saving current state (\"balance is 120\"), event sourcing saves what happened (\"deposited 100, deposited 50, withdrew 30\"). State is derived by replaying events. This gives you a full audit trail and the ability to reconstruct any past state.</p>"},{"location":"guides/event-sourcing/#state","title":"State","text":"<p>A frozen dataclass holding the derived state:</p> <pre><code>@dataclass(frozen=True)\nclass AccountState:\n    balance: int = 0\n    transactions: int = 0\n</code></pre>"},{"location":"guides/event-sourcing/#events","title":"Events","text":"<p>Events are facts that already happened. They're persisted to the journal and replayed during recovery:</p> <pre><code>@dataclass(frozen=True)\nclass Deposited:\n    amount: int\n\n\n@dataclass(frozen=True)\nclass Withdrawn:\n    amount: int\n\n\ntype AccountEvent = Deposited | Withdrawn\n</code></pre>"},{"location":"guides/event-sourcing/#commands","title":"Commands","text":"<p>Commands are requests from the outside world. The actor decides whether to accept or reject them:</p> <pre><code>@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n\n@dataclass(frozen=True)\nclass Withdraw:\n    amount: int\n    reply_to: ActorRef[str]\n\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[int]\n\n\ntype AccountCmd = Deposit | Withdraw | GetBalance\n</code></pre> <p>Notice the separation: <code>Deposit</code> (command) vs <code>Deposited</code> (event). The command is the intent; the event is the outcome.</p>"},{"location":"guides/event-sourcing/#event-handler","title":"Event Handler","text":"<p>A pure function that applies one event to the current state. No side effects \u2014 this is called both during normal operation and during recovery:</p> <pre><code>def apply_event(state: AccountState, event: AccountEvent) -&gt; AccountState:\n    match event:\n        case Deposited(amount):\n            return AccountState(\n                balance=state.balance + amount,\n                transactions=state.transactions + 1,\n            )\n        case Withdrawn(amount):\n            return AccountState(\n                balance=state.balance - amount,\n                transactions=state.transactions + 1,\n            )\n</code></pre>"},{"location":"guides/event-sourcing/#command-handler","title":"Command Handler","text":"<p>The command handler decides which events to persist. It returns <code>Behaviors.persisted([...])</code> to queue events, or <code>Behaviors.same()</code> for read-only operations:</p> <pre><code>async def handle_command(\n    ctx: ActorContext[AccountCmd], state: AccountState, cmd: AccountCmd\n) -&gt; Behavior[AccountCmd]:\n    match cmd:\n        case Deposit(amount):\n            print(f\"  Depositing {amount}\")\n            return Behaviors.persisted([Deposited(amount)])\n\n        case Withdraw(amount, reply_to) if amount &lt;= state.balance:\n            print(f\"  Withdrawing {amount}\")\n            reply_to.tell(\"ok\")\n            return Behaviors.persisted([Withdrawn(amount)])\n\n        case Withdraw(amount, reply_to):\n            reply_to.tell(f\"insufficient funds (balance: {state.balance})\")\n            return Behaviors.same()\n\n        case GetBalance(reply_to):\n            reply_to.tell(state.balance)\n            return Behaviors.same()\n</code></pre> <p>Walking through the arms:</p> <ul> <li>Deposit \u2014 always succeeds. Persists a <code>Deposited</code> event.</li> <li>Withdraw (sufficient funds) \u2014 replies \"ok\", persists <code>Withdrawn</code>.</li> <li>Withdraw (insufficient) \u2014 replies with error, persists nothing.</li> <li>GetBalance \u2014 read-only. No events, just a reply.</li> </ul>"},{"location":"guides/event-sourcing/#wiring-it-together","title":"Wiring It Together","text":"<p><code>Behaviors.event_sourced()</code> ties the pieces together:</p> <pre><code>def bank_account(entity_id: str, journal: InMemoryJournal) -&gt; Behavior[AccountCmd]:\n    return Behaviors.event_sourced(\n        entity_id=entity_id,\n        journal=journal,\n        initial_state=AccountState(),\n        on_event=apply_event,\n        on_command=handle_command,\n        snapshot_policy=SnapshotEvery(n_events=5),\n    )\n</code></pre> <p><code>SnapshotEvery(n_events=5)</code> tells the journal to save a full state snapshot after every 5 events. On recovery, the actor loads the snapshot first and only replays events after it \u2014 much faster for entities with long histories.</p>"},{"location":"guides/event-sourcing/#running-it","title":"Running It","text":"<p>The example runs in three phases: transactions, recovery, and snapshot recovery:</p> <pre><code>async def main() -&gt; None:\n    journal = InMemoryJournal()\n\n    # --- Phase 1: build up state ---\n    print(\"\u2500\u2500 Phase 1: transactions \u2500\u2500\")\n    async with ActorSystem(name=\"bank\") as system:\n        account: ActorRef[AccountCmd] = system.spawn(\n            bank_account(\"acct-1\", journal), \"account\"\n        )\n\n        account.tell(Deposit(100))\n        account.tell(Deposit(50))\n        await asyncio.sleep(0.1)\n\n        result = await system.ask(\n            account,\n            lambda r: Withdraw(30, reply_to=r),\n            timeout=5.0,\n        )\n        print(f\"  Withdraw result: {result}\")\n\n        result = await system.ask(\n            account,\n            lambda r: Withdraw(999, reply_to=r),\n            timeout=5.0,\n        )\n        print(f\"  Withdraw result: {result}\")\n\n        balance = await system.ask(\n            account, lambda r: GetBalance(reply_to=r), timeout=5.0\n        )\n        print(f\"  Balance: {balance}\")\n\n    # Show what was persisted\n    events = await journal.load(\"acct-1\")\n    print(f\"\\n\u2500\u2500 Journal has {len(events)} events \u2500\u2500\")\n    for e in events:\n        print(f\"  #{e.sequence_nr}: {e.event}\")\n\n    # --- Phase 2: recover from journal ---\n    print(\"\\n\u2500\u2500 Phase 2: recovery \u2500\u2500\")\n    async with ActorSystem(name=\"bank\") as system:\n        account = system.spawn(bank_account(\"acct-1\", journal), \"account\")\n        await asyncio.sleep(0.1)\n\n        balance = await system.ask(\n            account, lambda r: GetBalance(reply_to=r), timeout=5.0\n        )\n        print(f\"  Recovered balance: {balance}\")\n\n    # --- Phase 3: demonstrate snapshot recovery ---\n    print(\"\\n\u2500\u2500 Phase 3: snapshot recovery \u2500\u2500\")\n    snap_journal = InMemoryJournal()\n\n    async with ActorSystem(name=\"bank\") as system:\n        account = system.spawn(\n            bank_account(\"acct-2\", snap_journal), \"account\"\n        )\n        for _ in range(7):\n            account.tell(Deposit(10))\n        await asyncio.sleep(0.2)\n\n        balance = await system.ask(\n            account, lambda r: GetBalance(reply_to=r), timeout=5.0\n        )\n        print(f\"  Balance after 7 deposits: {balance}\")\n\n    snapshot = await snap_journal.load_snapshot(\"acct-2\")\n    events = await snap_journal.load(\"acct-2\")\n    print(f\"  Snapshot at seq #{snapshot.sequence_nr}: {snapshot.state}\" if snapshot else \"  No snapshot\")\n    print(f\"  Journal has {len(events)} events total\")\n    print(f\"  Recovery replays only {len(events) - (snapshot.sequence_nr if snapshot else 0)} events\")\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Phase 1: transactions \u2500\u2500\n  Depositing 100\n  Depositing 50\n  Withdrawing 30\n  Withdraw result: ok\n  Withdraw result: insufficient funds (balance: 120)\n  Balance: 120\n\n\u2500\u2500 Journal has 3 events \u2500\u2500\n  #1: Deposited(amount=100)\n  #2: Deposited(amount=50)\n  #3: Withdrawn(amount=30)\n\n\u2500\u2500 Phase 2: recovery \u2500\u2500\n  Recovered balance: 120\n\n\u2500\u2500 Phase 3: snapshot recovery \u2500\u2500\n  Depositing 10\n  ...\n  Balance after 7 deposits: 70\n  Snapshot at seq #5: AccountState(balance=50, transactions=5)\n  Journal has 7 events total\n  Recovery replays only 2 events\n</code></pre> <p>Phase 2 shows recovery: a new actor with the same <code>entity_id</code> replays all events from the journal and arrives at the same state. Phase 3 shows snapshot optimization: after 7 deposits, a snapshot was taken at event #5. On recovery, only 2 events (6 and 7) need replaying.</p>"},{"location":"guides/event-sourcing/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/05_event_sourcing.py\n</code></pre> <p>What you learned:</p> <ul> <li>Commands are requests; events are facts. Commands may be rejected; events are always applied.</li> <li><code>on_event</code> is a pure function that applies events to state \u2014 used for both normal operation and recovery.</li> <li><code>on_command</code> decides which events to persist. Return <code>Behaviors.persisted([...])</code> or <code>Behaviors.same()</code>.</li> <li>Recovery replays all events from the journal, reconstructing state automatically.</li> <li>Snapshots speed up recovery by checkpointing state at regular intervals.</li> </ul>"},{"location":"guides/job-executor/","title":"Job Executor","text":"<p>In this guide you'll build a distributed job executor that runs arbitrary Python functions across a 10-node Docker cluster. Along the way you'll learn how workers register via the receptionist, how <code>CloudPickleSerializer</code> sends lambdas over the wire, and how runtime <code>pip install</code> provisions dependencies on remote nodes.</p> <p>The end result: 10 eigenvalue computations on 200x200 matrices, each on a different node, completing in the wall-clock time of a single job.</p>"},{"location":"guides/job-executor/#messages","title":"Messages","text":"<p>The job protocol has two request-reply pairs \u2014 one for installing dependencies, one for running functions:</p> <pre><code>JOB_WORKER_KEY: ServiceKey[Any] = ServiceKey(\"job-worker\")\n\n\n@dataclass(frozen=True)\nclass InstallDeps:\n    packages: tuple[str, ...]\n    reply_to: ActorRef[DepsInstalled | DepsFailed]\n\n\n@dataclass(frozen=True)\nclass DepsInstalled:\n    node: str\n\n\n@dataclass(frozen=True)\nclass DepsFailed:\n    node: str\n    error: str\n\n\n@dataclass(frozen=True)\nclass RunJob:\n    fn: Callable[[], Any]\n    reply_to: ActorRef[JobResult]\n\n\n@dataclass(frozen=True)\nclass JobResult:\n    node: str\n    value: Any | None = None\n    error: str | None = None\n</code></pre> <p>Every response carries <code>node: str</code> so the client can print which worker handled each job.</p>"},{"location":"guides/job-executor/#worker-actor","title":"Worker Actor","text":"<p>The worker handles two message types. <code>InstallDeps</code> shells out to <code>pip install</code> via subprocess. <code>RunJob</code> executes the received function in a thread to avoid blocking the event loop:</p> <pre><code>type WorkerMsg = InstallDeps | RunJob | Shutdown\n\n\ndef job_worker(node_name: str, done: asyncio.Event) -&gt; Behavior[WorkerMsg]:\n    async def receive(ctx: Any, msg: WorkerMsg) -&gt; Behavior[WorkerMsg]:\n        match msg:\n            case InstallDeps(packages=packages, reply_to=reply_to):\n                log.info(\"[%s] installing %s\", node_name, \", \".join(packages))\n                proc = await asyncio.create_subprocess_exec(\n                    \"uv\", \"pip\", \"install\", \"--quiet\", *packages,\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE,\n                )\n                _, stderr = await proc.communicate()\n                if proc.returncode == 0:\n                    reply_to.tell(DepsInstalled(node=node_name))\n                else:\n                    reply_to.tell(DepsFailed(node=node_name, error=stderr.decode()))\n                return Behaviors.same()\n\n            case RunJob(fn=fn, reply_to=reply_to):\n                try:\n                    result = await asyncio.to_thread(fn)\n                    reply_to.tell(JobResult(node=node_name, value=result))\n                except Exception:\n</code></pre> <p>The function <code>fn</code> arrives as a cloudpickle-serialized closure \u2014 it can reference any libraries installed on the node, even if they weren't present when the cluster started.</p>"},{"location":"guides/job-executor/#node-entrypoint","title":"Node Entrypoint","text":"<p>Each node runs the same script. CLI args control port, seed node, and expected cluster size. The worker is wrapped with <code>Behaviors.discoverable</code> so the receptionist advertises it to the cluster:</p> <pre><code>    tuple(\n        (s.split(\":\")[0], int(s.split(\":\")[1]))\n        for s in args.seed.split(\",\")\n    )\n    if args.seed\n    else ()\n)\nnode_name = f\"{host}:{args.port}\"\n\ndone = asyncio.Event()\n\nasync with ClusteredActorSystem(\n    name=\"job-cluster\",\n    host=host,\n    port=args.port,\n    node_id=node_name,\n    seed_nodes=seed_nodes,\n    bind_host=args.bind_host,\n    serializer=CloudPickleSerializer(),\n) as system:\n    system.spawn(\n</code></pre> <p><code>wait_for(args.nodes)</code> blocks until all 10 nodes are <code>up</code> before the node is ready.</p>"},{"location":"guides/job-executor/#client","title":"Client","text":"<p>The client connects from outside the cluster via <code>ClusterClient</code>. It discovers workers through the receptionist, installs numpy on all of them, then fans out 10 jobs in parallel:</p> <pre><code>async with ClusterClient(\n    contact_points=contact_points,\n    system_name=\"job-cluster\",\n    serializer=CloudPickleSerializer(),\n) as client:\n    # 1. Discover workers\n    print(f\"\\n{BOLD}Waiting for {args.workers} workers...{RESET}\")\n    workers: list[ServiceInstance[Any]] = []\n    while len(workers) &lt; args.workers:\n        listing: Listing[Any] = client.lookup(JOB_WORKER_KEY)\n        workers = sorted(listing.instances, key=lambda w: str(w.node))\n        if len(workers) &lt; args.workers:\n            await asyncio.sleep(0.5)\n</code></pre> <p>Runtime dependency installation \u2014 <code>client.ask</code> sends <code>InstallDeps</code> to each worker and waits for confirmation:</p> <pre><code># 2. Install numpy on all workers\nprint(f\"{BOLD}Installing numpy on {len(workers)} workers...{RESET}\")\ninstall_tasks = [\n    client.ask(\n        w.ref,\n        lambda reply_to, _w=w: InstallDeps(\n            packages=(\"numpy\",), reply_to=reply_to\n        ),\n        timeout=120.0,\n    )\n    for w in workers\n]\ninstall_results = await asyncio.gather(*install_tasks)\nfor result in install_results:\n    match result:\n        case DepsInstalled(node=node):\n            print(f\"  {GREEN}\u2713{RESET} {node}\")\n        case DepsFailed(node=node, error=error):\n</code></pre> <p>The job factory creates a closure that imports numpy inside the function. This is key \u2014 the import happens on the remote node after <code>pip install</code>, not on the client:</p> <pre><code># 3. Submit jobs in parallel\ndef make_job(job_id: int):\n    def compute():\n        import numpy as np\n\n        rng = np.random.default_rng(seed=job_id)\n        matrix = rng.standard_normal((200, 200))\n        eigenvalues = np.linalg.eigvals(matrix)\n        top3 = sorted(np.abs(eigenvalues), reverse=True)[:3]\n        return {\n            \"job_id\": job_id,\n            \"matrix_shape\": (200, 200),\n            \"top_eigenvalues\": [round(float(v), 2) for v in top3],\n        }\n</code></pre> <p>Fan-out: 10 jobs dispatched simultaneously via <code>asyncio.gather</code>, one per worker:</p> <pre><code>print(f\"\\n{BOLD}Submitting {len(workers)} jobs in parallel...{RESET}\")\nt0 = time.monotonic()\n\njob_tasks = [\n    client.ask(\n        workers[i].ref,\n        lambda reply_to, _i=i: RunJob(fn=make_job(_i), reply_to=reply_to),\n        timeout=120.0,\n    )\n    for i in range(len(workers))\n]\njob_results: list[JobResult] = await asyncio.gather(*job_tasks)\n\nelapsed = time.monotonic() - t0\nfor r in job_results:\n    match r:\n        case JobResult(node=node, value=value) if value is not None:\n            v = value\n            print(\n                f\"  {CYAN}job-{v['job_id']}{RESET} \u2192 {node}  \"\n                f\"eigenvalues: {v['top_eigenvalues']}\"\n            )\n        case JobResult(node=node, error=error):\n            print(f\"  {RED}job{RESET} \u2192 {node}  ERROR: {error}\")\n</code></pre>"},{"location":"guides/job-executor/#docker-setup","title":"Docker Setup","text":"<p>The <code>docker-compose.yml</code> defines 12 services: one seed node, nine workers, and one client. All worker nodes share the same image and entrypoint \u2014 only the hostname differs:</p> <pre><code>x-node: &amp;node\n  build:\n    context: ../..\n    dockerfile: examples/18_job_executor/Dockerfile.node\n\nservices:\n  seed:\n    &lt;&lt;: *node\n    hostname: seed\n    command:\n      [\n        \"--port\", \"25520\",\n        \"--host\", \"auto\",\n        \"--bind-host\", \"0.0.0.0\",\n        \"--seed\", \"seed:25520\",\n        \"--nodes\", \"10\",\n      ]\n\n  worker-1:\n</code></pre> <p>The client depends on all workers and connects to the seed:</p> <pre><code>hostname: client\ncommand: [\"--contact\", \"seed:25520\", \"--workers\", \"10\"]\ndepends_on:\n  - seed\n  - worker-1\n  - worker-2\n  - worker-3\n  - worker-4\n  - worker-5\n  - worker-6\n  - worker-7\n  - worker-8\n  - worker-9\n</code></pre>"},{"location":"guides/job-executor/#running-it","title":"Running It","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty/examples/18_job_executor\ndocker compose up --build\n</code></pre> <p>Expected output:</p> <pre><code>Waiting for 10 workers...\n  found 10 workers\n\nInstalling numpy on 10 workers...\n  \u2713 seed:25520\n  \u2713 worker-1:25520\n  \u2713 worker-2:25520\n  ...\n\nSubmitting 10 jobs in parallel...\n  job-0 \u2192 seed:25520       eigenvalues: [14.23, 14.18, 14.05]\n  job-1 \u2192 worker-1:25520   eigenvalues: [14.21, 14.15, 14.09]\n  ...\n\nAll 10 jobs completed in 0.32s\n</code></pre> <p>What you learned:</p> <ul> <li><code>Behaviors.discoverable</code> auto-registers actors with the receptionist for cluster-wide service discovery.</li> <li><code>CloudPickleSerializer</code> serializes lambdas and closures over the wire \u2014 standard pickle can't do this.</li> <li>Runtime <code>pip install</code> via subprocess lets workers provision dependencies on demand, without baking them into the Docker image.</li> <li><code>asyncio.gather</code> + <code>client.ask</code> fans out work across nodes in parallel \u2014 total time approaches single-job time.</li> <li><code>ClusterClient</code> connects from outside the cluster, discovers services, and sends messages without cluster membership.</li> </ul>"},{"location":"guides/pipe-to-self/","title":"Pipe to Self","text":"<p>In this guide you'll build a weather station that fetches forecasts from an external API without blocking its mailbox. Along the way you'll learn how <code>pipe_to_self</code> turns async operations into messages, how the actor stays responsive during slow I/O, and how to handle both success and failure.</p>"},{"location":"guides/pipe-to-self/#the-problem","title":"The Problem","text":"<p>Actors process one message at a time. If your handler awaits a slow network call directly, the mailbox blocks \u2014 no other messages get through until the I/O completes. <code>pipe_to_self</code> solves this by running the coroutine in the background and delivering the result as a regular message.</p>"},{"location":"guides/pipe-to-self/#external-api","title":"External API","text":"<p>A simulated weather service with a 500ms delay:</p> <pre><code>async def fetch_weather(city: str) -&gt; dict[str, str | float]:\n    \"\"\"Simulate a slow network call to a weather API.\"\"\"\n    await asyncio.sleep(0.5)\n    forecasts: dict[str, dict[str, str | float]] = {\n        \"london\": {\"city\": \"London\", \"temp\": 12.5, \"sky\": \"cloudy\"},\n        \"tokyo\": {\"city\": \"Tokyo\", \"temp\": 28.0, \"sky\": \"sunny\"},\n        \"paris\": {\"city\": \"Paris\", \"temp\": 18.3, \"sky\": \"rainy\"},\n    }\n    if city.lower() not in forecasts:\n        raise ValueError(f\"Unknown city: {city}\")\n    return forecasts[city.lower()]\n</code></pre>"},{"location":"guides/pipe-to-self/#messages","title":"Messages","text":"<p>The actor needs messages for requests, results, errors, and a ping to prove responsiveness:</p> <pre><code>@dataclass(frozen=True)\nclass FetchWeather:\n    city: str\n\n\n@dataclass(frozen=True)\nclass WeatherResult:\n    city: str\n    temp: float\n    sky: str\n\n\n@dataclass(frozen=True)\nclass WeatherFailed:\n    city: str\n    error: str\n\n\n@dataclass(frozen=True)\nclass Ping:\n    reply_to: ActorRef[str]\n\n\n@dataclass(frozen=True)\nclass GetForecasts:\n    reply_to: ActorRef[tuple[WeatherResult, ...]]\n\n\ntype WeatherMsg = FetchWeather | WeatherResult | WeatherFailed | Ping | GetForecasts\n</code></pre> <p>Notice <code>WeatherResult</code> and <code>WeatherFailed</code> \u2014 these are the messages the actor sends to itself when the background fetch completes or fails. The caller never sees them.</p>"},{"location":"guides/pipe-to-self/#the-behavior","title":"The Behavior","text":"<pre><code>def weather_station(\n    forecasts: tuple[WeatherResult, ...] = (),\n) -&gt; Behavior[WeatherMsg]:\n    async def receive(\n        ctx: ActorContext[WeatherMsg], msg: WeatherMsg\n    ) -&gt; Behavior[WeatherMsg]:\n        match msg:\n            case FetchWeather(city):\n                ctx.pipe_to_self(\n                    fetch_weather(city),\n                    lambda data: WeatherResult(\n                        city=str(data[\"city\"]),\n                        temp=float(data[\"temp\"]),\n                        sky=str(data[\"sky\"]),\n                    ),\n                    on_failure=lambda exc: WeatherFailed(\n                        city=city, error=str(exc)\n                    ),\n                )\n                return Behaviors.same()\n\n            case WeatherResult() as result:\n                print(f\"  Weather in {result.city}: {result.temp}C, {result.sky}\")\n                return weather_station((*forecasts, result))\n\n            case WeatherFailed(city, error):\n                print(f\"  Failed to fetch {city}: {error}\")\n                return Behaviors.same()\n\n            case Ping(reply_to):\n                reply_to.tell(\"pong\")\n                return Behaviors.same()\n\n            case GetForecasts(reply_to):\n                reply_to.tell(forecasts)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p>Walking through each arm:</p> <ul> <li>FetchWeather \u2014 calls <code>ctx.pipe_to_self()</code> with three arguments: the coroutine, a mapper that converts the raw API response into a <code>WeatherResult</code> message, and an <code>on_failure</code> handler that wraps exceptions into <code>WeatherFailed</code>. Returns <code>Behaviors.same()</code> immediately \u2014 the mailbox is free.</li> <li>WeatherResult \u2014 the piped result arrives as a normal message. The actor appends it to state via behavior recursion.</li> <li>WeatherFailed \u2014 the error handler fired. Log it and move on.</li> <li>Ping \u2014 proves the mailbox is responsive even while fetches are in-flight.</li> </ul>"},{"location":"guides/pipe-to-self/#running-it","title":"Running It","text":"<pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        station: ActorRef[WeatherMsg] = system.spawn(\n            weather_station(), \"weather-station\"\n        )\n\n        # Fire off three fetch requests \u2014 all run concurrently in the background\n        print(\"\u2500\u2500 Requesting forecasts (non-blocking) \u2500\u2500\")\n        station.tell(FetchWeather(\"london\"))\n        station.tell(FetchWeather(\"tokyo\"))\n        station.tell(FetchWeather(\"narnia\"))  # will fail\n\n        # The mailbox is NOT blocked \u2014 Ping is processed immediately\n        reply = await system.ask(\n            station, lambda r: Ping(reply_to=r), timeout=1.0\n        )\n        print(f\"  Ping reply: {reply} (mailbox is responsive!)\")\n\n        # Wait for all fetches to complete\n        await asyncio.sleep(1.0)\n\n        # Read the collected results\n        results = await system.ask(\n            station, lambda r: GetForecasts(reply_to=r), timeout=1.0\n        )\n        print(f\"\\n\u2500\u2500 Collected {len(results)} forecasts \u2500\u2500\")\n        for r in results:\n            print(f\"  {r.city}: {r.temp}C, {r.sky}\")\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Requesting forecasts (non-blocking) \u2500\u2500\n  Ping reply: pong (mailbox is responsive!)\n  Weather in London: 12.5C, cloudy\n  Weather in Tokyo: 28.0C, sunny\n  Failed to fetch narnia: Unknown city: narnia\n\n\u2500\u2500 Collected 2 forecasts \u2500\u2500\n  London: 12.5C, cloudy\n  Tokyo: 28.0C, sunny\n</code></pre> <p>Notice: the <code>Ping</code> reply arrives before any weather results. The three fetches are running concurrently in the background, but the mailbox keeps processing other messages.</p>"},{"location":"guides/pipe-to-self/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/04_pipe_to_self.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>pipe_to_self</code> runs a coroutine in the background and delivers the result as a message \u2014 no mailbox blocking.</li> <li>Mappers convert raw results into typed messages: <code>lambda data: WeatherResult(...)</code>.</li> <li><code>on_failure</code> converts exceptions into messages so the actor can handle errors as part of its normal message loop.</li> <li>Multiple <code>pipe_to_self</code> calls run concurrently \u2014 the actor stays responsive to other messages.</li> </ul>"},{"location":"guides/shopping-cart/","title":"Shopping Cart","text":"<p>In this guide you'll build a shopping cart that expires if abandoned. Along the way you'll learn how behavior recursion models state, how terminal behaviors protect invariants, and how the scheduler handles timeouts.</p>"},{"location":"guides/shopping-cart/#messages","title":"Messages","text":"<p>The cart uses frozen dataclasses for all messages, plus a value object for items and a result type for receipts:</p> <pre><code>@dataclass(frozen=True)\nclass Item:\n    name: str\n    price: float\n\n\n@dataclass(frozen=True)\nclass Receipt:\n    items: tuple[Item, ...]\n    total: float\n\n\n@dataclass(frozen=True)\nclass AddItem:\n    item: Item\n\n\n@dataclass(frozen=True)\nclass RemoveItem:\n    name: str\n\n\n@dataclass(frozen=True)\nclass GetTotal:\n    reply_to: ActorRef[float]\n\n\n@dataclass(frozen=True)\nclass Checkout:\n    reply_to: ActorRef[Receipt]\n\n\n@dataclass(frozen=True)\nclass CartExpired:\n    pass\n\n\ntype CartMsg = AddItem | RemoveItem | GetTotal | Checkout | CartExpired\n</code></pre> <p><code>Item</code> is a value object. <code>Receipt</code> is the checkout result returned to the caller. The rest are commands the cart understands. Notice <code>CartExpired</code> carries no data \u2014 it's a signal sent by the scheduler when the timeout fires. The cart never creates it directly.</p>"},{"location":"guides/shopping-cart/#the-empty-cart","title":"The Empty Cart","text":"<p>An empty cart only accepts <code>AddItem</code>. Everything else is ignored:</p> <pre><code>type SchedulerRef = ActorRef[ScheduleOnce | CancelSchedule]\n\n\ndef empty_cart(scheduler_ref: SchedulerRef) -&gt; Behavior[CartMsg]:\n    async def receive(\n        ctx: ActorContext[CartMsg], msg: CartMsg\n    ) -&gt; Behavior[CartMsg]:\n        match msg:\n            case AddItem(item):\n                scheduler_ref.tell(\n                    ScheduleOnce(\n                        key=\"cart-timeout\",\n                        target=ctx.self,\n                        message=CartExpired(),\n                        delay=5.0,\n                    )\n                )\n                return active_cart(scheduler_ref, {item.name: item})\n            case _:\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p>When the first item arrives, two things happen: we schedule a 5-second timeout via <code>ScheduleOnce</code>, and we transition to <code>active_cart</code> with the item in our dict. The <code>SchedulerRef</code> type alias keeps the signatures readable.</p>"},{"location":"guides/shopping-cart/#the-active-cart","title":"The Active Cart","text":"<p>This is where the interesting logic lives. Each match arm handles one command:</p> <pre><code>def active_cart(\n    scheduler_ref: SchedulerRef, items: dict[str, Item]\n) -&gt; Behavior[CartMsg]:\n    async def receive(\n        ctx: ActorContext[CartMsg], msg: CartMsg\n    ) -&gt; Behavior[CartMsg]:\n        match msg:\n            case AddItem(item):\n                return active_cart(scheduler_ref, {**items, item.name: item})\n\n            case RemoveItem(name) if name in items:\n                remaining = {k: v for k, v in items.items() if k != name}\n                if remaining:\n                    return active_cart(scheduler_ref, remaining)\n                scheduler_ref.tell(CancelSchedule(key=\"cart-timeout\"))\n                return empty_cart(scheduler_ref)\n\n            case GetTotal(reply_to):\n                reply_to.tell(sum(item.price for item in items.values()))\n                return Behaviors.same()\n\n            case Checkout(reply_to):\n                scheduler_ref.tell(CancelSchedule(key=\"cart-timeout\"))\n                receipt = Receipt(\n                    items=tuple(items.values()),\n                    total=sum(item.price for item in items.values()),\n                )\n                reply_to.tell(receipt)\n                return checked_out()\n\n            case CartExpired():\n                print(\"Cart expired \u2014 items discarded\")\n                return expired()\n\n            case _:\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p>Walking through each arm:</p> <ul> <li>AddItem \u2014 behavior recursion with a new dict (<code>{**items, item.name: item}</code>). This is the state transition. No mutation, no <code>self.items[name] = item</code>.</li> <li>RemoveItem \u2014 filters out the item. If the cart becomes empty, cancels the timer and transitions back to <code>empty_cart</code>.</li> <li>GetTotal \u2014 request-reply: computes the sum, sends it to <code>reply_to</code>, stays in the same behavior.</li> <li>Checkout \u2014 cancels the timer, builds a <code>Receipt</code>, replies, and transitions to <code>checked_out</code>. The cart is done.</li> <li>CartExpired \u2014 the timer fired. Transitions to <code>expired</code>. No cleanup needed \u2014 the behavior change is the cleanup.</li> </ul>"},{"location":"guides/shopping-cart/#terminal-states","title":"Terminal States","text":"<p>Once a cart is checked out or expired, it ignores all messages:</p> <pre><code>def checked_out() -&gt; Behavior[CartMsg]:\n    return Behaviors.ignore()\n\n\ndef expired() -&gt; Behavior[CartMsg]:\n    return Behaviors.ignore()\n</code></pre> <p><code>Behaviors.ignore()</code> accepts any message and does nothing. A checked-out cart can't accept items because the behavior simply doesn't handle them. The behavior is the state.</p>"},{"location":"guides/shopping-cart/#wiring-it-up","title":"Wiring It Up","text":"<p>The <code>shopping_cart</code> function ties everything together with <code>Behaviors.setup()</code>:</p> <pre><code>def shopping_cart() -&gt; Behavior[CartMsg]:\n    async def setup(ctx: ActorContext[CartMsg]) -&gt; Behavior[CartMsg]:\n        scheduler_ref = ctx.spawn(scheduler(), \"scheduler\")\n        return empty_cart(scheduler_ref)\n\n    return Behaviors.setup(setup)\n</code></pre> <p><code>Behaviors.setup()</code> gives us an <code>ActorContext</code> before the first message arrives. We use it to spawn a scheduler as a child actor and pass its ref to <code>empty_cart</code>. Because the scheduler is a child, it shares the cart's lifecycle \u2014 when the cart stops, the scheduler stops with it.</p>"},{"location":"guides/shopping-cart/#running-it","title":"Running It","text":"<p>The <code>main()</code> function spawns two carts: one that checks out successfully, and one that gets abandoned:</p> <pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        # Cart 1: successful checkout\n        cart1 = system.spawn(shopping_cart(), \"cart-1\")\n        cart1.tell(AddItem(Item(\"keyboard\", 75.0)))\n        cart1.tell(AddItem(Item(\"mouse\", 25.0)))\n        await asyncio.sleep(0.1)\n\n        total: float = await system.ask(\n            cart1, lambda r: GetTotal(reply_to=r), timeout=5.0\n        )\n        print(f\"Total: ${total:.2f}\")\n\n        receipt: Receipt = await system.ask(\n            cart1, lambda r: Checkout(reply_to=r), timeout=5.0\n        )\n        print(f\"Receipt: {len(receipt.items)} items, ${receipt.total:.2f}\")\n\n        # Cart 2: abandonment / expiration\n        cart2 = system.spawn(shopping_cart(), \"cart-2\")\n        cart2.tell(AddItem(Item(\"monitor\", 300.0)))\n        print(\"Waiting for cart-2 to expire...\")\n        await asyncio.sleep(6.0)\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>Total: $100.00\nReceipt: 2 items, $100.00\nWaiting for cart-2 to expire...\nCart expired \u2014 items discarded\n</code></pre>"},{"location":"guides/shopping-cart/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/02_shopping_cart.py\n</code></pre> <p>What you learned:</p> <ul> <li>Behavior recursion passes new state as function arguments \u2014 no mutation needed.</li> <li>Terminal behaviors (<code>checked_out</code>, <code>expired</code>) protect invariants by construction \u2014 a checked-out cart can't accept items because the behavior simply doesn't handle them.</li> <li>The scheduler sends timed messages without blocking the actor's mailbox.</li> <li>Child actors (the scheduler) share their parent's lifecycle \u2014 when the cart stops, the scheduler stops.</li> </ul>"},{"location":"guides/stateful-companion/","title":"Stateful Companion","text":"<p>In this guide you'll build a circuit breaker \u2014 an immutable companion object that an actor uses to protect against cascading failures. Along the way you'll learn how to extract business rules into plain frozen dataclasses whose methods work as a constant fold: every call returns the result plus the updated state.</p>"},{"location":"guides/stateful-companion/#the-problem","title":"The Problem","text":"<p>Sometimes an actor's message handler grows complex \u2014 rate limiting, failure detection, metrics collection. You can keep everything inline, but if you'd rather extract that logic into a reusable, testable object, that object needs state. In a mutable world you'd reach for a class with internal dicts. In Casty, mutation is the enemy.</p> <p>The stateful companion is one way to handle this: a frozen dataclass whose every method returns <code>tuple[result, new_self]</code>. The actor carries it as a behavior parameter and does behavior recursion as usual \u2014 the companion is just another piece of immutable state.</p>"},{"location":"guides/stateful-companion/#the-companion","title":"The Companion","text":"<p>A circuit breaker has three states: closed (normal operation), open (all calls rejected), and half-open (one trial call allowed). The <code>CircuitBreaker</code> companion tracks failure counts and timestamps, all as immutable fields:</p> <pre><code>class CircuitState(Enum):\n    closed = \"closed\"\n    open = \"open\"\n    half_open = \"half_open\"\n\n\n@dataclass(frozen=True)\nclass CircuitBreaker:\n    failure_threshold: int\n    reset_timeout: float\n    state: CircuitState = CircuitState.closed\n    failure_count: int = 0\n    last_failure_time: float = 0.0\n</code></pre> <p>The <code>allow</code> method decides whether a call can proceed. When the circuit is open but the reset timeout has elapsed, it transitions to half-open:</p> <pre><code>def allow(self, now: float) -&gt; tuple[bool, CircuitBreaker]:\n    match self.state:\n        case CircuitState.closed:\n            return True, self\n\n        case CircuitState.open if now - self.last_failure_time &gt;= self.reset_timeout:\n            new = CircuitBreaker(\n                failure_threshold=self.failure_threshold,\n                reset_timeout=self.reset_timeout,\n                state=CircuitState.half_open,\n                failure_count=self.failure_count,\n                last_failure_time=self.last_failure_time,\n            )\n            return True, new\n\n        case CircuitState.open:\n            return False, self\n\n        case CircuitState.half_open:\n            return True, self\n</code></pre> <p>After the call completes, the actor reports the outcome. <code>record_success</code> resets the breaker to closed. <code>record_failure</code> increments the counter and trips the circuit when it hits the threshold:</p> <pre><code>def record_success(self) -&gt; tuple[None, CircuitBreaker]:\n    return None, CircuitBreaker(\n        failure_threshold=self.failure_threshold,\n        reset_timeout=self.reset_timeout,\n        state=CircuitState.closed,\n        failure_count=0,\n        last_failure_time=0.0,\n    )\n\ndef record_failure(self, now: float) -&gt; tuple[None, CircuitBreaker]:\n    new_count = self.failure_count + 1\n    new_state = (\n        CircuitState.open if new_count &gt;= self.failure_threshold else self.state\n    )\n    return None, CircuitBreaker(\n        failure_threshold=self.failure_threshold,\n        reset_timeout=self.reset_timeout,\n        state=new_state,\n        failure_count=new_count,\n        last_failure_time=now,\n    )\n</code></pre> <p>Notice: every method returns <code>tuple[result, CircuitBreaker]</code>. Even <code>status</code>, which doesn't change state, follows the same signature \u2014 it returns <code>self</code> unchanged:</p> <pre><code>def status(self) -&gt; tuple[CircuitState, CircuitBreaker]:\n    return self.state, self\n</code></pre> <p>This uniformity means the actor never has to wonder whether a method changed the companion. It always unpacks <code>result, new_companion</code> and moves on.</p>"},{"location":"guides/stateful-companion/#messages","title":"Messages","text":"<p>The caller handles two messages: service calls and status queries:</p> <pre><code>@dataclass(frozen=True)\nclass CallResult:\n    success: bool\n    message: str\n\n\n@dataclass(frozen=True)\nclass Call:\n    should_fail: bool\n    reply_to: ActorRef[CallResult]\n\n\n@dataclass(frozen=True)\nclass GetStatus:\n    reply_to: ActorRef[CircuitState]\n\n\ntype CallerMsg = Call | GetStatus\n</code></pre>"},{"location":"guides/stateful-companion/#using-it-in-an-actor","title":"Using It in an Actor","text":"<p>The <code>service_caller</code> actor is thin \u2014 it checks the breaker, makes the call, records the outcome, and transitions with the updated breaker:</p> <pre><code>def service_caller(breaker: CircuitBreaker) -&gt; Behavior[CallerMsg]:\n    async def receive(\n        _ctx: ActorContext[CallerMsg], msg: CallerMsg\n    ) -&gt; Behavior[CallerMsg]:\n        match msg:\n            case Call(should_fail=should_fail, reply_to=reply_to):\n                now = time.monotonic()\n                allowed, new_breaker = breaker.allow(now)\n\n                if not allowed:\n                    reply_to.tell(CallResult(success=False, message=\"circuit open\"))\n                    return service_caller(new_breaker)\n\n                if should_fail:\n                    _, new_breaker = new_breaker.record_failure(now)\n                    reply_to.tell(CallResult(success=False, message=\"service error\"))\n                else:\n                    _, new_breaker = new_breaker.record_success()\n                    reply_to.tell(CallResult(success=True, message=\"ok\"))\n\n                return service_caller(new_breaker)\n\n            case GetStatus(reply_to=reply_to):\n                state, _ = breaker.status()\n                reply_to.tell(state)\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p>The circuit breaker state machine (closed/open/half-open, failure counting, timeout tracking) lives entirely in the companion. The actor just asks questions and reports results.</p>"},{"location":"guides/stateful-companion/#running-it","title":"Running It","text":"<p>The <code>main()</code> function trips the breaker with failures, observes it blocking calls, waits for the reset timeout, and watches it recover:</p> <pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        ref = system.spawn(\n            service_caller(\n                CircuitBreaker(failure_threshold=3, reset_timeout=1.0)\n            ),\n            \"caller\",\n        )\n\n        print(\"\u2500\u2500 Sending 3 failures to trip the breaker \u2500\u2500\")\n        for i in range(3):\n            result: CallResult = await system.ask(\n                ref, lambda r: Call(should_fail=True, reply_to=r), timeout=5.0\n            )\n            print(f\"  Call {i + 1}: {result.message}\")\n\n        status: CircuitState = await system.ask(\n            ref, lambda r: GetStatus(reply_to=r), timeout=5.0\n        )\n        print(f\"  Circuit state: {status.value}\")\n\n        print(\"\\n\u2500\u2500 Calling while circuit is open \u2500\u2500\")\n        result = await system.ask(\n            ref, lambda r: Call(should_fail=False, reply_to=r), timeout=5.0\n        )\n        print(f\"  Call: {result.message}\")\n\n        print(\"\\n\u2500\u2500 Waiting for reset timeout (1s) \u2500\u2500\")\n        await asyncio.sleep(1.1)\n\n        print(\"\\n\u2500\u2500 Calling after timeout (half-open) \u2500\u2500\")\n        result = await system.ask(\n            ref, lambda r: Call(should_fail=False, reply_to=r), timeout=5.0\n        )\n        print(f\"  Call: {result.message}\")\n\n        status = await system.ask(\n            ref, lambda r: GetStatus(reply_to=r), timeout=5.0\n        )\n        print(f\"  Circuit state: {status.value}\")\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Sending 3 failures to trip the breaker \u2500\u2500\n  Call 1: service error\n  Call 2: service error\n  Call 3: service error\n  Circuit state: open\n\n\u2500\u2500 Calling while circuit is open \u2500\u2500\n  Call: circuit open\n\n\u2500\u2500 Waiting for reset timeout (1s) \u2500\u2500\n\n\u2500\u2500 Calling after timeout (half-open) \u2500\u2500\n  Call: ok\n  Circuit state: closed\n</code></pre> <p>Three failures trip the breaker open. The next call is rejected immediately \u2014 the companion said no. After the 1-second timeout the breaker moves to half-open, the trial call succeeds, and the circuit closes again. The actor never mutated anything.</p>"},{"location":"guides/stateful-companion/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/09_stateful_companion.py\n</code></pre> <p>What you learned:</p> <ul> <li>Stateful companions are frozen dataclasses whose methods return <code>tuple[result, new_self]</code> \u2014 a constant fold over immutable state.</li> <li>Companions can hold business rules you choose to extract \u2014 keeping the actor focused on message routing and state transitions.</li> <li>The fold signature is uniform \u2014 every method returns the pair, even read-only ones. No guessing about side effects.</li> <li>Companions compose with behavior recursion naturally \u2014 they're just another parameter in the closure.</li> </ul>"},{"location":"guides/streams-network/","title":"Reactive Streams over the Network","text":"<p>In this guide you'll build a sensor pipeline that streams data across a cluster and consumes it from an external client. Along the way you'll learn how <code>stream_producer</code> and <code>stream_consumer</code> work across nodes via service discovery, and how a <code>ClusterClient</code> can both consume and produce streams.</p>"},{"location":"guides/streams-network/#the-problem","title":"The Problem","text":"<p>The Reactive Streams guide showed how <code>stream_producer</code> and <code>stream_consumer</code> wire up a backpressured pipeline inside a single actor system. But in a distributed setting, the producer and consumer live on different machines. A sensor node pushes readings into a producer \u2014 how does a monitoring client on another machine iterate that stream?</p> <p>The answer: the stream protocol is just actor messages. <code>StreamElement</code>, <code>StreamDemand</code>, <code>StreamCompleted</code> \u2014 they all flow through <code>ActorRef</code>, which is network-transparent. If two actors can exchange messages over TCP, they can run a stream between them. Nothing changes in the API.</p>"},{"location":"guides/streams-network/#the-data","title":"The Data","text":"<p>Sensor readings and anomaly alerts, shared between all nodes and the client:</p> <pre><code>@dataclass(frozen=True)\nclass SensorReading:\n    sensor: str\n    value: float\n    timestamp: float\n    node: str\n\n\n@dataclass(frozen=True)\nclass Alert:\n    sensor: str\n    value: float\n    threshold: float\n    timestamp: float\n\n\n@dataclass(frozen=True)\nclass ConsumeAlerts:\n    producer: ActorRef[StreamProducerMsg[Alert]]\n</code></pre>"},{"location":"guides/streams-network/#producing-on-a-cluster-node","title":"Producing on a Cluster Node","text":"<p>A cluster node spawns a <code>stream_producer</code>, registers it with a <code>ServiceKey</code> for discovery, and obtains a <code>SinkRef</code> \u2014 all in one place:</p> <pre><code>SENSOR_KEY: ServiceKey[StreamProducerMsg[SensorReading]] = ServiceKey(\"sensor\")\n\nproducer_ref = system.spawn(\n    Behaviors.discoverable(stream_producer(), key=SENSOR_KEY),\n    f\"sensor-{sensor}\",\n)\nsink: SinkRef[SensorReading] = await system.ask(\n    producer_ref, lambda r: GetSink(reply_to=r), timeout=5.0\n)\n</code></pre> <p><code>Behaviors.discoverable</code> wraps the producer so the cluster receptionist knows about it. Any node or <code>ClusterClient</code> can now discover this producer via <code>lookup(SENSOR_KEY)</code>. The <code>SinkRef</code> wraps an in-process <code>asyncio.Queue</code> \u2014 it never leaves this node.</p> <p>The node pushes readings into the sink:</p> <pre><code>for i in range(20):\n    value = round(random.uniform(lo, hi), 2)\n    reading = SensorReading(\n        sensor=sensor,\n        value=value,\n        timestamp=time.time(),\n        node=node_name,\n    )\n    await sink.put(reading)\n</code></pre> <p>After all readings are pushed, <code>sink.complete()</code> signals the end of the stream.</p>"},{"location":"guides/streams-network/#consuming-from-another-cluster-node","title":"Consuming from Another Cluster Node","text":"<p>A second cluster node can discover the producer via <code>lookup</code> and spawn a local <code>stream_consumer</code> wired to the remote producer's ref. The code is identical to a single-node stream \u2014 <code>stream_consumer</code> sends <code>Subscribe</code> to the remote producer, <code>StreamElement</code> messages travel over TCP, and <code>StreamDemand</code> messages travel back. <code>SourceRef</code> wraps a local queue \u2014 iterating it feels the same as a single-node stream.</p>"},{"location":"guides/streams-network/#consuming-from-a-clusterclient","title":"Consuming from a ClusterClient","text":"<p>A <code>ClusterClient</code> connects from outside the cluster without joining it. It discovers producers via <code>lookup</code> and spawns a local <code>stream_consumer</code> for each one:</p> <pre><code>for i, instance in enumerate(producers):\n    producer_ref = instance.ref\n    consumer_name = f\"sensor-consumer-{i}\"\n\n    consumer_ref = client.spawn(\n        stream_consumer(producer_ref, timeout=10.0),\n        consumer_name,\n    )\n    source: SourceRef[SensorReading] = await client.ask(\n        consumer_ref, lambda r: GetSource(reply_to=r), timeout=5.0\n    )\n\n    node_host = instance.node.host if instance.node else \"unknown\"\n    node_port = instance.node.port if instance.node else 0\n    sensor_label = f\"sensor@{node_host}:{node_port}\"\n\n    task = asyncio.create_task(\n        consume_sensor(source, alert_sink, sensor_label)\n    )\n    tasks.append(task)\n</code></pre> <p><code>client.spawn()</code> creates a local actor inside the client's internal actor system. That actor has a full <code>ActorRef</code> addressable over TCP \u2014 the cluster-side producer can send <code>StreamElement</code> messages to it. The pattern is identical to consuming from within the cluster.</p>"},{"location":"guides/streams-network/#streaming-back-client-to-cluster","title":"Streaming Back: Client to Cluster","text":"<p>The client can also be the producer. It spawns a local <code>stream_producer</code>, obtains a <code>SinkRef</code>, and tells a cluster-side actor to subscribe. The alert monitor is discoverable under its own key:</p> <pre><code>ALERT_MONITOR_KEY: ServiceKey[ConsumeAlerts] = ServiceKey(\"alert-monitor\")\n\n# -- Spawn local alert producer and get its SinkRef --\nalert_producer = client.spawn(stream_producer(), \"alert-producer\")\nalert_sink: SinkRef[Alert] = await client.ask(\n    alert_producer, lambda r: GetSink(reply_to=r), timeout=5.0\n)\n\n# -- Discover alert monitor (with retries) --\nfor attempt in range(15):\n    monitor_listing = client.lookup(ALERT_MONITOR_KEY)\n    if monitor_listing and monitor_listing.instances:\n        monitor_ref = next(iter(monitor_listing.instances)).ref\n        monitor_ref.tell(ConsumeAlerts(producer=alert_producer))\n        log.info(\"%sAlert monitor%s connected\", MAGENTA, RESET)\n        break\n    log.info(\n        \"Alert monitor not found yet (attempt %d/15), retrying ...\",\n        attempt + 1,\n    )\n    await asyncio.sleep(2.0)\nelse:\n    log.warning(\"No alert monitor found after 15 attempts\")\n</code></pre> <p>The client discovers the alert monitor via <code>lookup(ALERT_MONITOR_KEY)</code> and sends <code>ConsumeAlerts(producer=alert_producer)</code>. The cluster-side node receives the client's producer ref and spawns a <code>stream_consumer</code> wired to it \u2014 the same <code>stream_consumer</code> + <code>SourceRef</code> + <code>async for</code> pattern as every other direction:</p> <pre><code>consumer = system.spawn(\n    stream_consumer(producer_ref, timeout=30.0), \"alert-consumer\"\n)\nsource: SourceRef[Alert] = await system.ask(\n    consumer, lambda r: GetSource(reply_to=r), timeout=5.0\n)\n\ncount = 0\nasync for alert in source:\n    count += 1\n    log.warning(\n        \"%sALERT #%d%s  sensor=%s%s%s  value=%.1f  threshold=%.1f\",\n        RED,\n        count,\n        RESET,\n        YELLOW,\n        alert.sensor,\n        RESET,\n        alert.value,\n        alert.threshold,\n    )\n</code></pre>"},{"location":"guides/streams-network/#what-stays-local-what-crosses-the-wire","title":"What Stays Local, What Crosses the Wire","text":"<p><code>SinkRef</code> and <code>SourceRef</code> wrap <code>asyncio.Queue</code>s \u2014 they never cross the wire. Each one lives on the same node as its actor:</p> Object Where it lives Serialized? <code>SinkRef</code> Same node as the producer No \u2014 wraps a local <code>asyncio.Queue</code> <code>SourceRef</code> Same node as the consumer No \u2014 wraps a local <code>asyncio.Queue</code> <code>ActorRef</code> Anywhere \u2014 addressable by URI Yes \u2014 <code>casty://system@host:port/path</code> <code>StreamElement</code>, <code>StreamDemand</code>, ... Actor messages Yes \u2014 via the configured serializer <p>This is why you always obtain <code>SinkRef</code> and <code>SourceRef</code> locally via <code>GetSink</code> / <code>GetSource</code> on the node where the actor lives. The stream protocol messages handle the rest.</p>"},{"location":"guides/streams-network/#docker-compose","title":"Docker Compose","text":"<p>The <code>examples/17_stream_pipeline/</code> directory contains the full example \u2014 3 cluster nodes producing sensor readings and 1 external client consuming all streams, detecting anomalies, and pushing alerts back:</p> <pre><code>x-node: &amp;node\n  build:\n    context: ../..\n    dockerfile: examples/17_stream_pipeline/Dockerfile.node\n\nservices:\n  node-1:\n    &lt;&lt;: *node\n    hostname: node-1\n    command:\n      [\n        \"--port\", \"25520\",\n        \"--host\", \"auto\",\n        \"--bind-host\", \"0.0.0.0\",\n        \"--seed\", \"node-1:25520\",\n        \"--nodes\", \"3\",\n        \"--sensor\", \"temp\",\n      ]\n  node-2:\n    &lt;&lt;: *node\n    hostname: node-2\n    command:\n      [\n        \"--port\", \"25520\",\n        \"--host\", \"auto\",\n        \"--bind-host\", \"0.0.0.0\",\n        \"--seed\", \"node-1:25520\",\n        \"--nodes\", \"3\",\n        \"--sensor\", \"humidity\",\n      ]\n  node-3:\n    &lt;&lt;: *node\n    hostname: node-3\n    command:\n      [\n        \"--port\", \"25520\",\n        \"--host\", \"auto\",\n        \"--bind-host\", \"0.0.0.0\",\n        \"--seed\", \"node-1:25520\",\n        \"--nodes\", \"3\",\n        \"--sensor\", \"pressure\",\n        \"--alert-monitor\",\n      ]\n  client:\n    build:\n      context: ../..\n      dockerfile: examples/17_stream_pipeline/Dockerfile.client\n    hostname: client\n    command: [\"--contact\", \"node-1:25520\", \"--producers\", \"3\"]\n    depends_on:\n      - node-1\n      - node-2\n      - node-3\n</code></pre>"},{"location":"guides/streams-network/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty/examples/17_stream_pipeline\ndocker compose up --build\n</code></pre> <p>What you learned:</p> <ul> <li>Streams work across nodes transparently \u2014 the same <code>Subscribe</code>/<code>StreamDemand</code>/<code>StreamElement</code> protocol flows over TCP with no extra configuration.</li> <li><code>SinkRef</code> and <code>SourceRef</code> stay local \u2014 they wrap <code>asyncio.Queue</code>s and are never serialized. Only <code>ActorRef</code> messages cross the wire.</li> <li>Service discovery (<code>Behaviors.discoverable</code> + <code>lookup</code>) is how consumers find remote producers in the cluster.</li> <li><code>ClusterClient.spawn()</code> creates locally-addressable actors that cluster nodes can send messages to \u2014 enabling bidirectional streaming from outside the cluster.</li> </ul>"},{"location":"guides/streams/","title":"Reactive Streams","text":"<p>In this guide you'll build a price feed pipeline that pushes stock prices through a demand-gated stream. Along the way you'll learn how <code>stream_producer</code> and <code>stream_consumer</code> implement backpressure, how <code>SinkRef</code> provides input-side backpressure, how the consumer exposes an <code>async for</code> iterator, and how <code>break</code> triggers deterministic cleanup across the pipeline.</p>"},{"location":"guides/streams/#the-problem","title":"The Problem","text":"<p>An actor producing data faster than its consumer can handle needs backpressure \u2014 a way for the consumer to say \"send me N more\". Without it, buffers grow unbounded or messages get dropped. Casty's reactive streams solve this with two cooperating behaviors: a producer that buffers elements and respects demand, and a consumer that mediates between the producer and an async iterator.</p> <p>The input side has the same problem: a fast caller can overwhelm the producer's buffer. <code>SinkRef</code> solves this \u2014 it wraps the producer's bounded queue and blocks when full.</p>"},{"location":"guides/streams/#the-data","title":"The Data","text":"<p>A simple frozen dataclass for price updates:</p> <pre><code>@dataclass(frozen=True)\nclass PriceUpdate:\n    symbol: str\n    price: float\n</code></pre>"},{"location":"guides/streams/#setting-up-the-pipeline","title":"Setting Up the Pipeline","text":"<p>The producer and consumer are regular actors \u2014 spawn them, wire them together:</p> <pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        producer = system.spawn(stream_producer(buffer_size=16), \"price-producer\")\n        consumer = system.spawn(\n            stream_consumer(producer, initial_demand=2), \"price-consumer\"\n        )\n        await asyncio.sleep(0.05)\n</code></pre> <p><code>stream_producer(buffer_size=16)</code> starts idle with a bounded internal queue. <code>stream_consumer(producer)</code> subscribes on setup and waits for a <code>GetSource</code> request. <code>initial_demand=2</code> means the consumer will request 2 elements at a time \u2014 the producer won't send more than 2 without further demand.</p>"},{"location":"guides/streams/#getting-a-sinkref-and-sourceref","title":"Getting a SinkRef and SourceRef","text":"<p>Ask the producer for a <code>SinkRef</code> and the consumer for a <code>SourceRef</code>:</p> <pre><code>sink: SinkRef[PriceUpdate] = await system.ask(\n    producer, lambda r: GetSink(reply_to=r), timeout=5.0\n)\n\nsource: SourceRef[PriceUpdate] = await system.ask(\n    consumer, lambda r: GetSource(reply_to=r), timeout=5.0\n)\n</code></pre> <p><code>SinkRef</code> wraps the producer's internal <code>asyncio.Queue</code>. Calling <code>await sink.put(elem)</code> pushes an element into the queue and nudges the producer to drain it. If the queue is full (<code>buffer_size</code> reached), <code>put</code> blocks until the consumer drains elements \u2014 input-side backpressure with zero configuration.</p>"},{"location":"guides/streams/#concurrent-push-and-consume","title":"Concurrent Push and Consume","text":"<p>Push and consume run concurrently \u2014 streams can be infinite:</p> <pre><code>async def consume() -&gt; int:\n    count = 0\n    async for update in source:\n        count += 1\n        print(f\"  #{count} {update.symbol}: ${update.price:.2f}\")\n    return count\n\nconsume_task = asyncio.create_task(consume())\n\nprint(\"\u2500\u2500 Pushing 5 prices into producer \u2500\u2500\")\nprint(\"\u2500\u2500 Consuming via async for (initial_demand=2) \u2500\u2500\")\nfor price in prices:\n    await sink.put(PriceUpdate(symbol=\"AAPL\", price=price))\nawait sink.complete()\n\ncount = await consume_task\n</code></pre> <p>The consumer starts iterating immediately. Elements flow as they're pushed \u2014 no need to complete the stream first. <code>sink.complete()</code> signals the end, the consumer's <code>async for</code> exits, and the task finishes.</p> <p>Output:</p> <pre><code>\u2500\u2500 Pushing 5 prices into producer \u2500\u2500\n\u2500\u2500 Consuming via async for (initial_demand=2) \u2500\u2500\n  #1 AAPL: $142.50\n  #2 AAPL: $143.10\n  #3 AAPL: $141.80\n  #4 AAPL: $144.20\n  #5 AAPL: $145.00\n\n\u2500\u2500 Stream complete: 5 prices received \u2500\u2500\n</code></pre> <p>All five prices arrive in order despite <code>initial_demand=2</code> \u2014 demand replenishment keeps the pipeline flowing.</p>"},{"location":"guides/streams/#early-cancellation","title":"Early Cancellation","text":"<p><code>break</code> out of an <code>async for</code> and the stream cleans up automatically. The iterator's <code>finally</code> block sends <code>StreamCancel</code> through the consumer to the producer, which stops:</p> <pre><code>print(\"\\n\u2500\u2500 Early break: take 5 from 20 \u2500\u2500\")\n\nproducer2 = system.spawn(stream_producer(buffer_size=32), \"num-producer\")\nconsumer2 = system.spawn(\n    stream_consumer(producer2, initial_demand=4), \"num-consumer\"\n)\nawait asyncio.sleep(0.05)\n\nsink2: SinkRef[int] = await system.ask(\n    producer2, lambda r: GetSink(reply_to=r), timeout=5.0\n)\n\nsource2: SourceRef[int] = await system.ask(\n    consumer2, lambda r: GetSource(reply_to=r), timeout=5.0\n)\n\ntaken: list[int] = []\n\nasync def take_five() -&gt; None:\n    async for value in source2:\n        taken.append(value)\n        if len(taken) &gt;= 5:\n            break\n\ntake_task = asyncio.create_task(take_five())\n\nfor i in range(20):\n    await sink2.put(i)\n\nawait take_task\n\nprint(f\"  Took: {taken}\")\nprint(\"  Stream cancelled \u2014 producer stopped via StreamCancel\")\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Early break: take 5 from 20 \u2500\u2500\n  Took: [0, 1, 2, 3, 4]\n  Stream cancelled \u2014 producer stopped via StreamCancel\n</code></pre> <p>The same cleanup happens on <code>return</code>, unhandled exceptions, or inactivity timeout. The producer is always notified.</p>"},{"location":"guides/streams/#how-it-works","title":"How It Works","text":"<p>The data flow has three directions:</p> <p>Input (elements): <code>await sink.put(elem)</code> \u2192 producer's <code>asyncio.Queue</code> \u2192 blocks at <code>buffer_size</code>.</p> <p>Forward (elements): queue drains \u2192 <code>StreamElement</code> \u2192 consumer's asyncio queue \u2192 <code>async for</code> yields.</p> <p>Backward (demand): <code>async for</code> consumes \u2192 consumer sends <code>StreamDemand(1)</code> \u2192 producer flushes next element from queue.</p> <p>The producer sends <code>StreamElement</code> and <code>StreamCompleted</code> to the consumer actor's own ref. The consumer receives them in its mailbox and forwards them to an internal queue that the <code>async for</code> reads from. Everything goes through actors \u2014 no side channels.</p> <pre><code>sequenceDiagram\n    participant CC as consumer caller\n    participant S as stream_consumer\n    participant P as stream_producer\n    participant SK as SinkRef\n\n    Note over SK,P: 1. Setup\n    SK-&gt;&gt;P: GetSink(reply_to)\n    P--&gt;&gt;SK: SinkRef(queue, producer)\n    S-&gt;&gt;P: Subscribe(consumer, demand=0)\n    CC-&gt;&gt;S: GetSource(reply_to)\n    S--&gt;&gt;CC: AsyncIterable\n    Note over CC: loop begins\n\n    Note over SK,CC: 2. Element flow\n    S-&gt;&gt;P: StreamDemand(n)\n    SK-&gt;&gt;P: await queue.put(elem) + InputReady\n    P-&gt;&gt;S: StreamElement(elem)\n    S-&gt;&gt;CC: queue.put\n    CC--&gt;&gt;S: StreamDemand(1)\n    S-&gt;&gt;P: StreamDemand(1)\n    Note over S,P: repeats per element\n\n    Note over SK,CC: 3. Completion\n    SK-&gt;&gt;P: CompleteStream()\n    P-&gt;&gt;S: StreamCompleted()\n    S-&gt;&gt;CC: queue.put\n    Note over CC: loop ends</code></pre>"},{"location":"guides/streams/#cross-node-streams","title":"Cross-Node Streams","text":"<p><code>SinkRef</code> and <code>SourceRef</code> both wrap in-process queues, but they work seamlessly across nodes \u2014 each one lives on its respective actor's node:</p> <ul> <li>Node A (producer side): obtain <code>SinkRef</code> via <code>GetSink</code>, push elements locally</li> <li>Node B (consumer side): obtain <code>SourceRef</code> via <code>GetSource</code>, iterate locally</li> </ul> <p>The cross-node communication flows through actor messages (<code>Subscribe</code>, <code>StreamDemand</code>, <code>StreamElement</code>, <code>StreamCompleted</code>) \u2014 the same protocol that powers single-node streams. No special configuration needed.</p> <p>For detailed coverage of streaming across cluster nodes, external clients, and Docker deployments, see Reactive Streams over the Network.</p>"},{"location":"guides/streams/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/10_streams.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>SinkRef</code> wraps the producer's bounded queue \u2014 <code>await sink.put(elem)</code> blocks when full, providing input-side backpressure.</li> <li><code>stream_producer(buffer_size=N)</code> creates a bounded internal queue. <code>buffer_size=0</code> means unbounded.</li> <li><code>stream_consumer</code> subscribes to a producer and exposes the stream as a <code>SourceRef</code> via <code>GetSource</code>.</li> <li>Push and consume run concurrently \u2014 streams can be infinite, no need to complete before consuming.</li> <li>Demand is automatic \u2014 each consumed element replenishes demand one-for-one.</li> <li><code>break</code>, <code>return</code>, and exceptions all trigger <code>StreamCancel</code> via the async generator's <code>finally</code> block \u2014 deterministic cleanup with no manual resource management.</li> </ul>"},{"location":"guides/supervision/","title":"Supervision","text":"<p>In this guide you'll build supervised workers that recover from failures automatically. Along the way you'll learn how supervision strategies decide what happens on crash, how lifecycle hooks observe the recovery process, and how supervision trees protect entire hierarchies of actors.</p>"},{"location":"guides/supervision/#messages","title":"Messages","text":"<p>A worker processes tasks. Some tasks are \"bad\" and cause it to crash:</p> <pre><code>@dataclass(frozen=True)\nclass Process:\n    task: str\n\n\n@dataclass(frozen=True)\nclass GetProcessed:\n    reply_to: ActorRef[tuple[str, ...]]\n\n\n@dataclass(frozen=True)\nclass CrashOnPurpose:\n    pass\n\n\ntype WorkerMsg = Process | GetProcessed | CrashOnPurpose\n</code></pre> <p><code>Process</code> carries the task payload. <code>CrashOnPurpose</code> forces a <code>RuntimeError</code> \u2014 we'll use it to test how different strategies respond to different exception types.</p>"},{"location":"guides/supervision/#the-worker","title":"The Worker","text":"<p>The worker raises on bad input and accumulates results via behavior recursion:</p> <pre><code>def worker(processed: tuple[str, ...] = ()) -&gt; Behavior[WorkerMsg]:\n    async def receive(\n        ctx: ActorContext[WorkerMsg], msg: WorkerMsg\n    ) -&gt; Behavior[WorkerMsg]:\n        match msg:\n            case Process(task) if task == \"bad\":\n                raise ValueError(f\"Cannot process: {task}\")\n            case Process(task):\n                print(f\"  [{ctx.self.address.path}] Processed: {task}\")\n                return worker((*processed, task))\n            case GetProcessed(reply_to):\n                reply_to.tell(processed)\n                return Behaviors.same()\n            case CrashOnPurpose():\n                raise RuntimeError(\"Intentional crash!\")\n\n    return Behaviors.receive(receive)\n</code></pre> <p>No try/except anywhere \u2014 the worker doesn't know it's supervised. Supervision is a concern of the parent, not the actor itself.</p>"},{"location":"guides/supervision/#supervision-strategies","title":"Supervision Strategies","text":"<p>A <code>OneForOneStrategy</code> decides what to do when a child fails. The simplest form restarts on every error:</p> <pre><code>def always_restart() -&gt; OneForOneStrategy:\n    return OneForOneStrategy(\n        max_restarts=3,\n        within=60.0,\n        decider=lambda _exc: Directive.restart,\n    )\n</code></pre> <p><code>max_restarts=3</code> within <code>60.0</code> seconds means: after 3 crashes in a sliding window, stop trying and let the actor die. This prevents infinite restart loops.</p> <p>For more control, pass a <code>decider</code> that inspects the exception:</p> <pre><code>def selective_strategy() -&gt; OneForOneStrategy:\n    def decide(exc: Exception) -&gt; Directive:\n        match exc:\n            case ValueError():\n                print(f\"  ValueError \u2014 restarting: {exc}\")\n                return Directive.restart\n            case RuntimeError():\n                print(f\"  RuntimeError \u2014 stopping: {exc}\")\n                return Directive.stop\n            case _:\n                return Directive.escalate\n\n    return OneForOneStrategy(max_restarts=5, within=60.0, decider=decide)\n</code></pre> <p><code>ValueError</code> \u2192 restart (transient). <code>RuntimeError</code> \u2192 stop (fatal). Anything else \u2192 escalate to the parent.</p>"},{"location":"guides/supervision/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p><code>Behaviors.with_lifecycle()</code> wraps a behavior with hooks that fire at key moments:</p> <pre><code>def resilient_worker() -&gt; Behavior[WorkerMsg]:\n    async def on_start(ctx: ActorContext[WorkerMsg]) -&gt; None:\n        print(f\"  [{ctx.self.address.path}] Started\")\n\n    async def on_stop(ctx: ActorContext[WorkerMsg]) -&gt; None:\n        print(f\"  [{ctx.self.address.path}] Stopped\")\n\n    async def on_restart(ctx: ActorContext[WorkerMsg], exc: Exception) -&gt; None:\n        print(f\"  [{ctx.self.address.path}] Restarting after: {exc}\")\n\n    return Behaviors.with_lifecycle(\n        Behaviors.supervise(worker(), selective_strategy()),\n        pre_start=on_start,\n        post_stop=on_stop,\n        pre_restart=on_restart,\n    )\n</code></pre> <p>Notice the composition: <code>worker()</code> \u2192 <code>Behaviors.supervise()</code> \u2192 <code>Behaviors.with_lifecycle()</code>. Each layer adds a concern without modifying the inner behavior. <code>pre_restart</code> receives the exception that caused the failure \u2014 useful for logging or cleanup.</p>"},{"location":"guides/supervision/#supervision-trees","title":"Supervision Trees","text":"<p>A supervisor spawns children and wraps each with a strategy. If a child crashes, only that child is affected:</p> <pre><code>@dataclass(frozen=True)\nclass DelegateWork:\n    task: str\n    worker_index: int\n\n\n@dataclass(frozen=True)\nclass GetReport:\n    reply_to: ActorRef[str]\n\n\ntype SupervisorMsg = DelegateWork | GetReport\n\n\ndef team_supervisor(num_workers: int = 3) -&gt; Behavior[SupervisorMsg]:\n    async def setup(ctx: ActorContext[SupervisorMsg]) -&gt; Behavior[SupervisorMsg]:\n        workers = tuple(\n            ctx.spawn(\n                Behaviors.supervise(worker(), always_restart()),\n                f\"worker-{i}\",\n            )\n            for i in range(num_workers)\n        )\n        return supervising(workers)\n\n    return Behaviors.setup(setup)\n\n\ndef supervising(\n    workers: tuple[ActorRef[WorkerMsg], ...],\n) -&gt; Behavior[SupervisorMsg]:\n    async def receive(\n        ctx: ActorContext[SupervisorMsg], msg: SupervisorMsg\n    ) -&gt; Behavior[SupervisorMsg]:\n        match msg:\n            case DelegateWork(task, worker_index):\n                idx = worker_index % len(workers)\n                workers[idx].tell(Process(task))\n                return Behaviors.same()\n            case GetReport(reply_to):\n                reply_to.tell(f\"Team has {len(workers)} workers\")\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p><code>team_supervisor</code> uses <code>Behaviors.setup()</code> to spawn workers as children in <code>pre_start</code>. Each child is independently supervised with <code>always_restart()</code>. The supervisor delegates work by index \u2014 when <code>worker-2</code> crashes on a bad task, it restarts silently and handles the next task.</p>"},{"location":"guides/supervision/#running-it","title":"Running It","text":"<pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        # 1. Basic supervision: restart on failure\n        print(\"\u2500\u2500 Basic supervision \u2500\u2500\")\n        ref: ActorRef[WorkerMsg] = system.spawn(\n            Behaviors.supervise(worker(), always_restart()),\n            \"basic-worker\",\n        )\n        ref.tell(Process(\"hello\"))\n        ref.tell(Process(\"bad\"))  # crashes \u2192 restarts\n        ref.tell(Process(\"world\"))  # continues after restart\n        await asyncio.sleep(0.2)\n\n        # 2. Selective strategy + lifecycle hooks\n        print(\"\\n\u2500\u2500 Selective strategy + lifecycle hooks \u2500\u2500\")\n        ref2: ActorRef[WorkerMsg] = system.spawn(\n            resilient_worker(), \"selective-worker\"\n        )\n        ref2.tell(Process(\"good\"))\n        await asyncio.sleep(0.1)\n        ref2.tell(Process(\"bad\"))  # ValueError \u2192 restart\n        await asyncio.sleep(0.1)\n        ref2.tell(CrashOnPurpose())  # RuntimeError \u2192 stop\n        await asyncio.sleep(0.2)\n\n        # 3. Supervision tree: parent manages children\n        print(\"\\n\u2500\u2500 Supervision tree \u2500\u2500\")\n        supervisor: ActorRef[SupervisorMsg] = system.spawn(\n            team_supervisor(3), \"team\"\n        )\n        supervisor.tell(DelegateWork(\"task-a\", 0))\n        supervisor.tell(DelegateWork(\"task-b\", 1))\n        supervisor.tell(DelegateWork(\"bad\", 2))  # worker-2 crashes, restarts\n        supervisor.tell(DelegateWork(\"task-c\", 2))  # worker-2 handles after restart\n        await asyncio.sleep(0.3)\n\n        report: str = await system.ask(\n            supervisor, lambda r: GetReport(reply_to=r), timeout=5.0\n        )\n        print(f\"  {report}\")\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 Basic supervision \u2500\u2500\n  [/basic-worker] Processed: hello\n  [/basic-worker] Processed: world\n\n\u2500\u2500 Selective strategy + lifecycle hooks \u2500\u2500\n  [/selective-worker] Started\n  [/selective-worker] Processed: good\n  ValueError \u2014 restarting: Cannot process: bad\n  [/selective-worker] Restarting after: Cannot process: bad\n  [/selective-worker] Started\n  RuntimeError \u2014 stopping: Intentional crash!\n  [/selective-worker] Stopped\n\n\u2500\u2500 Supervision tree \u2500\u2500\n  [/team/worker-0] Processed: task-a\n  [/team/worker-1] Processed: task-b\n  [/team/worker-2] Processed: task-c\n  Team has 3 workers\n</code></pre>"},{"location":"guides/supervision/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/03_supervision.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>OneForOneStrategy</code> decides per-child recovery: restart, stop, or escalate.</li> <li>Custom deciders inspect the exception to choose different directives for different failure modes.</li> <li>Lifecycle hooks (<code>pre_start</code>, <code>post_stop</code>, <code>pre_restart</code>) observe the actor lifecycle without modifying behavior.</li> <li>Supervision trees protect hierarchies \u2014 a crashing child is restarted without affecting siblings or the parent.</li> </ul>"},{"location":"guides/your-first-actor/","title":"Your First Actor","text":"<p>In this guide you'll build a traffic light \u2014 a state machine that cycles through green, yellow, and red. Along the way you'll learn the three core ideas in Casty: messages, behaviors, and state transitions.</p>"},{"location":"guides/your-first-actor/#messages","title":"Messages","text":"<p>Every actor communicates through messages. In Casty, messages are frozen dataclasses \u2014 immutable values that are safe to send between actors.</p> <p>Our traffic light handles two messages:</p> <pre><code>@dataclass(frozen=True)\nclass Tick:\n    pass\n\n\n@dataclass(frozen=True)\nclass GetColor:\n    reply_to: ActorRef[str]\n\n\ntype TrafficLightMsg = Tick | GetColor\n</code></pre> <p><code>Tick</code> advances the light to the next color. <code>GetColor</code> is a request-reply message: the sender includes its own <code>ActorRef</code> so the light can respond.</p>"},{"location":"guides/your-first-actor/#behaviors-as-state","title":"Behaviors as State","text":"<p>Instead of a single handler with an <code>if color == \"green\"</code> check, each color is its own behavior function. The traffic light is whichever behavior is currently active:</p> <pre><code>def green() -&gt; Behavior[TrafficLightMsg]:\n    async def receive(\n        ctx: ActorContext[TrafficLightMsg], msg: TrafficLightMsg\n    ) -&gt; Behavior[TrafficLightMsg]:\n        match msg:\n            case Tick():\n                print(\"\ud83d\udfe2 \u2192 \ud83d\udfe1\")\n                return yellow()\n            case GetColor(reply_to):\n                reply_to.tell(\"green\")\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n\ndef yellow() -&gt; Behavior[TrafficLightMsg]:\n    async def receive(\n        ctx: ActorContext[TrafficLightMsg], msg: TrafficLightMsg\n    ) -&gt; Behavior[TrafficLightMsg]:\n        match msg:\n            case Tick():\n                print(\"\ud83d\udfe1 \u2192 \ud83d\udd34\")\n                return red()\n            case GetColor(reply_to):\n                reply_to.tell(\"yellow\")\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n\n\ndef red() -&gt; Behavior[TrafficLightMsg]:\n    async def receive(\n        ctx: ActorContext[TrafficLightMsg], msg: TrafficLightMsg\n    ) -&gt; Behavior[TrafficLightMsg]:\n        match msg:\n            case Tick():\n                print(\"\ud83d\udd34 \u2192 \ud83d\udfe2\")\n                return green()\n            case GetColor(reply_to):\n                reply_to.tell(\"red\")\n                return Behaviors.same()\n\n    return Behaviors.receive(receive)\n</code></pre> <p>Notice: there are no mutable variables. No <code>self.color = \"yellow\"</code>. When the light receives a <code>Tick</code>, it returns a different behavior \u2014 and that is the state transition. This is called behavior recursion.</p>"},{"location":"guides/your-first-actor/#running-it","title":"Running It","text":"<p>The <code>main()</code> function spawns the actor and sends messages:</p> <pre><code>async def main() -&gt; None:\n    async with ActorSystem() as system:\n        light = system.spawn(green(), \"traffic-light\")\n\n        light.tell(Tick())  # green \u2192 yellow\n        light.tell(Tick())  # yellow \u2192 red\n        light.tell(Tick())  # red \u2192 green\n\n        await asyncio.sleep(0.1)\n\n        color = await system.ask(\n            light, lambda r: GetColor(reply_to=r), timeout=5.0\n        )\n        print(f\"Current color: {color}\")  # green\n\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udfe2 \u2192 \ud83d\udfe1\n\ud83d\udfe1 \u2192 \ud83d\udd34\n\ud83d\udd34 \u2192 \ud83d\udfe2\nCurrent color: green\n</code></pre>"},{"location":"guides/your-first-actor/#run-the-full-example","title":"Run the Full Example","text":"<pre><code>git clone https://github.com/gabfssilva/casty.git\ncd casty\nuv run python examples/guides/01_your_first_actor.py\n</code></pre> <p>What you learned:</p> <ul> <li>Messages are frozen dataclasses \u2014 immutable and type-safe.</li> <li>Behaviors are values, not classes. Each state is a function that returns a <code>Behavior</code>.</li> <li>State transitions happen by returning a different behavior \u2014 no mutation needed.</li> <li>Request-reply works by including an <code>ActorRef</code> in the message.</li> </ul>"},{"location":"persistence/event-sourcing/","title":"Event Sourcing","text":"<p>Traditional persistence stores the current state \u2014 a row in a database with <code>balance = 500</code>. Event sourcing stores the sequence of facts that produced the state \u2014 <code>Deposited(100)</code>, <code>Deposited(500)</code>, <code>Withdrawn(100)</code>. The current state is a derived value, computed by folding events from left to right.</p> <p>This distinction has profound consequences:</p> <ul> <li>Complete audit trail. Every state change is recorded as an immutable event. You can reconstruct the state at any point in time by replaying events up to that moment.</li> <li>Recovery after failure. When an actor restarts (via supervision) or a process crashes and restarts, the actor replays its events from the journal and recovers its exact state. Supervision provides the restart; event sourcing provides the memory.</li> <li>Separation of concerns. The decision to change state (command handling) is separated from the application of the change (event handling). Commands can be rejected; events are facts that have already occurred and cannot be rejected.</li> </ul> <p>Event sourcing introduces three distinct concepts:</p> <ul> <li>Commands \u2014 Messages from the outside world requesting a state change. \"Deposit 100\" is a command. It may be accepted or rejected (e.g., \"insufficient funds\" for a withdrawal).</li> <li>Events \u2014 Immutable records of what actually happened. \"Deposited 100\" is an event. Events are persisted to a journal and never modified.</li> <li>State \u2014 The current value derived from the event sequence. Computed by folding <code>on_event</code> over all persisted events from an initial state.</li> </ul> <p>A regular Casty actor holds state in a closure \u2014 but closures cannot be replayed. Event sourcing requires a different behavior shape with two explicit functions:</p> <ul> <li><code>on_event(state, event) -&gt; state</code> \u2014 A pure, synchronous function that applies one event to the current state. Used both for live processing and for recovery (replay).</li> <li><code>on_command(ctx, state, command) -&gt; Behavior</code> \u2014 An async function that receives the current state and a command, decides what events to persist, and returns <code>Behaviors.persisted(events=[...])</code>.</li> </ul> <pre><code># --- State ---\n\n@dataclass(frozen=True)\nclass AccountState:\n    balance: int\n    tx_count: int\n\n# --- Events (persisted to the journal) ---\n\n@dataclass(frozen=True)\nclass Deposited:\n    amount: int\n\n@dataclass(frozen=True)\nclass Withdrawn:\n    amount: int\n\ntype AccountEvent = Deposited | Withdrawn\n\n# --- Commands (sent by the outside world) ---\n\n@dataclass(frozen=True)\nclass Deposit:\n    amount: int\n\n@dataclass(frozen=True)\nclass Withdraw:\n    amount: int\n    reply_to: ActorRef[str]\n\n@dataclass(frozen=True)\nclass GetBalance:\n    reply_to: ActorRef[AccountState]\n\ntype AccountCommand = Deposit | Withdraw | GetBalance\n\n# --- Event handler (pure \u2014 used for replay and live updates) ---\n\ndef on_event(state: AccountState, event: AccountEvent) -&gt; AccountState:\n    match event:\n        case Deposited(amount):\n            return AccountState(balance=state.balance + amount, tx_count=state.tx_count + 1)\n        case Withdrawn(amount):\n            return AccountState(balance=state.balance - amount, tx_count=state.tx_count + 1)\n    return state\n\n# --- Command handler (async \u2014 decides what events to persist) ---\n\nasync def on_command(ctx: Any, state: AccountState, cmd: AccountCommand) -&gt; Any:\n    match cmd:\n        case Deposit(amount):\n            return Behaviors.persisted(events=[Deposited(amount)])\n        case Withdraw(amount, reply_to) if state.balance &gt;= amount:\n            reply_to.tell(\"ok\")\n            return Behaviors.persisted(events=[Withdrawn(amount)])\n        case Withdraw(_, reply_to):\n            reply_to.tell(f\"insufficient funds (balance={state.balance})\")\n            return Behaviors.same()\n        case GetBalance(reply_to):\n            reply_to.tell(state)\n            return Behaviors.same()\n    return Behaviors.unhandled()\n\n# --- Entity factory ---\n\njournal = InMemoryJournal()\n\ndef bank_account(entity_id: str) -&gt; Behavior[AccountCommand]:\n    return Behaviors.event_sourced(\n        entity_id=entity_id,\n        journal=journal,\n        initial_state=AccountState(balance=0, tx_count=0),\n        on_event=on_event,\n        on_command=on_command,\n        snapshot_policy=SnapshotEvery(n_events=100),\n    )\n\n# --- Recovery demonstration ---\n\nasync def main() -&gt; None:\n    # Phase 1: Normal operations\n    async with ActorSystem(name=\"bank\") as system:\n        account = system.spawn(bank_account(\"acc-001\"), \"account\")\n        await asyncio.sleep(0.1)\n\n        account.tell(Deposit(1000))\n        account.tell(Deposit(500))\n        await asyncio.sleep(0.1)\n\n        state = await system.ask(\n            account, lambda r: GetBalance(reply_to=r), timeout=2.0\n        )\n        print(f\"Balance: {state.balance}, transactions: {state.tx_count}\")\n        # Balance: 1500, transactions: 2\n\n    # Phase 2: Recovery \u2014 new actor, same journal\n    async with ActorSystem(name=\"bank\") as system:\n        account = system.spawn(bank_account(\"acc-001\"), \"account\")\n        await asyncio.sleep(0.1)\n\n        state = await system.ask(\n            account, lambda r: GetBalance(reply_to=r), timeout=2.0\n        )\n        print(f\"Recovered balance: {state.balance}, transactions: {state.tx_count}\")\n        # Recovered balance: 1500, transactions: 2\n\nasyncio.run(main())\n</code></pre> <p>In Phase 2, a new actor is spawned with the same <code>entity_id</code>. The framework automatically loads the latest snapshot (if any), replays events from the journal since that snapshot, and reconstructs the state. The actor resumes exactly where it left off.</p> <p>The <code>SnapshotEvery(n_events=100)</code> policy periodically saves the current state to the journal. Without snapshots, recovery requires replaying every event from the beginning \u2014 acceptable for entities with few events, but expensive for long-lived entities with thousands.</p>"},{"location":"persistence/event-sourcing/#journal-backends","title":"Journal Backends","text":"<p>The <code>EventJournal</code> protocol is storage-agnostic \u2014 four methods (<code>persist</code>, <code>load</code>, <code>save_snapshot</code>, <code>load_snapshot</code>) and a <code>kind</code> property. Casty ships two implementations:</p> Backend Durability Use case <code>InMemoryJournal</code> None (process-scoped) Tests, prototyping <code>SqliteJournal</code> File on disk Single-node production, local development"},{"location":"persistence/event-sourcing/#sqlitejournal","title":"SqliteJournal","text":"<p>Durable event sourcing backed by stdlib <code>sqlite3</code>. Zero external dependencies.</p> <pre><code>from casty import SqliteJournal\n\njournal = SqliteJournal(\"data/events.db\")\n\ndef bank_account(entity_id: str) -&gt; Behavior[AccountCommand]:\n    return Behaviors.event_sourced(\n        entity_id=entity_id,\n        journal=journal,\n        initial_state=AccountState(balance=0, tx_count=0),\n        on_event=on_event,\n        on_command=on_command,\n    )\n</code></pre> <p>WAL mode is enabled for concurrent reads. All writes go through <code>asyncio.to_thread</code> so the event loop is never blocked. Serialization defaults to <code>pickle</code> but is pluggable:</p> <pre><code>import json\n\njournal = SqliteJournal(\n    \"data/events.db\",\n    serialize=lambda obj: json.dumps(obj, default=str).encode(),\n    deserialize=lambda data: json.loads(data),\n)\n</code></pre>"},{"location":"persistence/event-sourcing/#journalkind-local-vs-centralized","title":"JournalKind \u2014 Local vs Centralized","text":"<p>Every journal declares its <code>kind</code>: <code>local</code> or <code>centralized</code>.</p> <pre><code>local (SQLite, InMemory)           centralized (PostgreSQL, S3, ...)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nEach node has its own store.       All nodes share one store.\nReplicas must persist events       Replicas skip persist \u2014 the\nthey receive from the primary.     primary's write is already\n                                   visible to every node.\n</code></pre> <p><code>InMemoryJournal</code> and <code>SqliteJournal</code> are both <code>local</code>. When you implement a journal backed by a shared database (PostgreSQL, DynamoDB, S3), set <code>kind</code> to <code>centralized</code> \u2014 the replication layer will automatically skip redundant writes on replica nodes:</p> <pre><code>from casty import JournalKind\n\nclass PostgresJournal:\n    @property\n    def kind(self) -&gt; JournalKind:\n        return JournalKind.centralized\n\n    async def persist(self, entity_id, events): ...\n    async def load(self, entity_id, from_sequence_nr=0): ...\n    async def save_snapshot(self, entity_id, snapshot): ...\n    async def load_snapshot(self, entity_id): ...\n</code></pre> <p>With a centralized journal, replicas still receive <code>ReplicateEvents</code> messages and maintain in-memory state (hot standby for fast failover), but they don't write to the journal since the data is already there.</p>"},{"location":"persistence/event-sourcing/#custom-backends","title":"Custom Backends","text":"<p>Implement the <code>EventJournal</code> protocol for any storage:</p> <ol> <li>Define <code>persist</code>, <code>load</code>, <code>save_snapshot</code>, <code>load_snapshot</code> with matching signatures.</li> <li>Set <code>kind</code> to <code>local</code> (node-local store) or <code>centralized</code> (shared store).</li> <li>Pass the instance to <code>Behaviors.event_sourced(journal=...)</code>.</li> </ol> <p>No inheritance needed \u2014 structural subtyping handles the rest.</p> <p>Next: Cluster Sharding</p>"},{"location":"reference/address/","title":"Address","text":""},{"location":"reference/address/#casty.ActorAddress","title":"<code>casty.ActorAddress</code>  <code>dataclass</code>","text":"<p>Immutable address identifying an actor within a Casty system.</p> <p>Uses a URI scheme: <code>casty://system@host:port/path</code> for remote actors and <code>casty://system/path</code> for local actors.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>Name of the actor system.</p> required <code>path</code> <code>str</code> <p>Slash-prefixed path of the actor within the system.</p> required <code>host</code> <code>str or None</code> <p>Hostname for remote addresses, <code>None</code> for local.</p> <code>None</code> <code>port</code> <code>int or None</code> <p>TCP port for remote addresses, <code>None</code> for local.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; local = ActorAddress(system=\"my-system\", path=\"/user/counter\")\n&gt;&gt;&gt; local.is_local\nTrue\n&gt;&gt;&gt; local.to_uri()\n'casty://my-system/user/counter'\n</code></pre> <pre><code>&gt;&gt;&gt; remote = ActorAddress(system=\"cluster\", path=\"/user/worker\", host=\"10.0.0.1\", port=25520)\n&gt;&gt;&gt; remote.to_uri()\n'casty://cluster@10.0.0.1:25520/user/worker'\n</code></pre>"},{"location":"reference/address/#casty.ActorAddress.system","title":"<code>system</code>  <code>instance-attribute</code>","text":""},{"location":"reference/address/#casty.ActorAddress.path","title":"<code>path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/address/#casty.ActorAddress.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/address/#casty.ActorAddress.port","title":"<code>port = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/address/#casty.ActorAddress.node_id","title":"<code>node_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/address/#casty.ActorAddress.is_local","title":"<code>is_local</code>  <code>property</code>","text":"<p>Return <code>True</code> if this address has no host (i.e. is local).</p> <p>Returns:</p> Type Description <code>bool</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ActorAddress(system=\"sys\", path=\"/a\").is_local\nTrue\n&gt;&gt;&gt; ActorAddress(system=\"sys\", path=\"/a\", host=\"h\", port=1).is_local\nFalse\n</code></pre>"},{"location":"reference/address/#casty.ActorAddress.to_uri","title":"<code>to_uri()</code>","text":"<p>Serialize the address to a <code>casty://</code> URI string.</p> <p>Returns:</p> Type Description <code>str</code> <p>URI representation of the address.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; addr = ActorAddress(system=\"sys\", path=\"/user/greeter\", host=\"localhost\", port=9000)\n&gt;&gt;&gt; addr.to_uri()\n'casty://sys@localhost:9000/user/greeter'\n</code></pre>"},{"location":"reference/address/#casty.ActorAddress.from_uri","title":"<code>from_uri(uri)</code>  <code>staticmethod</code>","text":"<p>Parse a <code>casty://</code> URI into an <code>ActorAddress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>A URI of the form <code>casty://system@host:port/path</code> (remote) or <code>casty://system/path</code> (local).</p> required <p>Returns:</p> Type Description <code>ActorAddress</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If uri does not match either pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; addr = ActorAddress.from_uri(\"casty://sys@127.0.0.1:8000/user/actor\")\n&gt;&gt;&gt; addr.host\n'127.0.0.1'\n&gt;&gt;&gt; addr.path\n'/user/actor'\n</code></pre>"},{"location":"reference/address/#casty.ActorAddress.__init__","title":"<code>__init__(system, path, host=None, port=None, node_id=None)</code>","text":""},{"location":"reference/behaviors/","title":"Behaviors","text":""},{"location":"reference/behaviors/#casty.Behavior","title":"<code>casty.Behavior</code>  <code>dataclass</code>","text":"<p>Minimal actor behavior primitive.</p> <p>Exactly one of <code>on_receive</code>, <code>on_setup</code>, or <code>signal</code> is set.</p>"},{"location":"reference/behaviors/#casty.Behavior.on_receive","title":"<code>on_receive = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.on_setup","title":"<code>on_setup = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.signal","title":"<code>signal = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.receive","title":"<code>receive(handler)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.setup","title":"<code>setup(factory)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.same","title":"<code>same()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.stopped","title":"<code>stopped()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.unhandled","title":"<code>unhandled()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.restart","title":"<code>restart()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behavior.__init__","title":"<code>__init__(*, on_receive=None, on_setup=None, signal=None)</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors","title":"<code>casty.Behaviors</code>","text":"<p>Factory for composing behaviors from the Behavior primitive.</p>"},{"location":"reference/behaviors/#casty.Behaviors.receive","title":"<code>receive(handler)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.setup","title":"<code>setup(factory)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.same","title":"<code>same()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.ignore","title":"<code>ignore()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.stopped","title":"<code>stopped()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.unhandled","title":"<code>unhandled()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.restart","title":"<code>restart()</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.with_lifecycle","title":"<code>with_lifecycle(behavior, *, pre_start=None, post_stop=None, pre_restart=None, post_restart=None)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.supervise","title":"<code>supervise(behavior, strategy)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.spy","title":"<code>spy(behavior, observer, *, spy_children=False)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.event_sourced","title":"<code>event_sourced(*, entity_id, journal, initial_state, on_event, on_command, snapshot_policy=None, replica_refs=None, replication=None)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.persisted","title":"<code>persisted(events, then=None)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.sharded","title":"<code>sharded(entity_factory, *, num_shards=100, replication=None)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.singleton","title":"<code>singleton(factory)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.discoverable","title":"<code>discoverable(behavior, *, key)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Behaviors.broadcasted","title":"<code>broadcasted(behavior)</code>  <code>staticmethod</code>","text":""},{"location":"reference/behaviors/#casty.Signal","title":"<code>casty.Signal</code>","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/behaviors/#casty.Signal.same","title":"<code>same = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Signal.stopped","title":"<code>stopped = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Signal.restart","title":"<code>restart = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.Signal.unhandled","title":"<code>unhandled = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.BroadcastedBehavior","title":"<code>casty.BroadcastedBehavior</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.BroadcastedBehavior.behavior","title":"<code>behavior</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.BroadcastedBehavior.__init__","title":"<code>__init__(behavior)</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior","title":"<code>casty.EventSourcedBehavior</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.entity_id","title":"<code>entity_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.journal","title":"<code>journal</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.initial_state","title":"<code>initial_state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.on_event","title":"<code>on_event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.on_command","title":"<code>on_command</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.snapshot_policy","title":"<code>snapshot_policy = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.replica_refs","title":"<code>replica_refs = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.replication","title":"<code>replication = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.EventSourcedBehavior.__init__","title":"<code>__init__(entity_id, journal, initial_state, on_event, on_command, snapshot_policy=None, replica_refs=(), replication=None)</code>","text":""},{"location":"reference/behaviors/#casty.PersistedBehavior","title":"<code>casty.PersistedBehavior</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.PersistedBehavior.events","title":"<code>events</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.PersistedBehavior.then","title":"<code>then</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.PersistedBehavior.__init__","title":"<code>__init__(events, then)</code>","text":""},{"location":"reference/behaviors/#casty.SnapshotEvery","title":"<code>casty.SnapshotEvery</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.SnapshotEvery.n_events","title":"<code>n_events</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.SnapshotEvery.__init__","title":"<code>__init__(n_events)</code>","text":""},{"location":"reference/behaviors/#casty.SnapshotPolicy","title":"<code>casty.SnapshotPolicy = SnapshotEvery</code>","text":""},{"location":"reference/behaviors/#casty.SpyEvent","title":"<code>casty.SpyEvent</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.SpyEvent.actor_path","title":"<code>actor_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.SpyEvent.event","title":"<code>event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.SpyEvent.timestamp","title":"<code>timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.SpyEvent.__init__","title":"<code>__init__(actor_path, event, timestamp)</code>","text":""},{"location":"reference/behaviors/#casty.ShardedBehavior","title":"<code>casty.ShardedBehavior</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.ShardedBehavior.entity_factory","title":"<code>entity_factory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.ShardedBehavior.num_shards","title":"<code>num_shards = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.ShardedBehavior.replication","title":"<code>replication = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.ShardedBehavior.__init__","title":"<code>__init__(entity_factory, num_shards=100, replication=None)</code>","text":""},{"location":"reference/behaviors/#casty.SingletonBehavior","title":"<code>casty.SingletonBehavior</code>  <code>dataclass</code>","text":""},{"location":"reference/behaviors/#casty.SingletonBehavior.factory","title":"<code>factory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/behaviors/#casty.SingletonBehavior.__init__","title":"<code>__init__(factory)</code>","text":""},{"location":"reference/client/","title":"Cluster Client","text":""},{"location":"reference/client/#casty.ClusterClient","title":"<code>casty.ClusterClient</code>","text":"<p>External client that routes messages to a Casty cluster.</p> <p>Connects to cluster contact points via TCP, subscribes to topology updates, and routes <code>ShardEnvelope</code> messages directly to the node owning each shard \u2014 zero hops, no cluster membership.</p> <p>Parameters:</p> Name Type Description Default <code>contact_points</code> <code>list[tuple[str, int]]</code> <p>List of <code>(host, port)</code> for cluster nodes to connect to.</p> required <code>system_name</code> <code>str</code> <p>Logical name of the cluster's actor system (must match).</p> required <code>client_host</code> <code>str</code> <p>Advertised hostname for this client (for receiving responses).</p> <code>'127.0.0.1'</code> <code>client_port</code> <code>int</code> <p>Advertised port for this client (<code>0</code> for auto-assignment).</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; async with ClusterClient(\n...     contact_points=[(\"10.0.1.10\", 25520)],\n...     system_name=\"my-cluster\",\n... ) as client:\n...     counter = client.entity_ref(\"counter\", num_shards=100)\n...     counter.tell(ShardEnvelope(\"user-42\", Increment(1)))\n...     count = await client.ask(\n...         counter,\n...         lambda r: ShardEnvelope(\"user-42\", GetCount(reply_to=r)),\n...     )\n</code></pre>"},{"location":"reference/client/#casty.ClusterClient.__init__","title":"<code>__init__(*, contact_points, system_name, client_host='127.0.0.1', client_port=0, advertised_host=None, advertised_port=None, address_map=None, serializer=None)</code>","text":""},{"location":"reference/client/#casty.ClusterClient.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":""},{"location":"reference/client/#casty.ClusterClient.__aexit__","title":"<code>__aexit__(*exc)</code>  <code>async</code>","text":""},{"location":"reference/client/#casty.ClusterClient.lookup","title":"<code>lookup(key)</code>","text":"<p>Look up service instances registered in the cluster.</p> <p>Reads from the locally cached topology snapshot \u2014 no network round-trip required.  Returns an empty <code>Listing</code> if no topology has been received yet.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>Typed service key to search for.</p> required <p>Returns:</p> Type Description <code>Listing[M]</code> <p>Current set of instances matching the key.</p>"},{"location":"reference/client/#casty.ClusterClient.spawn","title":"<code>spawn(behavior, name)</code>","text":"<p>Spawn a local actor on this client's internal actor system.</p> <p>The actor is remotely addressable \u2014 cluster nodes can send messages back to it via TCP.</p> <p>Parameters:</p> Name Type Description Default <code>behavior</code> <code>Behavior[M]</code> <p>The behavior for the new actor.</p> required <code>name</code> <code>str</code> <p>Unique name for the actor.</p> required <p>Returns:</p> Type Description <code>ActorRef[M]</code> <p>A ref to the newly spawned actor.</p>"},{"location":"reference/client/#casty.ClusterClient.entity_ref","title":"<code>entity_ref(shard_type, *, num_shards)</code>","text":"<p>Get a ref that routes <code>ShardEnvelope</code> messages to the cluster.</p> <p>Creates a local proxy actor for the given shard type on first call. Subsequent calls with the same <code>shard_type</code> return the cached proxy.</p> <p>Parameters:</p> Name Type Description Default <code>shard_type</code> <code>str</code> <p>Name of the sharded entity type (must match the cluster's name).</p> required <code>num_shards</code> <code>int</code> <p>Number of shards (must match the cluster's configuration).</p> required <p>Returns:</p> Type Description <code>ActorRef[ShardEnvelope[Any]]</code> <p>A ref that accepts <code>ShardEnvelope</code> messages and routes them to the correct cluster node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; counter = client.entity_ref(\"counter\", num_shards=100)\n&gt;&gt;&gt; counter.tell(ShardEnvelope(\"entity-1\", Increment(1)))\n</code></pre>"},{"location":"reference/client/#casty.ClusterClient.ask","title":"<code>ask(ref, msg_factory, *, timeout=5.0)</code>  <code>async</code>","text":"<p>Send a message and wait for a reply.</p> <p>Creates a temporary remotely-addressable ref so the cluster can respond directly to this client via TCP.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>ActorRef[M]</code> <p>Target actor (typically from <code>entity_ref</code>).</p> required <code>msg_factory</code> <code>Callable[[ActorRef[R]], M]</code> <p>Factory that receives a reply-to ref and returns the message.</p> required <code>timeout</code> <code>float</code> <p>Maximum seconds to wait.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>R</code> <p>The reply from the cluster actor.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If no reply is received within <code>timeout</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; count = await client.ask(\n...     counter_ref,\n...     lambda r: ShardEnvelope(\"user-42\", GetCount(reply_to=r)),\n... )\n</code></pre>"},{"location":"reference/client/#casty.ClusterClient.region_ref","title":"<code>region_ref(key, factory, *, num_shards)</code>","text":"<p>Return a ref that routes <code>ShardEnvelope</code> to a cluster region.</p> <p>Delegates to <code>entity_ref</code>; factory is ignored because the entities already exist on the cluster nodes.</p>"},{"location":"reference/client/#casty.ClusterClient.entity_ask","title":"<code>entity_ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Send a message to a sharded entity and wait for a reply.</p>"},{"location":"reference/client/#casty.ClusterClient.distributed","title":"<code>distributed()</code>","text":"<p>Create a <code>Distributed</code> facade for this client.</p> <p>Returns:</p> Type Description <code>Distributed</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = client.distributed()\n&gt;&gt;&gt; counter = d.counter(\"hits\")\n</code></pre>"},{"location":"reference/cluster-state/","title":"Cluster State","text":""},{"location":"reference/cluster-state/#casty.ClusterState","title":"<code>casty.ClusterState</code>  <code>dataclass</code>","text":"<p>Immutable, CRDT-mergeable snapshot of cluster membership.</p> <p>Every node maintains its own <code>ClusterState</code> and periodically gossips it to peers.  Merging two states produces a new state that is the union of members with vector-clock-ordered conflict resolution, guaranteeing convergence without coordination.</p> <p>Parameters:</p> Name Type Description Default <code>members</code> <code>frozenset[Member]</code> <p>Current cluster members.</p> <code>(lambda: frozenset[Member]())()</code> <code>unreachable</code> <code>frozenset[NodeAddress]</code> <p>Nodes detected as unreachable by the failure detector.</p> <code>(lambda: frozenset[NodeAddress]())()</code> <code>version</code> <code>VectorClock</code> <p>Causal version of this state.</p> <code>VectorClock()</code> <code>shard_allocations</code> <code>dict[str, dict[int, ShardAllocation]]</code> <p>Per-shard-type allocation map (populated by the coordinator).</p> <code>(lambda: {})()</code> <code>allocation_epoch</code> <code>int</code> <p>Monotonically increasing epoch for shard allocation changes.</p> <code>0</code> <code>seen</code> <code>frozenset[NodeAddress]</code> <p>Nodes that have acknowledged this version (for convergence check).</p> <code>(lambda: frozenset[NodeAddress]())()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; node = NodeAddress(\"10.0.0.1\", 2551)\n&gt;&gt;&gt; m = Member(node, MemberStatus.up, frozenset(), id=\"node-1\")\n&gt;&gt;&gt; s1 = ClusterState().add_member(m)\n&gt;&gt;&gt; s2 = ClusterState().add_member(m)\n&gt;&gt;&gt; merged = s1.merge_members(s2)\n&gt;&gt;&gt; len(merged)\n1\n</code></pre>"},{"location":"reference/cluster-state/#casty.ClusterState.members","title":"<code>members = field(default_factory=(lambda: frozenset[Member]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.unreachable","title":"<code>unreachable = field(default_factory=(lambda: frozenset[NodeAddress]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.version","title":"<code>version = field(default_factory=VectorClock)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.shard_allocations","title":"<code>shard_allocations = field(default_factory=(lambda: {}))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.allocation_epoch","title":"<code>allocation_epoch = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.seen","title":"<code>seen = field(default_factory=(lambda: frozenset[NodeAddress]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.registry","title":"<code>registry = field(default_factory=(lambda: frozenset[ServiceEntry]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.is_converged","title":"<code>is_converged</code>  <code>property</code>","text":"<p>Whether all active members have seen the current state version.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> when every non-down, non-removed member is in <code>seen</code>.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.leader","title":"<code>leader</code>  <code>property</code>","text":"<p>The current cluster leader, determined by lowest sorted address.</p> <p>Returns:</p> Type Description <code>NodeAddress or None</code> <p>Address of the leader, or <code>None</code> if no members are <code>up</code>.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.add_member","title":"<code>add_member(member)</code>","text":"<p>Return a new state with member added or replaced.</p> <p>If a member with the same address already exists it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>member</code> <code>Member</code> <p>The member to add.</p> required <p>Returns:</p> Type Description <code>ClusterState</code> <p>New state containing the member.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.merge_members","title":"<code>merge_members(other)</code>","text":"<p>Merge member sets from two states, keeping self's version on conflict.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ClusterState</code> <p>The remote state to merge with.</p> required <p>Returns:</p> Type Description <code>frozenset[Member]</code> <p>Union of members; when both states contain the same address, self's entry wins (last-writer in dict update order).</p>"},{"location":"reference/cluster-state/#casty.ClusterState.update_status","title":"<code>update_status(address, status)</code>","text":"<p>Return a new state with the given node's status changed.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>NodeAddress</code> <p>The node to update.</p> required <code>status</code> <code>MemberStatus</code> <p>The new status.</p> required <p>Returns:</p> Type Description <code>ClusterState</code> <p>New state with the updated member.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.mark_unreachable","title":"<code>mark_unreachable(address)</code>","text":"<p>Return a new state with address added to the unreachable set.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>NodeAddress</code> <p>The node to mark unreachable.</p> required <p>Returns:</p> Type Description <code>ClusterState</code> <p>New state with updated unreachable set.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.mark_reachable","title":"<code>mark_reachable(address)</code>","text":"<p>Return a new state with address removed from the unreachable set.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>NodeAddress</code> <p>The node to mark reachable again.</p> required <p>Returns:</p> Type Description <code>ClusterState</code> <p>New state with updated unreachable set.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.with_allocations","title":"<code>with_allocations(shard_type, allocations, epoch)</code>","text":"<p>Return a new state with updated shard allocations for shard_type.</p> <p>Parameters:</p> Name Type Description Default <code>shard_type</code> <code>str</code> <p>The shard type name (e.g. <code>\"counter\"</code>).</p> required <code>allocations</code> <code>dict[int, ShardAllocation]</code> <p>Shard-id to allocation mapping.</p> required <code>epoch</code> <code>int</code> <p>Allocation epoch for consistency tracking.</p> required <p>Returns:</p> Type Description <code>ClusterState</code> <p>New state with the updated allocations.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/cluster-state/#casty.ClusterState.diff","title":"<code>diff(previous)</code>","text":"<p>Compute the changes between previous and this state.</p> <p>Parameters:</p> Name Type Description Default <code>previous</code> <code>ClusterState or None</code> <p>The previous state to diff against.  <code>None</code> means empty.</p> required <p>Returns:</p> Type Description <code>ClusterChanges</code> <p>A summary of joined/removed members and leader election.</p>"},{"location":"reference/cluster-state/#casty.ClusterState.__init__","title":"<code>__init__(members=(lambda: frozenset[Member]())(), unreachable=(lambda: frozenset[NodeAddress]())(), version=VectorClock(), shard_allocations=(lambda: {})(), allocation_epoch=0, seen=(lambda: frozenset[NodeAddress]())(), registry=(lambda: frozenset[ServiceEntry]())())</code>","text":""},{"location":"reference/cluster-state/#casty.Member","title":"<code>casty.Member</code>  <code>dataclass</code>","text":"<p>A single node's membership record in the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>NodeAddress</code> <p>Network address of the member.</p> required <code>status</code> <code>MemberStatus</code> <p>Current lifecycle status.</p> required <code>roles</code> <code>frozenset[str]</code> <p>Roles advertised by this member (e.g. <code>{\"frontend\", \"backend\"}</code>).</p> required <code>id</code> <code>NodeId</code> <p>Human-readable identifier for this node (e.g. <code>\"worker-1\"</code>).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = Member(NodeAddress(\"10.0.0.1\", 2551), MemberStatus.up, frozenset(), id=\"node-1\")\n&gt;&gt;&gt; m.status\n&lt;MemberStatus.up: ...&gt;\n</code></pre>"},{"location":"reference/cluster-state/#casty.Member.address","title":"<code>address</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.Member.status","title":"<code>status</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.Member.roles","title":"<code>roles</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.Member.id","title":"<code>id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.Member.__init__","title":"<code>__init__(address, status, roles, id)</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus","title":"<code>casty.MemberStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Lifecycle status of a cluster member.</p> <p>Members progress through statuses in order::</p> <pre><code>joining -&gt; up -&gt; leaving -&gt; down -&gt; removed\n</code></pre> <p>Only the cluster leader promotes <code>joining</code> to <code>up</code> once gossip has converged.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; MemberStatus.joining\n&lt;MemberStatus.joining: ...&gt;\n&gt;&gt;&gt; MemberStatus.up.name\n'up'\n</code></pre>"},{"location":"reference/cluster-state/#casty.MemberStatus.joining","title":"<code>joining = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus.up","title":"<code>up = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus.leaving","title":"<code>leaving = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus.down","title":"<code>down = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus.removed","title":"<code>removed = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.MemberStatus.merge_priority","title":"<code>merge_priority</code>  <code>property</code>","text":"<p>Priority key for CRDT merge on concurrent vector clocks.</p> <p>Returns a <code>(alive, value)</code> tuple where alive statuses always win over terminal ones.  This ensures a rejoining node (<code>joining</code>) beats stale <code>down</code>/<code>removed</code> gossip.</p>"},{"location":"reference/cluster-state/#casty.NodeAddress","title":"<code>casty.NodeAddress</code>  <code>dataclass</code>","text":"<p>Network address identifying a cluster node.</p> <p>Ordered lexicographically by <code>(host, port)</code> so that deterministic leader election can sort members by address.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Hostname or IP address of the node.</p> required <code>port</code> <code>int</code> <p>TCP port the node listens on.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; addr = NodeAddress(host=\"127.0.0.1\", port=2551)\n&gt;&gt;&gt; addr.host\n'127.0.0.1'\n&gt;&gt;&gt; NodeAddress(\"a\", 1) &lt; NodeAddress(\"b\", 1)\nTrue\n</code></pre>"},{"location":"reference/cluster-state/#casty.NodeAddress.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.NodeAddress.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.NodeAddress.__lt__","title":"<code>__lt__(other)</code>","text":""},{"location":"reference/cluster-state/#casty.NodeAddress.__init__","title":"<code>__init__(host, port)</code>","text":""},{"location":"reference/cluster-state/#casty.NodeId","title":"<code>casty.NodeId = str</code>","text":""},{"location":"reference/cluster-state/#casty.VectorClock","title":"<code>casty.VectorClock</code>  <code>dataclass</code>","text":"<p>Logical clock for tracking causal ordering of cluster state versions.</p> <p>Each node increments its own entry on every state mutation.  Two clocks can be compared for happens-before or concurrency, and merged to produce a least upper bound.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; node_a = NodeAddress(\"a\", 1)\n&gt;&gt;&gt; node_b = NodeAddress(\"b\", 1)\n&gt;&gt;&gt; vc = VectorClock().increment(node_a).increment(node_a)\n&gt;&gt;&gt; vc.version_of(node_a)\n2\n&gt;&gt;&gt; merged = vc.merge(VectorClock().increment(node_b))\n&gt;&gt;&gt; merged.version_of(node_a), merged.version_of(node_b)\n(2, 1)\n</code></pre>"},{"location":"reference/cluster-state/#casty.VectorClock.version_of","title":"<code>version_of(node)</code>","text":"<p>Return the version counter for node, defaulting to 0.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>NodeAddress</code> <p>The node whose version to look up.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The current counter for node.</p>"},{"location":"reference/cluster-state/#casty.VectorClock.increment","title":"<code>increment(node)</code>","text":"<p>Return a new clock with node's counter incremented by one.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>NodeAddress</code> <p>The node whose counter to increment.</p> required <p>Returns:</p> Type Description <code>VectorClock</code> <p>A new clock with the updated counter.</p>"},{"location":"reference/cluster-state/#casty.VectorClock.merge","title":"<code>merge(other)</code>","text":"<p>Merge two clocks by taking the element-wise maximum.</p> <p>This is the least upper bound (join) operation, guaranteeing CRDT convergence.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>VectorClock</code> <p>The clock to merge with.</p> required <p>Returns:</p> Type Description <code>VectorClock</code> <p>A new clock where each entry is <code>max(self[node], other[node])</code>.</p>"},{"location":"reference/cluster-state/#casty.VectorClock.is_before","title":"<code>is_before(other)</code>","text":"<p>Return whether this clock strictly happens-before other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>VectorClock</code> <p>The clock to compare against.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if every entry in self is <code>&lt;=</code> the corresponding entry in other and at least one is strictly less.</p>"},{"location":"reference/cluster-state/#casty.VectorClock.is_concurrent_with","title":"<code>is_concurrent_with(other)</code>","text":"<p>Return whether this clock is concurrent with other.</p> <p>Two clocks are concurrent when neither happens-before the other and they are not equal.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>VectorClock</code> <p>The clock to compare against.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the clocks are causally unrelated.</p>"},{"location":"reference/cluster-state/#casty.VectorClock.__init__","title":"<code>__init__(_versions=(lambda: dict[NodeAddress, int]())())</code>","text":""},{"location":"reference/cluster-state/#casty.ShardAllocation","title":"<code>casty.ShardAllocation</code>  <code>dataclass</code>","text":"<p>Tracks which node owns a shard and where its replicas live.</p> <p>Parameters:</p> Name Type Description Default <code>primary</code> <code>NodeAddress</code> <p>Node that owns the shard and handles writes.</p> required <code>replicas</code> <code>tuple[NodeAddress, ...]</code> <p>Nodes holding passive replica copies.</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alloc = ShardAllocation(primary=NodeAddress(\"10.0.0.1\", 25520))\n&gt;&gt;&gt; alloc.replicas\n()\n</code></pre>"},{"location":"reference/cluster-state/#casty.ShardAllocation.primary","title":"<code>primary</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ShardAllocation.replicas","title":"<code>replicas = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ShardAllocation.__init__","title":"<code>__init__(primary, replicas=())</code>","text":""},{"location":"reference/cluster-state/#casty.ServiceEntry","title":"<code>casty.ServiceEntry</code>  <code>dataclass</code>","text":"<p>A registered service in the cluster registry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Service name (e.g. <code>\"payment-service\"</code>).</p> required <code>node</code> <code>NodeAddress</code> <p>Node where the actor lives.</p> required <code>path</code> <code>str</code> <p>Actor path on that node.</p> required"},{"location":"reference/cluster-state/#casty.ServiceEntry.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ServiceEntry.node","title":"<code>node</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ServiceEntry.path","title":"<code>path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster-state/#casty.ServiceEntry.__init__","title":"<code>__init__(key, node, path)</code>","text":""},{"location":"reference/cluster/","title":"Cluster","text":""},{"location":"reference/cluster/#casty.Cluster","title":"<code>casty.Cluster</code>","text":"<p>High-level handle for starting and querying a cluster on an <code>ActorSystem</code>.</p> <p>Wraps the internal cluster actor, providing <code>start</code>, <code>get_state</code>, and <code>shutdown</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system that will host the cluster actor.</p> required <code>config</code> <code>ClusterConfig</code> <p>Cluster network and seed configuration.</p> required <code>remote_transport</code> <code>RemoteTransport or None</code> <p>Transport for cross-node communication.  <code>None</code> for local-only.</p> <code>None</code> <code>system_name</code> <code>str</code> <p>Logical name of the actor system (used in actor addresses).</p> <code>''</code> <code>gossip_interval</code> <code>float</code> <p>Seconds between gossip rounds.</p> <code>1.0</code> <code>gossip_fanout</code> <code>int</code> <p>Number of peers contacted per gossip round.</p> <code>3</code> <code>heartbeat_interval</code> <code>float</code> <p>Seconds between heartbeat state polls.</p> <code>0.5</code> <code>availability_interval</code> <code>float</code> <p>Seconds between failure-detector availability checks.</p> <code>2.0</code> <code>failure_detector_config</code> <code>FailureDetectorConfig or None</code> <p>Tuning for the phi accrual failure detector.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cluster = Cluster(system, ClusterConfig(\"127.0.0.1\", 2551, node_id=\"node-1\"))\n&gt;&gt;&gt; await cluster.start()\n&gt;&gt;&gt; state = await cluster.get_state()\n&gt;&gt;&gt; len(state.members)\n1\n</code></pre>"},{"location":"reference/cluster/#casty.Cluster.ref","title":"<code>ref</code>  <code>property</code>","text":"<p>The cluster actor's ref.</p> <p>Returns:</p> Type Description <code>ActorRef[ClusterCmd]</code> <p>Reference to the running cluster actor.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the cluster has not been started yet.</p>"},{"location":"reference/cluster/#casty.Cluster.__init__","title":"<code>__init__(system, config, *, remote_transport=None, local_transport=None, system_name='', gossip_interval=1.0, gossip_fanout=3, heartbeat_interval=0.5, availability_interval=2.0, failure_detector_config=None, event_stream=None)</code>","text":""},{"location":"reference/cluster/#casty.Cluster.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Spawn the cluster actor and begin membership protocol.</p> <p>After this method returns the cluster is actively gossiping and monitoring heartbeats.</p>"},{"location":"reference/cluster/#casty.Cluster.get_state","title":"<code>get_state(*, timeout=5.0)</code>  <code>async</code>","text":"<p>Request the current cluster state from the topology actor.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds to wait for a reply.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>ClusterState</code> <p>The latest cluster state snapshot.</p>"},{"location":"reference/cluster/#casty.Cluster.get_receptionist","title":"<code>get_receptionist(*, timeout=5.0)</code>  <code>async</code>","text":"<p>Ask the cluster actor for the receptionist ref.</p>"},{"location":"reference/cluster/#casty.Cluster.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Gracefully shut down the cluster actor.</p>"},{"location":"reference/cluster/#casty.ClusterConfig","title":"<code>casty.ClusterConfig</code>  <code>dataclass</code>","text":"<p>Configuration for joining or forming a cluster.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Bind address for this node.</p> required <code>port</code> <code>int</code> <p>TCP port for this node.</p> required <code>seed_nodes</code> <code>tuple[tuple[str, int], ...]</code> <p>Initial contact points for cluster formation.  An empty tuple means this node forms a new single-node cluster.</p> <code>()</code> <code>node_id</code> <code>NodeId</code> <p>Human-readable identifier for this node (e.g. <code>\"worker-1\"</code>).</p> required <code>roles</code> <code>frozenset[str]</code> <p>Roles to advertise for this node.</p> <code>(lambda: frozenset[str]())()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ClusterConfig(\"127.0.0.1\", 2551, node_id=\"node-1\", seed_nodes=((\"127.0.0.1\", 2552),))\n&gt;&gt;&gt; cfg.host, cfg.port\n('127.0.0.1', 2551)\n</code></pre>"},{"location":"reference/cluster/#casty.ClusterConfig.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ClusterConfig.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ClusterConfig.node_id","title":"<code>node_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ClusterConfig.seed_nodes","title":"<code>seed_nodes = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ClusterConfig.roles","title":"<code>roles = field(default_factory=(lambda: frozenset[str]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ClusterConfig.__init__","title":"<code>__init__(host, port, node_id, seed_nodes=(), roles=(lambda: frozenset[str]())())</code>","text":""},{"location":"reference/cluster/#casty.ResolveNode","title":"<code>casty.ResolveNode</code>  <code>dataclass</code>","text":"<p>Query to resolve a <code>NodeId</code> to its <code>NodeAddress</code>.</p>"},{"location":"reference/cluster/#casty.ResolveNode.node_id","title":"<code>node_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ResolveNode.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.ResolveNode.__init__","title":"<code>__init__(node_id, reply_to)</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot","title":"<code>casty.TopologySnapshot</code>  <code>dataclass</code>","text":"<p>Full cluster topology pushed to subscribers.</p> <p>Parameters:</p> Name Type Description Default <code>members</code> <code>frozenset[Member]</code> <p>Current cluster members with their statuses.</p> required <code>leader</code> <code>NodeAddress | None</code> <p>Address of the current cluster leader, or <code>None</code>.</p> required <code>shard_allocations</code> <code>dict[str, dict[int, ShardAllocation]]</code> <p>Per-shard-type mapping of shard_id to allocation (primary + replicas).</p> required <code>allocation_epoch</code> <code>int</code> <p>Monotonically increasing epoch for allocation consistency.</p> required <code>unreachable</code> <code>frozenset[NodeAddress]</code> <p>Nodes currently marked unreachable by the failure detector.</p> <code>(lambda: frozenset[NodeAddress]())()</code> <code>registry</code> <code>frozenset[ServiceEntry]</code> <p>Service registry entries replicated across the cluster.</p> <code>(lambda: frozenset[ServiceEntry]())()</code>"},{"location":"reference/cluster/#casty.TopologySnapshot.members","title":"<code>members</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.leader","title":"<code>leader</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.shard_allocations","title":"<code>shard_allocations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.allocation_epoch","title":"<code>allocation_epoch</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.unreachable","title":"<code>unreachable = field(default_factory=(lambda: frozenset[NodeAddress]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.registry","title":"<code>registry = field(default_factory=(lambda: frozenset[ServiceEntry]()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.TopologySnapshot.__init__","title":"<code>__init__(members, leader, shard_allocations, allocation_epoch, unreachable=(lambda: frozenset[NodeAddress]())(), registry=(lambda: frozenset[ServiceEntry]())())</code>","text":""},{"location":"reference/cluster/#casty.SubscribeTopology","title":"<code>casty.SubscribeTopology</code>  <code>dataclass</code>","text":"<p>Request to receive topology updates.</p> <p>Parameters:</p> Name Type Description Default <code>reply_to</code> <code>ActorRef[TopologySnapshot]</code> <p>Ref that will receive <code>TopologySnapshot</code> messages.</p> required"},{"location":"reference/cluster/#casty.SubscribeTopology.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.SubscribeTopology.__init__","title":"<code>__init__(reply_to)</code>","text":""},{"location":"reference/cluster/#casty.UnsubscribeTopology","title":"<code>casty.UnsubscribeTopology</code>  <code>dataclass</code>","text":"<p>Request to stop receiving topology updates.</p> <p>Parameters:</p> Name Type Description Default <code>subscriber</code> <code>ActorRef[TopologySnapshot]</code> <p>The ref to remove from the subscriber list.</p> required"},{"location":"reference/cluster/#casty.UnsubscribeTopology.subscriber","title":"<code>subscriber</code>  <code>instance-attribute</code>","text":""},{"location":"reference/cluster/#casty.UnsubscribeTopology.__init__","title":"<code>__init__(subscriber)</code>","text":""},{"location":"reference/config/","title":"Configuration","text":""},{"location":"reference/config/#casty.CastyConfig","title":"<code>casty.CastyConfig</code>  <code>dataclass</code>","text":"<p>Top-level configuration container for a Casty actor system.</p> <p>Holds system-wide defaults, cluster settings, and per-actor overrides. Typically created via <code>load_config()</code> but can be constructed manually.</p> <p>Parameters:</p> Name Type Description Default <code>system_name</code> <code>str</code> <p>Logical name of the actor system.</p> <code>'casty'</code> <code>cluster</code> <code>ClusterConfig | None</code> <p>Cluster settings (<code>None</code> for local-only systems).</p> <code>None</code> <code>transport</code> <code>TransportConfig</code> <p>Transport-layer tuning.</p> <code>TransportConfig()</code> <code>serialization</code> <code>SerializationConfig</code> <p>Serialization settings for inter-node communication.</p> <code>SerializationConfig()</code> <code>gossip</code> <code>GossipConfig</code> <p>Gossip protocol tuning.</p> <code>GossipConfig()</code> <code>heartbeat</code> <code>HeartbeatConfig</code> <p>Heartbeat exchange tuning.</p> <code>HeartbeatConfig()</code> <code>failure_detector</code> <code>FailureDetectorConfig</code> <p>Phi accrual failure detector tuning.</p> <code>FailureDetectorConfig()</code> <code>defaults_mailbox</code> <code>MailboxConfig</code> <p>System-wide default mailbox settings.</p> <code>MailboxConfig()</code> <code>defaults_supervision</code> <code>SupervisionConfig</code> <p>System-wide default supervision settings.</p> <code>SupervisionConfig()</code> <code>defaults_sharding</code> <code>ShardingConfig</code> <p>System-wide default sharding settings.</p> <code>ShardingConfig()</code> <code>defaults_replication</code> <code>ReplicationConfig</code> <p>System-wide default replication settings.</p> <code>ReplicationConfig()</code> <code>actors</code> <code>tuple[ActorConfig, ...]</code> <p>Per-actor overrides matched by name pattern.</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = CastyConfig(system_name=\"my-app\")\n&gt;&gt;&gt; config.system_name\n'my-app'\n</code></pre> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"casty.toml\"))\n</code></pre>"},{"location":"reference/config/#casty.CastyConfig.system_name","title":"<code>system_name = 'casty'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.cluster","title":"<code>cluster = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.tls","title":"<code>tls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.transport","title":"<code>transport = field(default_factory=TransportConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.serialization","title":"<code>serialization = field(default_factory=SerializationConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.gossip","title":"<code>gossip = field(default_factory=GossipConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.heartbeat","title":"<code>heartbeat = field(default_factory=HeartbeatConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.failure_detector","title":"<code>failure_detector = field(default_factory=FailureDetectorConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.defaults_mailbox","title":"<code>defaults_mailbox = field(default_factory=MailboxConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.defaults_supervision","title":"<code>defaults_supervision = field(default_factory=SupervisionConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.defaults_sharding","title":"<code>defaults_sharding = field(default_factory=ShardingConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.defaults_replication","title":"<code>defaults_replication = field(default_factory=ReplicationConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.suppress_dead_letters_on_shutdown","title":"<code>suppress_dead_letters_on_shutdown = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.actors","title":"<code>actors = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CastyConfig.resolve_actor","title":"<code>resolve_actor(name)</code>","text":"<p>Resolve the effective configuration for an actor by name.</p> <p>Merges the first matching <code>ActorConfig</code> override (if any) on top of the system defaults.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Actor name to resolve.</p> required <p>Returns:</p> Type Description <code>ResolvedActorConfig</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = CastyConfig()\n&gt;&gt;&gt; resolved = cfg.resolve_actor(\"my-actor\")\n&gt;&gt;&gt; resolved.supervision.strategy\n'restart'\n</code></pre>"},{"location":"reference/config/#casty.CastyConfig.__init__","title":"<code>__init__(system_name='casty', cluster=None, tls=None, transport=TransportConfig(), serialization=SerializationConfig(), gossip=GossipConfig(), heartbeat=HeartbeatConfig(), failure_detector=FailureDetectorConfig(), defaults_mailbox=MailboxConfig(), defaults_supervision=SupervisionConfig(), defaults_sharding=ShardingConfig(), defaults_replication=ReplicationConfig(), suppress_dead_letters_on_shutdown=False, actors=())</code>","text":""},{"location":"reference/config/#casty.load_config","title":"<code>casty.load_config(path=None)</code>","text":"<p>Load a <code>CastyConfig</code> from a TOML file.</p> <p>If path is <code>None</code>, auto-discovers <code>casty.toml</code> by walking up from the current working directory.  Returns default config if no file is found.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Explicit path to a TOML config file.</p> <code>None</code> <p>Returns:</p> Type Description <code>CastyConfig</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If an explicit path is given but does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = load_config()\n&gt;&gt;&gt; config = load_config(Path(\"casty.toml\"))\n&gt;&gt;&gt; config.system_name\n'casty'\n</code></pre>"},{"location":"reference/config/#casty.discover_config","title":"<code>casty.discover_config(start=None)</code>","text":"<p>Walk up from start (default: cwd) looking for <code>casty.toml</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Path | None</code> <p>Directory to start searching from.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path | None</code> <p>Path to the discovered config file, or <code>None</code> if not found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discover_config(Path(\"/my/project\"))\nPosixPath('/my/project/casty.toml')\n</code></pre>"},{"location":"reference/config/#casty.ActorConfig","title":"<code>casty.ActorConfig</code>  <code>dataclass</code>","text":"<p>Per-actor configuration override matched by name pattern.</p> <p>When <code>CastyConfig.resolve_actor(name)</code> is called, the first <code>ActorConfig</code> whose <code>pattern</code> matches name supplies overrides for mailbox, supervision, sharding, and replication settings.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>Pattern[str]</code> <p>Regex matched against actor names via <code>fullmatch</code>.</p> required <code>mailbox_overrides</code> <code>dict[str, Any]</code> <p>Fields to override in <code>MailboxConfig</code>.</p> required <code>supervision_overrides</code> <code>dict[str, Any]</code> <p>Fields to override in <code>SupervisionConfig</code>.</p> required <code>sharding_overrides</code> <code>dict[str, Any]</code> <p>Fields to override in <code>ShardingConfig</code>.</p> required <code>replication_overrides</code> <code>dict[str, Any]</code> <p>Fields to override in <code>ReplicationConfig</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ActorConfig(\n...     pattern=re.compile(\"worker-.*\"),\n...     mailbox_overrides={\"capacity\": 5000},\n...     supervision_overrides={},\n...     sharding_overrides={},\n...     replication_overrides={},\n... )\nActorConfig(pattern=..., ...)\n</code></pre>"},{"location":"reference/config/#casty.ActorConfig.pattern","title":"<code>pattern</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ActorConfig.mailbox_overrides","title":"<code>mailbox_overrides</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ActorConfig.supervision_overrides","title":"<code>supervision_overrides</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ActorConfig.sharding_overrides","title":"<code>sharding_overrides</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ActorConfig.replication_overrides","title":"<code>replication_overrides</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ActorConfig.__init__","title":"<code>__init__(pattern, mailbox_overrides, supervision_overrides, sharding_overrides, replication_overrides)</code>","text":""},{"location":"reference/config/#casty.CompressionAlgorithm","title":"<code>casty.CompressionAlgorithm = Literal['zlib', 'gzip', 'bz2', 'lzma', 'lz4']</code>","text":""},{"location":"reference/config/#casty.CompressionConfig","title":"<code>casty.CompressionConfig</code>  <code>dataclass</code>","text":"<p>Compression settings for serialized messages.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>CompressionAlgorithm</code> <p>Compression algorithm: <code>\"zlib\"</code>, <code>\"gzip\"</code>, <code>\"bz2\"</code>, or <code>\"lzma\"</code>.</p> <code>'zlib'</code> <code>level</code> <code>int</code> <p>Compression level (0-9 for zlib/gzip/bz2, 0-9 for lzma preset).</p> <code>6</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CompressionConfig(algorithm=\"gzip\", level=9)\nCompressionConfig(algorithm='gzip', level=9)\n</code></pre>"},{"location":"reference/config/#casty.CompressionConfig.algorithm","title":"<code>algorithm = 'zlib'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CompressionConfig.level","title":"<code>level = 6</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.CompressionConfig.__init__","title":"<code>__init__(algorithm='zlib', level=6)</code>","text":""},{"location":"reference/config/#casty.MailboxConfig","title":"<code>casty.MailboxConfig</code>  <code>dataclass</code>","text":"<p>Default mailbox settings applied to every actor unless overridden.</p> <p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int | None</code> <p>Maximum number of messages in the mailbox. <code>None</code> for unbounded.</p> <code>None</code> <code>strategy</code> <code>MailboxStrategy</code> <p>Overflow strategy: <code>\"drop_new\"</code>, <code>\"drop_oldest\"</code>, or <code>\"reject\"</code>.</p> <code>'drop_new'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; MailboxConfig(capacity=500, strategy=\"drop_oldest\")\nMailboxConfig(capacity=500, strategy='drop_oldest')\n</code></pre>"},{"location":"reference/config/#casty.MailboxConfig.capacity","title":"<code>capacity = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.MailboxConfig.strategy","title":"<code>strategy = 'drop_new'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.MailboxConfig.__init__","title":"<code>__init__(capacity=None, strategy='drop_new')</code>","text":""},{"location":"reference/config/#casty.SerializationConfig","title":"<code>casty.SerializationConfig</code>  <code>dataclass</code>","text":"<p>Serialization settings for inter-node communication.</p> <p>Parameters:</p> Name Type Description Default <code>serializer</code> <code>SerializerKind</code> <p>Serializer implementation: <code>\"pickle\"</code> or <code>\"json\"</code>.</p> <code>'pickle'</code> <code>compression</code> <code>CompressionConfig | None</code> <p>Compression settings. <code>None</code> means no compression (opt-in).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SerializationConfig(serializer=\"pickle\", compression=CompressionConfig())\nSerializationConfig(serializer='pickle', compression=CompressionConfig(...))\n</code></pre>"},{"location":"reference/config/#casty.SerializationConfig.serializer","title":"<code>serializer = 'pickle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.SerializationConfig.compression","title":"<code>compression = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.SerializationConfig.__init__","title":"<code>__init__(serializer='pickle', compression=None)</code>","text":""},{"location":"reference/config/#casty.SerializerKind","title":"<code>casty.SerializerKind = Literal['pickle', 'json', 'msgpack', 'cloudpickle']</code>","text":""},{"location":"reference/config/#casty.SupervisionConfig","title":"<code>casty.SupervisionConfig</code>  <code>dataclass</code>","text":"<p>Default supervision settings for child actors.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>One of <code>\"restart\"</code>, <code>\"stop\"</code>, or <code>\"escalate\"</code>.</p> <code>'restart'</code> <code>max_restarts</code> <code>int</code> <p>Maximum restarts allowed within the time window.</p> <code>3</code> <code>within_seconds</code> <code>float</code> <p>Rolling window (seconds) for restart counting.</p> <code>60.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SupervisionConfig(strategy=\"stop\")\nSupervisionConfig(strategy='stop', max_restarts=3, within_seconds=60.0)\n</code></pre>"},{"location":"reference/config/#casty.SupervisionConfig.strategy","title":"<code>strategy = 'restart'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.SupervisionConfig.max_restarts","title":"<code>max_restarts = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.SupervisionConfig.within_seconds","title":"<code>within_seconds = 60.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.SupervisionConfig.__init__","title":"<code>__init__(strategy='restart', max_restarts=3, within_seconds=60.0)</code>","text":""},{"location":"reference/config/#casty.ShardingConfig","title":"<code>casty.ShardingConfig</code>  <code>dataclass</code>","text":"<p>Default sharding settings.</p> <p>Parameters:</p> Name Type Description Default <code>num_shards</code> <code>int</code> <p>Number of virtual shards to distribute across the cluster.</p> <code>256</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ShardingConfig(num_shards=512)\nShardingConfig(num_shards=512)\n</code></pre>"},{"location":"reference/config/#casty.ShardingConfig.num_shards","title":"<code>num_shards = 256</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ShardingConfig.__init__","title":"<code>__init__(num_shards=256)</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig","title":"<code>casty.FailureDetectorConfig</code>  <code>dataclass</code>","text":"<p>Phi accrual failure detector tuning (Hayashibara et al.).</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Phi value above which a node is considered unreachable.</p> <code>8.0</code> <code>max_sample_size</code> <code>int</code> <p>Maximum heartbeat interval samples to retain.</p> <code>200</code> <code>min_std_deviation_ms</code> <code>float</code> <p>Floor for the standard deviation estimate (ms).</p> <code>100.0</code> <code>acceptable_heartbeat_pause_ms</code> <code>float</code> <p>Grace period added to the heartbeat interval estimate (ms).</p> <code>0.0</code> <code>first_heartbeat_estimate_ms</code> <code>float</code> <p>Initial heartbeat interval estimate before real samples arrive (ms).</p> <code>1000.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; FailureDetectorConfig(threshold=12.0, max_sample_size=500)\nFailureDetectorConfig(threshold=12.0, max_sample_size=500, ...)\n</code></pre>"},{"location":"reference/config/#casty.FailureDetectorConfig.threshold","title":"<code>threshold = 8.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig.max_sample_size","title":"<code>max_sample_size = 200</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig.min_std_deviation_ms","title":"<code>min_std_deviation_ms = 100.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig.acceptable_heartbeat_pause_ms","title":"<code>acceptable_heartbeat_pause_ms = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig.first_heartbeat_estimate_ms","title":"<code>first_heartbeat_estimate_ms = 1000.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.FailureDetectorConfig.__init__","title":"<code>__init__(threshold=8.0, max_sample_size=200, min_std_deviation_ms=100.0, acceptable_heartbeat_pause_ms=0.0, first_heartbeat_estimate_ms=1000.0)</code>","text":""},{"location":"reference/config/#casty.GossipConfig","title":"<code>casty.GossipConfig</code>  <code>dataclass</code>","text":"<p>Gossip protocol tuning.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>Seconds between gossip rounds.</p> <code>1.0</code> <code>fanout</code> <code>int</code> <p>Number of peers contacted per gossip round.</p> <code>3</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; GossipConfig(interval=0.5, fanout=5)\nGossipConfig(interval=0.5, fanout=5)\n</code></pre>"},{"location":"reference/config/#casty.GossipConfig.interval","title":"<code>interval = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.GossipConfig.fanout","title":"<code>fanout = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.GossipConfig.__init__","title":"<code>__init__(interval=1.0, fanout=3)</code>","text":""},{"location":"reference/config/#casty.HeartbeatConfig","title":"<code>casty.HeartbeatConfig</code>  <code>dataclass</code>","text":"<p>Heartbeat exchange tuning.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>Seconds between heartbeat sends to each monitored peer.</p> <code>0.5</code> <code>availability_check_interval</code> <code>float</code> <p>Seconds between phi-accrual availability checks.</p> <code>2.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; HeartbeatConfig(interval=1.0, availability_check_interval=5.0)\nHeartbeatConfig(interval=1.0, availability_check_interval=5.0)\n</code></pre>"},{"location":"reference/config/#casty.HeartbeatConfig.interval","title":"<code>interval = 0.5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.HeartbeatConfig.availability_check_interval","title":"<code>availability_check_interval = 2.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.HeartbeatConfig.__init__","title":"<code>__init__(interval=0.5, availability_check_interval=2.0)</code>","text":""},{"location":"reference/config/#casty.ResolvedActorConfig","title":"<code>casty.ResolvedActorConfig</code>  <code>dataclass</code>","text":"<p>Fully resolved configuration for a single actor.</p> <p>Produced by <code>CastyConfig.resolve_actor(name)</code> by merging per-actor overrides on top of the system defaults.</p> <p>Parameters:</p> Name Type Description Default <code>mailbox</code> <code>MailboxConfig</code> <p>Resolved mailbox settings.</p> required <code>supervision</code> <code>SupervisionConfig</code> <p>Resolved supervision settings.</p> required <code>sharding</code> <code>ShardingConfig</code> <p>Resolved sharding settings.</p> required <code>replication</code> <code>ReplicationConfig</code> <p>Resolved replication settings.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = CastyConfig()\n&gt;&gt;&gt; resolved = cfg.resolve_actor(\"my-actor\")\n&gt;&gt;&gt; resolved.mailbox.capacity\n1000\n</code></pre>"},{"location":"reference/config/#casty.ResolvedActorConfig.mailbox","title":"<code>mailbox</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ResolvedActorConfig.supervision","title":"<code>supervision</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ResolvedActorConfig.sharding","title":"<code>sharding</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ResolvedActorConfig.replication","title":"<code>replication</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.ResolvedActorConfig.__init__","title":"<code>__init__(mailbox, supervision, sharding, replication)</code>","text":""},{"location":"reference/config/#casty.TransportConfig","title":"<code>casty.TransportConfig</code>  <code>dataclass</code>","text":"<p>Transport-layer tuning.</p> <p>Parameters:</p> Name Type Description Default <code>max_pending_per_path</code> <code>int</code> <p>Maximum queued outbound messages per remote actor path.</p> <code>64</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; TransportConfig(max_pending_per_path=128)\nTransportConfig(max_pending_per_path=128)\n</code></pre>"},{"location":"reference/config/#casty.TransportConfig.max_pending_per_path","title":"<code>max_pending_per_path = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#casty.TransportConfig.__init__","title":"<code>__init__(max_pending_per_path=64)</code>","text":""},{"location":"reference/context/","title":"ActorContext","text":""},{"location":"reference/context/#casty.ActorContext","title":"<code>casty.ActorContext</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for the context available to actor behavior handlers.</p>"},{"location":"reference/context/#casty.ActorContext.self","title":"<code>self</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.ActorContext.system","title":"<code>system</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.ActorContext.log","title":"<code>log</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.ActorContext.spawn","title":"<code>spawn(behavior, name, *, mailbox=None)</code>","text":""},{"location":"reference/context/#casty.ActorContext.stop","title":"<code>stop(ref)</code>","text":""},{"location":"reference/context/#casty.ActorContext.watch","title":"<code>watch(ref)</code>","text":""},{"location":"reference/context/#casty.ActorContext.unwatch","title":"<code>unwatch(ref)</code>","text":""},{"location":"reference/context/#casty.ActorContext.on_stop","title":"<code>on_stop(callback)</code>","text":""},{"location":"reference/context/#casty.ActorContext.register_interceptor","title":"<code>register_interceptor(interceptor)</code>","text":""},{"location":"reference/context/#casty.ActorContext.pipe_to_self","title":"<code>pipe_to_self(coro, mapper=None, on_failure=None)</code>","text":"<pre><code>pipe_to_self(\n    coro: Awaitable[M],\n    *,\n    on_failure: Callable[[Exception], M] | None = None,\n) -&gt; None\n</code></pre><pre><code>pipe_to_self(\n    coro: Awaitable[T],\n    mapper: Callable[[T], M],\n    on_failure: Callable[[Exception], M] | None = None,\n) -&gt; None\n</code></pre>"},{"location":"reference/context/#casty.Interceptor","title":"<code>casty.Interceptor = Callable[[object], bool]</code>","text":""},{"location":"reference/context/#casty.System","title":"<code>casty.System</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol exposing the actor system's public API to behaviors.</p>"},{"location":"reference/context/#casty.System.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.System.event_stream","title":"<code>event_stream</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.System.scheduler","title":"<code>scheduler</code>  <code>property</code>","text":""},{"location":"reference/context/#casty.System.__make_ref__","title":"<code>__make_ref__(id, deliver)</code>","text":""},{"location":"reference/context/#casty.System.spawn","title":"<code>spawn(behavior, name, *, mailbox=None)</code>","text":""},{"location":"reference/context/#casty.System.ask","title":"<code>ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":""},{"location":"reference/context/#casty.System.ask_or_none","title":"<code>ask_or_none(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":""},{"location":"reference/context/#casty.System.lookup","title":"<code>lookup(path)</code>","text":""},{"location":"reference/context/#casty.System.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":""},{"location":"reference/distributed/","title":"Distributed","text":""},{"location":"reference/distributed/#casty.Distributed","title":"<code>casty.Distributed</code>","text":"<p>Facade for creating distributed data structures.</p> <p>Spawns sharded actor regions lazily, per structure name.</p> <p>Parameters:</p> Name Type Description Default <code>gateway</code> <code>EntityGateway</code> <p>Backend that provides access to sharded entities.  Both <code>ClusteredActorSystem</code> and <code>ClusterClient</code> satisfy this.</p> required <code>journal</code> <code>EventJournal | None</code> <p>If provided, structures use event sourcing for durability.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = Distributed(system)\n&gt;&gt;&gt; counter = d.counter(\"hits\", shards=50)\n&gt;&gt;&gt; users = d.map[str, User](\"users\", shards=10)\n&gt;&gt;&gt; tags = d.set[str](\"tags\")\n&gt;&gt;&gt; jobs = d.queue[Task](\"jobs\")\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.map","title":"<code>map</code>  <code>property</code>","text":"<p>Create a distributed map via <code>d.map[K, V](\"name\", shards=10)</code>.</p> <p>Returns:</p> Type Description <code>MapAccessor</code> <p>A subscriptable accessor parameterized by key and value types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; users = d.map[str, dict](\"users\")\n&gt;&gt;&gt; await users.put(\"alice\", {\"age\": 30})\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.set","title":"<code>set</code>  <code>property</code>","text":"<p>Create a distributed set via <code>d.set[V](\"name\", shards=10)</code>.</p> <p>Returns:</p> Type Description <code>SetAccessor</code> <p>A subscriptable accessor parameterized by the element type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tags = d.set[str](\"tags\")\n&gt;&gt;&gt; await tags.add(\"python\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.queue","title":"<code>queue</code>  <code>property</code>","text":"<p>Create a distributed queue via <code>d.queue[V](\"name\", shards=10)</code>.</p> <p>Returns:</p> Type Description <code>QueueAccessor</code> <p>A subscriptable accessor parameterized by the element type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; jobs = d.queue[str](\"jobs\")\n&gt;&gt;&gt; await jobs.enqueue(\"task-1\")\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.__init__","title":"<code>__init__(gateway, *, journal=None)</code>","text":""},{"location":"reference/distributed/#casty.Distributed.counter","title":"<code>counter(name, *, shards=100, timeout=5.0)</code>","text":"<p>Create a distributed counter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logical counter name (used as entity ID).</p> required <code>shards</code> <code>int</code> <p>Number of shards for the backing region.</p> <code>100</code> <code>timeout</code> <code>float</code> <p>Default timeout for counter operations.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>Counter</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; counter = d.counter(\"page-views\")\n&gt;&gt;&gt; await counter.increment()\n1\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.lock","title":"<code>lock(name, *, shards=100, timeout=5.0)</code>","text":"<p>Create a distributed lock.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logical lock name (used as entity ID).</p> required <code>shards</code> <code>int</code> <p>Number of shards for the backing region.</p> <code>100</code> <code>timeout</code> <code>float</code> <p>Default timeout for lock operations.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>Lock</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lock = d.lock(\"my-resource\")\n&gt;&gt;&gt; await lock.acquire()\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.semaphore","title":"<code>semaphore(name, permits, *, shards=100, timeout=5.0)</code>","text":"<p>Create a distributed semaphore with permits concurrent holders.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logical semaphore name (used as entity ID).</p> required <code>permits</code> <code>int</code> <p>Maximum number of concurrent holders.</p> required <code>shards</code> <code>int</code> <p>Number of shards for the backing region.</p> <code>100</code> <code>timeout</code> <code>float</code> <p>Default timeout for semaphore operations.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>Semaphore</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sem = d.semaphore(\"pool\", permits=5)\n&gt;&gt;&gt; await sem.acquire()\n</code></pre>"},{"location":"reference/distributed/#casty.Distributed.barrier","title":"<code>barrier(name, *, node_id=None, shards=10, timeout=60.0)</code>","text":"<p>Create a distributed barrier.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logical barrier name (used as entity ID).</p> required <code>node_id</code> <code>str | None</code> <p>Identifier for this node (defaults to <code>host:port</code>). Required when using <code>ClusterClient</code> as the gateway.</p> <code>None</code> <code>shards</code> <code>int</code> <p>Number of shards for the backing region.</p> <code>10</code> <code>timeout</code> <code>float</code> <p>Default timeout for the barrier wait.</p> <code>60.0</code> <p>Returns:</p> Type Description <code>Barrier</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; barrier = d.barrier(\"init-sync\")\n&gt;&gt;&gt; await barrier.arrive(expected=3)\n</code></pre>"},{"location":"reference/distributed/#casty.EntityGateway","title":"<code>casty.EntityGateway</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for backends that provide access to sharded entities.</p> <p>Both <code>ClusteredActorSystem</code> and <code>ClusterClient</code> satisfy this protocol structurally, allowing <code>Distributed</code> to work with either.</p>"},{"location":"reference/distributed/#casty.EntityGateway.region_ref","title":"<code>region_ref(key, factory, *, num_shards)</code>","text":"<p>Return a ref that routes <code>ShardEnvelope</code> to the named region.</p>"},{"location":"reference/distributed/#casty.EntityGateway.entity_ask","title":"<code>entity_ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Send a message and wait for a reply.</p>"},{"location":"reference/distributed/#casty.Barrier","title":"<code>casty.Barrier</code>","text":"<p>Client for a distributed barrier backed by a sharded actor.</p> <p>Multiple nodes call <code>arrive(expected)</code> and all block until expected nodes have arrived.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[BarrierMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Barrier name (used as entity ID).</p> required <code>node_id</code> <code>str</code> <p>Identifier for this node (typically <code>host:port</code>).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for the barrier wait.</p> <code>60.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; barrier = d.barrier(\"init-sync\")\n&gt;&gt;&gt; await barrier.arrive(expected=3)\n</code></pre>"},{"location":"reference/distributed/#casty.Barrier.__init__","title":"<code>__init__(*, gateway, region_ref, name, node_id, timeout=60.0)</code>","text":""},{"location":"reference/distributed/#casty.Barrier.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this barrier, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Barrier.arrive","title":"<code>arrive(expected)</code>  <code>async</code>","text":"<p>Block until expected nodes have reached this barrier.</p> <p>Parameters:</p> Name Type Description Default <code>expected</code> <code>int</code> <p>Number of nodes that must arrive before all are released.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; await barrier.arrive(expected=3)\n</code></pre>"},{"location":"reference/distributed/#casty.Counter","title":"<code>casty.Counter</code>","text":"<p>Client for a distributed counter backed by a sharded actor.</p> <p>Provides <code>increment</code>, <code>decrement</code>, and <code>get</code> operations that route through the shard proxy to the correct entity.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[CounterMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Counter name (used as entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; counter = d.counter(\"hits\")\n&gt;&gt;&gt; await counter.increment(5)\n5\n&gt;&gt;&gt; await counter.get()\n5\n</code></pre>"},{"location":"reference/distributed/#casty.Counter.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Counter.increment","title":"<code>increment(amount=1)</code>  <code>async</code>","text":"<p>Increment the counter and return the new value.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>int</code> <p>Value to add (default <code>1</code>).</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>The counter value after incrementing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await counter.increment()\n1\n</code></pre>"},{"location":"reference/distributed/#casty.Counter.decrement","title":"<code>decrement(amount=1)</code>  <code>async</code>","text":"<p>Decrement the counter and return the new value.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>int</code> <p>Value to subtract (default <code>1</code>).</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>The counter value after decrementing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await counter.decrement()\n-1\n</code></pre>"},{"location":"reference/distributed/#casty.Counter.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this counter, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Counter.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Get the current counter value.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await counter.get()\n0\n</code></pre>"},{"location":"reference/distributed/#casty.Dict","title":"<code>casty.Dict</code>","text":"<p>Client for a distributed key-value map backed by sharded actors.</p> <p>Each key maps to a separate entity actor (entity-per-key pattern).</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[MapEntryMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Map name prefix (combined with key for entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; users = d.map[str, dict](\"users\")\n&gt;&gt;&gt; await users.put(\"alice\", {\"age\": 30})\n&gt;&gt;&gt; await users.get(\"alice\")\n{'age': 30}\n</code></pre>"},{"location":"reference/distributed/#casty.Dict.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Dict.put","title":"<code>put(key, value)</code>  <code>async</code>","text":"<p>Store a value for the key.</p> <p>Returns:</p> Type Description <code>V | None</code> <p>The previous value, or <code>None</code> if the key was new.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await users.put(\"alice\", {\"age\": 30})\n</code></pre>"},{"location":"reference/distributed/#casty.Dict.get","title":"<code>get(key, *, local=False)</code>  <code>async</code>","text":"<p>Get the value for the key, or <code>None</code> if not set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>The key to look up.</p> required <code>local</code> <code>bool</code> <p>Reserved for future local-read optimization (currently unused).</p> <code>False</code> <p>Returns:</p> Type Description <code>V | None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await users.get(\"alice\")\n{'age': 30}\n</code></pre>"},{"location":"reference/distributed/#casty.Dict.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Delete the key.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the key existed, <code>False</code> otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await users.delete(\"alice\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Dict.contains","title":"<code>contains(key)</code>  <code>async</code>","text":"<p>Check whether the key exists.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await users.contains(\"alice\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Lock","title":"<code>casty.Lock</code>","text":"<p>Client for a distributed mutual-exclusion lock.</p> <p>Each <code>Lock</code> instance has a unique owner ID.  <code>acquire</code> blocks until the lock is granted; <code>try_acquire</code> returns immediately.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[LockMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Lock name (used as entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lock = d.lock(\"my-resource\")\n&gt;&gt;&gt; await lock.acquire()\n&gt;&gt;&gt; await lock.release()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Lock.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Lock.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this lock, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Lock.acquire","title":"<code>acquire()</code>  <code>async</code>","text":"<p>Block until the lock is acquired.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await lock.acquire()\n</code></pre>"},{"location":"reference/distributed/#casty.Lock.try_acquire","title":"<code>try_acquire()</code>  <code>async</code>","text":"<p>Try to acquire the lock without blocking.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the lock was acquired, <code>False</code> if already held.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await lock.try_acquire()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Lock.release","title":"<code>release()</code>  <code>async</code>","text":"<p>Release the lock.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if released, <code>False</code> if this instance was not the holder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await lock.release()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Queue","title":"<code>casty.Queue</code>","text":"<p>Client for a distributed FIFO queue backed by a sharded actor.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[QueueMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Queue name (used as entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; jobs = d.queue[str](\"jobs\")\n&gt;&gt;&gt; await jobs.enqueue(\"task-1\")\n&gt;&gt;&gt; await jobs.dequeue()\n'task-1'\n</code></pre>"},{"location":"reference/distributed/#casty.Queue.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Queue.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this queue, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Queue.enqueue","title":"<code>enqueue(value)</code>  <code>async</code>","text":"<p>Append a value to the back of the queue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await jobs.enqueue(\"task-1\")\n</code></pre>"},{"location":"reference/distributed/#casty.Queue.dequeue","title":"<code>dequeue()</code>  <code>async</code>","text":"<p>Remove and return the front value, or <code>None</code> if empty.</p> <p>Returns:</p> Type Description <code>V | None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await jobs.dequeue()\n'task-1'\n</code></pre>"},{"location":"reference/distributed/#casty.Queue.peek","title":"<code>peek()</code>  <code>async</code>","text":"<p>Return the front value without removing, or <code>None</code> if empty.</p> <p>Returns:</p> Type Description <code>V | None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await jobs.peek()\n'task-1'\n</code></pre>"},{"location":"reference/distributed/#casty.Queue.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Return the number of items in the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await jobs.size()\n0\n</code></pre>"},{"location":"reference/distributed/#casty.Semaphore","title":"<code>casty.Semaphore</code>","text":"<p>Client for a distributed counting semaphore.</p> <p>Each <code>Semaphore</code> instance has a unique owner ID.  <code>acquire</code> blocks until a permit is available; <code>try_acquire</code> returns immediately.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[SemaphoreMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Semaphore name (used as entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sem = d.semaphore(\"pool\", permits=3)\n&gt;&gt;&gt; await sem.acquire()\n&gt;&gt;&gt; await sem.release()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Semaphore.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Semaphore.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this semaphore, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Semaphore.acquire","title":"<code>acquire()</code>  <code>async</code>","text":"<p>Block until a permit is acquired.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await sem.acquire()\n</code></pre>"},{"location":"reference/distributed/#casty.Semaphore.try_acquire","title":"<code>try_acquire()</code>  <code>async</code>","text":"<p>Try to acquire a permit without blocking.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if a permit was acquired, <code>False</code> if none available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await sem.try_acquire()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Semaphore.release","title":"<code>release()</code>  <code>async</code>","text":"<p>Release a permit.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if released, <code>False</code> if this instance was not a holder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await sem.release()\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Set","title":"<code>casty.Set</code>","text":"<p>Client for a distributed set backed by a sharded actor.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>ActorSystem</code> <p>The actor system for sending messages.</p> required <code>region_ref</code> <code>ActorRef[ShardEnvelope[SetMsg]]</code> <p>Reference to the shard proxy / region.</p> required <code>name</code> <code>str</code> <p>Set name (used as entity ID).</p> required <code>timeout</code> <code>float</code> <p>Default timeout for each operation.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tags = d.set[str](\"tags\")\n&gt;&gt;&gt; await tags.add(\"python\")\nTrue\n&gt;&gt;&gt; await tags.contains(\"python\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Set.__init__","title":"<code>__init__(*, gateway, region_ref, name, timeout=5.0)</code>","text":""},{"location":"reference/distributed/#casty.Set.destroy","title":"<code>destroy()</code>  <code>async</code>","text":"<p>Destroy this set, stopping the backing entity actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if destroyed.</p>"},{"location":"reference/distributed/#casty.Set.add","title":"<code>add(value)</code>  <code>async</code>","text":"<p>Add a value to the set.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if added, <code>False</code> if already present.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await tags.add(\"python\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Set.remove","title":"<code>remove(value)</code>  <code>async</code>","text":"<p>Remove a value from the set.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if removed, <code>False</code> if not present.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await tags.remove(\"python\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Set.contains","title":"<code>contains(value)</code>  <code>async</code>","text":"<p>Check if a value is in the set.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await tags.contains(\"python\")\nTrue\n</code></pre>"},{"location":"reference/distributed/#casty.Set.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Get the number of elements in the set.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await tags.size()\n1\n</code></pre>"},{"location":"reference/events/","title":"Events","text":""},{"location":"reference/events/#casty.EventStreamMsg","title":"<code>casty.EventStreamMsg = Subscribe[Any] | Unsubscribe[Any] | Publish</code>","text":""},{"location":"reference/events/#casty.EventStreamSubscribe","title":"<code>casty.EventStreamSubscribe</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.EventStreamSubscribe.event_type","title":"<code>event_type</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.EventStreamSubscribe.handler","title":"<code>handler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.EventStreamSubscribe.__init__","title":"<code>__init__(event_type, handler)</code>","text":""},{"location":"reference/events/#casty.EventStreamUnsubscribe","title":"<code>casty.EventStreamUnsubscribe</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.EventStreamUnsubscribe.event_type","title":"<code>event_type</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.EventStreamUnsubscribe.handler","title":"<code>handler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.EventStreamUnsubscribe.__init__","title":"<code>__init__(event_type, handler)</code>","text":""},{"location":"reference/events/#casty.Publish","title":"<code>casty.Publish</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.Publish.event","title":"<code>event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.Publish.__init__","title":"<code>__init__(event)</code>","text":""},{"location":"reference/events/#casty.event_stream_actor","title":"<code>casty.event_stream_actor()</code>","text":""},{"location":"reference/events/#casty.ActorStarted","title":"<code>casty.ActorStarted</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.ActorStarted.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.ActorStarted.__init__","title":"<code>__init__(ref)</code>","text":""},{"location":"reference/events/#casty.ActorStopped","title":"<code>casty.ActorStopped</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.ActorStopped.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.ActorStopped.__init__","title":"<code>__init__(ref)</code>","text":""},{"location":"reference/events/#casty.ActorRestarted","title":"<code>casty.ActorRestarted</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.ActorRestarted.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.ActorRestarted.exception","title":"<code>exception</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.ActorRestarted.__init__","title":"<code>__init__(ref, exception)</code>","text":""},{"location":"reference/events/#casty.DeadLetter","title":"<code>casty.DeadLetter</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.DeadLetter.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.DeadLetter.intended_ref","title":"<code>intended_ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.DeadLetter.__init__","title":"<code>__init__(message, intended_ref)</code>","text":""},{"location":"reference/events/#casty.UnhandledMessage","title":"<code>casty.UnhandledMessage</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#casty.UnhandledMessage.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.UnhandledMessage.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.UnhandledMessage.__init__","title":"<code>__init__(message, ref)</code>","text":""},{"location":"reference/events/#casty.MemberUp","title":"<code>casty.MemberUp</code>  <code>dataclass</code>","text":"<p>Emitted when a cluster member transitions to the <code>up</code> status.</p> <p>Parameters:</p> Name Type Description Default <code>member</code> <code>Member</code> <p>The cluster member that came up.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import MemberUp\n&gt;&gt;&gt; event = MemberUp(member=member)\n</code></pre>"},{"location":"reference/events/#casty.MemberUp.member","title":"<code>member</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.MemberUp.__init__","title":"<code>__init__(member)</code>","text":""},{"location":"reference/events/#casty.MemberLeft","title":"<code>casty.MemberLeft</code>  <code>dataclass</code>","text":"<p>Emitted when a cluster member leaves the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>member</code> <code>Member</code> <p>The cluster member that left.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import MemberLeft\n&gt;&gt;&gt; event = MemberLeft(member=member)\n</code></pre>"},{"location":"reference/events/#casty.MemberLeft.member","title":"<code>member</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.MemberLeft.__init__","title":"<code>__init__(member)</code>","text":""},{"location":"reference/events/#casty.UnreachableMember","title":"<code>casty.UnreachableMember</code>  <code>dataclass</code>","text":"<p>Emitted when the failure detector marks a member as unreachable.</p> <p>Parameters:</p> Name Type Description Default <code>member</code> <code>Member</code> <p>The cluster member that became unreachable.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import UnreachableMember\n&gt;&gt;&gt; event = UnreachableMember(member=member)\n</code></pre>"},{"location":"reference/events/#casty.UnreachableMember.member","title":"<code>member</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.UnreachableMember.__init__","title":"<code>__init__(member)</code>","text":""},{"location":"reference/events/#casty.ReachableMember","title":"<code>casty.ReachableMember</code>  <code>dataclass</code>","text":"<p>Emitted when a previously unreachable member becomes reachable again.</p> <p>Parameters:</p> Name Type Description Default <code>member</code> <code>Member</code> <p>The cluster member that became reachable.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import ReachableMember\n&gt;&gt;&gt; event = ReachableMember(member=member)\n</code></pre>"},{"location":"reference/events/#casty.ReachableMember.member","title":"<code>member</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.ReachableMember.__init__","title":"<code>__init__(member)</code>","text":""},{"location":"reference/events/#casty.Terminated","title":"<code>casty.Terminated</code>  <code>dataclass</code>","text":"<p>Signal sent when a watched actor is stopped.</p> <p>Delivered to actors that called <code>ctx.watch()</code> on the terminated actor's reference.</p>"},{"location":"reference/events/#casty.Terminated.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#casty.Terminated.__init__","title":"<code>__init__(ref)</code>","text":""},{"location":"reference/failure-detector/","title":"Failure Detector","text":""},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector","title":"<code>casty.PhiAccrualFailureDetector</code>","text":"<p>Phi accrual failure detector (Hayashibara et al.).</p> <p>Outputs a continuous suspicion level (phi) instead of a binary alive/dead signal.  <code>phi = -log10(1 - CDF(elapsed))</code> where CDF is the normal distribution fitted to the observed heartbeat interval history.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Phi value above which a node is considered unreachable.  Higher values tolerate more jitter but detect failures more slowly.</p> <code>8.0</code> <code>max_sample_size</code> <code>int</code> <p>Maximum number of heartbeat intervals to keep per node.</p> <code>200</code> <code>min_std_deviation_ms</code> <code>float</code> <p>Floor for the standard deviation estimate, preventing overly aggressive detection when intervals are very stable.</p> <code>100.0</code> <code>acceptable_heartbeat_pause_ms</code> <code>float</code> <p>Additional grace period added to the mean estimate, accounting for expected pauses (e.g. GC).</p> <code>0.0</code> <code>first_heartbeat_estimate_ms</code> <code>float</code> <p>Assumed mean interval before enough samples have been collected.</p> <code>1000.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fd = PhiAccrualFailureDetector(threshold=8.0)\n&gt;&gt;&gt; fd.heartbeat(\"node-1\")\n&gt;&gt;&gt; fd.is_available(\"node-1\")\nTrue\n&gt;&gt;&gt; fd.phi(\"unknown-node\")\n0.0\n</code></pre>"},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.tracked_nodes","title":"<code>tracked_nodes</code>  <code>property</code>","text":"<p>Return the set of node keys that have received at least one heartbeat.</p> <p>Returns:</p> Type Description <code>frozenset[str]</code> <p>Node identifiers currently being tracked.</p>"},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.__init__","title":"<code>__init__(*, threshold=8.0, max_sample_size=200, min_std_deviation_ms=100.0, acceptable_heartbeat_pause_ms=0.0, first_heartbeat_estimate_ms=1000.0)</code>","text":""},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.heartbeat","title":"<code>heartbeat(node)</code>","text":"<p>Record arrival of a heartbeat from a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Identifier of the node that sent the heartbeat.</p> required"},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.phi","title":"<code>phi(node)</code>","text":"<p>Calculate the suspicion level for node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Identifier of the node to evaluate.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The phi value.  <code>0.0</code> if node has never sent a heartbeat; <code>inf</code> if the node is almost certainly down.</p>"},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.remove","title":"<code>remove(node)</code>","text":"<p>Stop tracking a node entirely.</p> <p>Called when a node is marked <code>down</code> so it no longer accumulates stale history in the failure detector.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Identifier of the node to remove.</p> required"},{"location":"reference/failure-detector/#casty.PhiAccrualFailureDetector.is_available","title":"<code>is_available(node)</code>","text":"<p>Check if a node is considered available (phi below threshold).</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Identifier of the node to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>phi(node) &lt; threshold</code>.</p>"},{"location":"reference/journal/","title":"Event Sourcing","text":""},{"location":"reference/journal/#casty.EventJournal","title":"<code>casty.EventJournal</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for event journal backends.</p> <p>Any class that implements <code>persist</code>, <code>load</code>, <code>save_snapshot</code>, and <code>load_snapshot</code> satisfies this protocol via structural subtyping.</p> <p>The <code>kind</code> property tells the replication layer whether replicas need to persist events themselves (<code>local</code>) or can skip persistence because the store is shared (<code>centralized</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import InMemoryJournal\n&gt;&gt;&gt; journal: EventJournal = InMemoryJournal()\n</code></pre>"},{"location":"reference/journal/#casty.EventJournal.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>Whether this journal is node-local or centralized.</p>"},{"location":"reference/journal/#casty.EventJournal.persist","title":"<code>persist(entity_id, events)</code>  <code>async</code>","text":"<p>Append events to the journal for a given entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>events</code> <code>Sequence[PersistedEvent[Any]]</code> <p>Events to persist, in order.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; await journal.persist(\"acct-1\", [PersistedEvent(1, \"evt\", 0.0)])\n</code></pre>"},{"location":"reference/journal/#casty.EventJournal.load","title":"<code>load(entity_id, from_sequence_nr=0)</code>  <code>async</code>","text":"<p>Load events for an entity starting from a sequence number.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>from_sequence_nr</code> <code>int</code> <p>Minimum sequence number (inclusive). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[PersistedEvent[Any]]</code> <p>Events with <code>sequence_nr &gt;= from_sequence_nr</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; events = await journal.load(\"acct-1\", from_sequence_nr=5)\n</code></pre>"},{"location":"reference/journal/#casty.EventJournal.save_snapshot","title":"<code>save_snapshot(entity_id, snapshot)</code>  <code>async</code>","text":"<p>Save a snapshot for an entity, replacing any previous one.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>snapshot</code> <code>Snapshot[Any]</code> <p>The snapshot to store.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; snap = Snapshot(sequence_nr=10, state=100, timestamp=1.0)\n&gt;&gt;&gt; await journal.save_snapshot(\"acct-1\", snap)\n</code></pre>"},{"location":"reference/journal/#casty.EventJournal.load_snapshot","title":"<code>load_snapshot(entity_id)</code>  <code>async</code>","text":"<p>Load the latest snapshot for an entity, if one exists.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <p>Returns:</p> Type Description <code>Snapshot[Any] | None</code> <p>The stored snapshot, or <code>None</code> if no snapshot exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; snap = await journal.load_snapshot(\"acct-1\")\n</code></pre>"},{"location":"reference/journal/#casty.InMemoryJournal","title":"<code>casty.InMemoryJournal</code>","text":"<p>In-memory event journal for testing and development.</p> <p>Stores events and snapshots in plain dictionaries. Data is lost when the process exits. Satisfies the <code>EventJournal</code> protocol.</p> <p>Always <code>local</code> \u2014 each process has its own independent store.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import InMemoryJournal, PersistedEvent\n&gt;&gt;&gt; journal = InMemoryJournal()\n&gt;&gt;&gt; await journal.persist(\"e1\", [PersistedEvent(1, \"created\", 0.0)])\n&gt;&gt;&gt; events = await journal.load(\"e1\")\n&gt;&gt;&gt; len(events)\n1\n</code></pre>"},{"location":"reference/journal/#casty.InMemoryJournal.kind","title":"<code>kind</code>  <code>property</code>","text":""},{"location":"reference/journal/#casty.InMemoryJournal.__init__","title":"<code>__init__()</code>","text":""},{"location":"reference/journal/#casty.InMemoryJournal.persist","title":"<code>persist(entity_id, events)</code>  <code>async</code>","text":"<p>Append events to the in-memory store.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>events</code> <code>Sequence[PersistedEvent[Any]]</code> <p>Events to persist.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; await journal.persist(\"e1\", [PersistedEvent(1, \"x\", 0.0)])\n</code></pre>"},{"location":"reference/journal/#casty.InMemoryJournal.load","title":"<code>load(entity_id, from_sequence_nr=0)</code>  <code>async</code>","text":"<p>Load events from the in-memory store.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>from_sequence_nr</code> <code>int</code> <p>Minimum sequence number (inclusive). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[PersistedEvent[Any]]</code> <p>Matching events in insertion order.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; events = await journal.load(\"e1\", from_sequence_nr=2)\n</code></pre>"},{"location":"reference/journal/#casty.InMemoryJournal.save_snapshot","title":"<code>save_snapshot(entity_id, snapshot)</code>  <code>async</code>","text":"<p>Save a snapshot, replacing any previous one for this entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <code>snapshot</code> <code>Snapshot[Any]</code> <p>The snapshot to store.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; snap = Snapshot(sequence_nr=5, state=\"s\", timestamp=1.0)\n&gt;&gt;&gt; await journal.save_snapshot(\"e1\", snap)\n</code></pre>"},{"location":"reference/journal/#casty.InMemoryJournal.load_snapshot","title":"<code>load_snapshot(entity_id)</code>  <code>async</code>","text":"<p>Load the latest snapshot for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier of the entity.</p> required <p>Returns:</p> Type Description <code>Snapshot[Any] | None</code> <p>The stored snapshot, or <code>None</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; snap = await journal.load_snapshot(\"e1\")\n</code></pre>"},{"location":"reference/journal/#casty.SqliteJournal","title":"<code>casty.SqliteJournal</code>","text":"<p>SQLite-backed event journal for durable persistence.</p> <p>Uses Python's stdlib <code>sqlite3</code> with WAL mode for concurrent reads and <code>asyncio.to_thread</code> for non-blocking I/O. A <code>threading.Lock</code> serializes all writes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the SQLite database file, or <code>\":memory:\"</code> for an in-memory database (useful for testing).</p> <code>':memory:'</code> <code>serialize</code> <code>Callable[[Any], bytes]</code> <p>Serializer for events and snapshot state. Defaults to <code>pickle.dumps</code>.</p> <code>dumps</code> <code>deserialize</code> <code>Callable[[bytes], Any]</code> <p>Deserializer for events and snapshot state. Defaults to <code>pickle.loads</code>.</p> <code>loads</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import SqliteJournal, PersistedEvent\n&gt;&gt;&gt; journal = SqliteJournal()\n&gt;&gt;&gt; await journal.persist(\"e1\", [PersistedEvent(1, \"created\", 0.0)])\n&gt;&gt;&gt; events = await journal.load(\"e1\")\n&gt;&gt;&gt; len(events)\n1\n</code></pre>"},{"location":"reference/journal/#casty.SqliteJournal.kind","title":"<code>kind</code>  <code>property</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.__init__","title":"<code>__init__(path=':memory:', *, serialize=pickle.dumps, deserialize=pickle.loads)</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.persist","title":"<code>persist(entity_id, events)</code>  <code>async</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.load","title":"<code>load(entity_id, from_sequence_nr=0)</code>  <code>async</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.save_snapshot","title":"<code>save_snapshot(entity_id, snapshot)</code>  <code>async</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.load_snapshot","title":"<code>load_snapshot(entity_id)</code>  <code>async</code>","text":""},{"location":"reference/journal/#casty.SqliteJournal.close","title":"<code>close()</code>","text":"<p>Close the underlying SQLite connection.</p>"},{"location":"reference/journal/#casty.JournalKind","title":"<code>casty.JournalKind</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Declares whether a journal backend is node-local or centralized.</p> <p><code>local</code>     Each node has its own independent store (e.g. SQLite file).     Replicas must persist events they receive via <code>ReplicateEvents</code>.</p> <p><code>centralized</code>     All nodes share the same durable store (e.g. PostgreSQL, S3).     Replicas skip persistence \u2014 the primary's write is already visible     to every node.</p>"},{"location":"reference/journal/#casty.JournalKind.local","title":"<code>local = 'local'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.JournalKind.centralized","title":"<code>centralized = 'centralized'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.PersistedEvent","title":"<code>casty.PersistedEvent</code>  <code>dataclass</code>","text":"<p>An event wrapped with journal metadata.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_nr</code> <code>int</code> <p>Monotonically increasing sequence number for the entity.</p> required <code>event</code> <code>E</code> <p>The domain event payload.</p> required <code>timestamp</code> <code>float</code> <p>Unix timestamp when the event was persisted.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import PersistedEvent\n&gt;&gt;&gt; evt = PersistedEvent(sequence_nr=1, event=\"deposited\", timestamp=1.0)\n&gt;&gt;&gt; evt.sequence_nr\n1\n</code></pre>"},{"location":"reference/journal/#casty.PersistedEvent.sequence_nr","title":"<code>sequence_nr</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.PersistedEvent.event","title":"<code>event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.PersistedEvent.timestamp","title":"<code>timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.PersistedEvent.__init__","title":"<code>__init__(sequence_nr, event, timestamp)</code>","text":""},{"location":"reference/journal/#casty.Snapshot","title":"<code>casty.Snapshot</code>  <code>dataclass</code>","text":"<p>A point-in-time snapshot of an entity's state.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_nr</code> <code>int</code> <p>The sequence number up to which this snapshot covers.</p> required <code>state</code> <code>S</code> <p>The serialized actor state at snapshot time.</p> required <code>timestamp</code> <code>float</code> <p>Unix timestamp when the snapshot was taken.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import Snapshot\n&gt;&gt;&gt; snap = Snapshot(sequence_nr=10, state={\"balance\": 100}, timestamp=2.0)\n&gt;&gt;&gt; snap.state\n{'balance': 100}\n</code></pre>"},{"location":"reference/journal/#casty.Snapshot.sequence_nr","title":"<code>sequence_nr</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.Snapshot.state","title":"<code>state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.Snapshot.timestamp","title":"<code>timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/journal/#casty.Snapshot.__init__","title":"<code>__init__(sequence_nr, state, timestamp)</code>","text":""},{"location":"reference/mailbox/","title":"Mailbox","text":""},{"location":"reference/mailbox/#casty.Mailbox","title":"<code>casty.Mailbox</code>","text":"<p>Async message queue for actor message delivery.</p> <p>Wraps an <code>asyncio.Queue</code> with configurable capacity and overflow handling. When no capacity is set the mailbox is unbounded.</p> <p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int | None</code> <p>Maximum number of messages. <code>None</code> for unbounded.</p> <code>None</code> <code>overflow</code> <code>MailboxOverflowStrategy</code> <p>Policy when the mailbox is full.</p> <code>drop_new</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import Mailbox, MailboxOverflowStrategy\n&gt;&gt;&gt; mb = Mailbox[str](capacity=10, overflow=MailboxOverflowStrategy.drop_new)\n</code></pre>"},{"location":"reference/mailbox/#casty.Mailbox.__init__","title":"<code>__init__(capacity=None, overflow=MailboxOverflowStrategy.drop_new)</code>","text":""},{"location":"reference/mailbox/#casty.Mailbox.put","title":"<code>put(msg)</code>","text":"<p>Enqueue a message, applying the overflow strategy if full.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>M</code> <p>The message to enqueue.</p> required <p>Raises:</p> Type Description <code>QueueFull</code> <p>When the overflow strategy is <code>reject</code> and the mailbox is at capacity.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mb = Mailbox[int](capacity=2)\n&gt;&gt;&gt; mb.put(1)\n&gt;&gt;&gt; mb.put(2)\n</code></pre>"},{"location":"reference/mailbox/#casty.Mailbox.put_async","title":"<code>put_async(msg)</code>  <code>async</code>","text":"<p>Enqueue a message, waiting if the mailbox is full.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>M</code> <p>The message to enqueue.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; mb = Mailbox[str](capacity=5)\n&gt;&gt;&gt; await mb.put_async(\"hi\")\n</code></pre>"},{"location":"reference/mailbox/#casty.Mailbox.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Dequeue the next message, waiting if the mailbox is empty.</p> <p>Returns:</p> Type Description <code>M</code> <p>The next message in the queue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mb = Mailbox[str]()\n&gt;&gt;&gt; mb.put(\"hello\")\n&gt;&gt;&gt; msg = await mb.get()\n</code></pre>"},{"location":"reference/mailbox/#casty.Mailbox.size","title":"<code>size()</code>","text":"<p>Return the number of messages currently in the mailbox.</p> <p>Returns:</p> Type Description <code>int</code> <p>Current queue depth.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mb = Mailbox[int]()\n&gt;&gt;&gt; mb.put(42)\n&gt;&gt;&gt; mb.size()\n1\n</code></pre>"},{"location":"reference/mailbox/#casty.Mailbox.empty","title":"<code>empty()</code>","text":"<p>Return whether the mailbox has no messages.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the mailbox is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mb = Mailbox[int]()\n&gt;&gt;&gt; mb.empty()\nTrue\n</code></pre>"},{"location":"reference/mailbox/#casty.MailboxOverflowStrategy","title":"<code>casty.MailboxOverflowStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Policy applied when a bounded mailbox is full.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import MailboxOverflowStrategy\n&gt;&gt;&gt; MailboxOverflowStrategy.drop_new\n&lt;MailboxOverflowStrategy.drop_new: 1&gt;\n</code></pre>"},{"location":"reference/mailbox/#casty.MailboxOverflowStrategy.drop_new","title":"<code>drop_new = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/mailbox/#casty.MailboxOverflowStrategy.drop_oldest","title":"<code>drop_oldest = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/mailbox/#casty.MailboxOverflowStrategy.reject","title":"<code>reject = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/","title":"Receptionist","text":""},{"location":"reference/receptionist/#casty.ServiceKey","title":"<code>casty.ServiceKey</code>  <code>dataclass</code>","text":"<p>Typed key identifying a service in the cluster registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable service name (e.g. <code>\"payment-service\"</code>).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; key: ServiceKey[int] = ServiceKey(\"counter\")\n&gt;&gt;&gt; key.name\n'counter'\n</code></pre>"},{"location":"reference/receptionist/#casty.ServiceKey.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.ServiceKey.__init__","title":"<code>__init__(name)</code>","text":""},{"location":"reference/receptionist/#casty.ServiceInstance","title":"<code>casty.ServiceInstance</code>  <code>dataclass</code>","text":"<p>A single instance of a service in the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>ActorRef[M]</code> <p>Reference to the actor providing the service.</p> required <code>node</code> <code>NodeAddress</code> <p>Node where the actor lives.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; instance = ServiceInstance(ref=some_ref, node=NodeAddress(\"127.0.0.1\", 2551))\n</code></pre>"},{"location":"reference/receptionist/#casty.ServiceInstance.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.ServiceInstance.node","title":"<code>node</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.ServiceInstance.__init__","title":"<code>__init__(ref, node)</code>","text":""},{"location":"reference/receptionist/#casty.Listing","title":"<code>casty.Listing</code>  <code>dataclass</code>","text":"<p>Current set of instances for a given service key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>The service key this listing is for.</p> required <code>instances</code> <code>frozenset[ServiceInstance[M]]</code> <p>All known instances of the service.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; listing = Listing(key=ServiceKey(\"counter\"), instances=frozenset())\n&gt;&gt;&gt; len(listing.instances)\n0\n</code></pre>"},{"location":"reference/receptionist/#casty.Listing.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Listing.instances","title":"<code>instances</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Listing.__init__","title":"<code>__init__(key, instances)</code>","text":""},{"location":"reference/receptionist/#casty.Register","title":"<code>casty.Register</code>  <code>dataclass</code>","text":"<p>Register a local actor as a service instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>Service key to register under.</p> required <code>ref</code> <code>ActorRef[M]</code> <p>Reference to the actor providing the service.</p> required"},{"location":"reference/receptionist/#casty.Register.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Register.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Register.__init__","title":"<code>__init__(key, ref)</code>","text":""},{"location":"reference/receptionist/#casty.Deregister","title":"<code>casty.Deregister</code>  <code>dataclass</code>","text":"<p>Remove a local actor from the service registry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>Service key to deregister from.</p> required <code>ref</code> <code>ActorRef[M]</code> <p>Reference to the actor to remove.</p> required"},{"location":"reference/receptionist/#casty.Deregister.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Deregister.ref","title":"<code>ref</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Deregister.__init__","title":"<code>__init__(key, ref)</code>","text":""},{"location":"reference/receptionist/#casty.Subscribe","title":"<code>casty.Subscribe</code>  <code>dataclass</code>","text":"<p>Subscribe to changes in a service key's instance set.</p> <p>The subscriber immediately receives the current <code>Listing</code> and is notified on every subsequent change.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>Service key to watch.</p> required <code>reply_to</code> <code>ActorRef[Listing[M]]</code> <p>Where to send <code>Listing</code> updates.</p> required"},{"location":"reference/receptionist/#casty.Subscribe.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Subscribe.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Subscribe.__init__","title":"<code>__init__(key, reply_to)</code>","text":""},{"location":"reference/receptionist/#casty.Find","title":"<code>casty.Find</code>  <code>dataclass</code>","text":"<p>One-shot query for the current instances of a service key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ServiceKey[M]</code> <p>Service key to look up.</p> required <code>reply_to</code> <code>ActorRef[Listing[M]]</code> <p>Where to send the current <code>Listing</code>.</p> required"},{"location":"reference/receptionist/#casty.Find.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Find.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/receptionist/#casty.Find.__init__","title":"<code>__init__(key, reply_to)</code>","text":""},{"location":"reference/ref/","title":"ActorRef","text":""},{"location":"reference/ref/#casty.ActorRef","title":"<code>casty.ActorRef</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Typed handle to an actor, used for fire-and-forget messaging.</p>"},{"location":"reference/ref/#casty.ActorRef.id","title":"<code>id</code>  <code>property</code>","text":""},{"location":"reference/ref/#casty.ActorRef.tell","title":"<code>tell(msg)</code>","text":""},{"location":"reference/ref/#casty.BroadcastRef","title":"<code>casty.BroadcastRef</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RemoteActorRef[M]</code></p>"},{"location":"reference/ref/#casty.BroadcastRef.__init__","title":"<code>__init__(address, _transport)</code>","text":""},{"location":"reference/ref/#casty.RemoteActorRef","title":"<code>casty.RemoteActorRef</code>  <code>dataclass</code>","text":"<p>Concrete ref for remote actors, carrying address + transport.</p>"},{"location":"reference/ref/#casty.RemoteActorRef.address","title":"<code>address</code>  <code>instance-attribute</code>","text":""},{"location":"reference/ref/#casty.RemoteActorRef.id","title":"<code>id</code>  <code>property</code>","text":""},{"location":"reference/ref/#casty.RemoteActorRef.tell","title":"<code>tell(msg)</code>","text":""},{"location":"reference/ref/#casty.RemoteActorRef.__reduce__","title":"<code>__reduce__()</code>","text":""},{"location":"reference/ref/#casty.RemoteActorRef.__init__","title":"<code>__init__(address, _transport)</code>","text":""},{"location":"reference/scheduler/","title":"Scheduler","text":""},{"location":"reference/scheduler/#casty.scheduler","title":"<code>casty.scheduler()</code>","text":"<p>Create a scheduler actor behavior.</p>"},{"location":"reference/scheduler/#casty.ScheduleTick","title":"<code>casty.ScheduleTick</code>  <code>dataclass</code>","text":"<p>Schedule a message to be sent repeatedly at a fixed interval.</p>"},{"location":"reference/scheduler/#casty.ScheduleTick.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleTick.target","title":"<code>target</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleTick.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleTick.interval","title":"<code>interval</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleTick.__init__","title":"<code>__init__(key, target, message, interval)</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleOnce","title":"<code>casty.ScheduleOnce</code>  <code>dataclass</code>","text":"<p>Schedule a message to be sent once after a delay.</p>"},{"location":"reference/scheduler/#casty.ScheduleOnce.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleOnce.target","title":"<code>target</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleOnce.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleOnce.delay","title":"<code>delay</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.ScheduleOnce.__init__","title":"<code>__init__(key, target, message, delay)</code>","text":""},{"location":"reference/scheduler/#casty.CancelSchedule","title":"<code>casty.CancelSchedule</code>  <code>dataclass</code>","text":"<p>Cancel a previously registered schedule by key.</p>"},{"location":"reference/scheduler/#casty.CancelSchedule.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/scheduler/#casty.CancelSchedule.__init__","title":"<code>__init__(key)</code>","text":""},{"location":"reference/scheduler/#casty.SchedulerMsg","title":"<code>casty.SchedulerMsg = ScheduleTick | ScheduleOnce | CancelSchedule</code>","text":""},{"location":"reference/serialization/","title":"Serialization","text":""},{"location":"reference/serialization/#casty.TypeRegistry","title":"<code>casty.TypeRegistry</code>","text":"<p>Bidirectional mapping between fully-qualified type names and Python classes.</p> <p>Supports both explicit registration and auto-resolution via <code>importlib</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass\n&gt;&gt;&gt; @dataclass(frozen=True)\n... class Ping:\n...     value: int\n&gt;&gt;&gt; registry = TypeRegistry()\n&gt;&gt;&gt; registry.register(Ping)\n&gt;&gt;&gt; registry.resolve(registry.type_name(Ping)) is Ping\nTrue\n</code></pre>"},{"location":"reference/serialization/#casty.TypeRegistry.__init__","title":"<code>__init__()</code>","text":""},{"location":"reference/serialization/#casty.TypeRegistry.register","title":"<code>register(cls)</code>","text":"<p>Register a type for serialization.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class to register.</p> required"},{"location":"reference/serialization/#casty.TypeRegistry.register_all","title":"<code>register_all(*types)</code>","text":"<p>Register multiple types at once.</p> <p>Parameters:</p> Name Type Description Default <code>*types</code> <code>type</code> <p>Classes to register.</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; registry = TypeRegistry()\n&gt;&gt;&gt; registry.register_all(int, str)\n&gt;&gt;&gt; registry.is_registered(int)\nTrue\n</code></pre>"},{"location":"reference/serialization/#casty.TypeRegistry.resolve","title":"<code>resolve(name)</code>","text":"<p>Resolve a fully-qualified type name to a Python class.</p> <p>Falls back to auto-resolution via <code>importlib</code> if the name is not explicitly registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Fully-qualified type name (e.g. <code>mymod.MyClass</code>).</p> required <p>Returns:</p> Type Description <code>type</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the type cannot be resolved.</p>"},{"location":"reference/serialization/#casty.TypeRegistry.type_name","title":"<code>type_name(cls)</code>","text":"<p>Return the fully-qualified name for a type, registering it if needed.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class to look up.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Fully-qualified name (e.g. <code>mymod.MyClass</code>).</p>"},{"location":"reference/serialization/#casty.TypeRegistry.is_registered","title":"<code>is_registered(cls)</code>","text":"<p>Return <code>True</code> if the type is already registered.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; registry = TypeRegistry()\n&gt;&gt;&gt; registry.is_registered(int)\nFalse\n&gt;&gt;&gt; registry.register(int)\n&gt;&gt;&gt; registry.is_registered(int)\nTrue\n</code></pre>"},{"location":"reference/serialization/#casty.JsonSerializer","title":"<code>casty.JsonSerializer</code>","text":"<p>JSON-based serializer with recursive handling of Casty types.</p> <p>Handles <code>ActorRef</code>, <code>frozenset</code>, <code>Enum</code>, <code>dict</code>, <code>tuple</code>, and frozen dataclasses registered in the <code>TypeRegistry</code>.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>TypeRegistry</code> <p>Registry for resolving type names.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; registry = TypeRegistry()\n&gt;&gt;&gt; ser = JsonSerializer(registry)\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.JsonSerializer.registry","title":"<code>registry</code>  <code>property</code>","text":"<p>Return the underlying type registry.</p>"},{"location":"reference/serialization/#casty.JsonSerializer.__init__","title":"<code>__init__(registry)</code>","text":""},{"location":"reference/serialization/#casty.JsonSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to JSON bytes.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>UTF-8 encoded JSON.</p>"},{"location":"reference/serialization/#casty.JsonSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Deserialize JSON bytes back to a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>UTF-8 encoded JSON from <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances from addresses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/serialization/#casty.JsonSerializer.to_dict","title":"<code>to_dict(value)</code>","text":""},{"location":"reference/serialization/#casty.JsonSerializer.from_dict","title":"<code>from_dict(value, *, ref_factory=None)</code>","text":""},{"location":"reference/serialization/#casty.PickleSerializer","title":"<code>casty.PickleSerializer</code>","text":"<p>Pickle-based serializer using protocol 5.</p> <p>Faster than JSON but not human-readable. Suitable for trusted environments where all nodes run the same code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ser = PickleSerializer()\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.PickleSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to pickle bytes (protocol 5).</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code>"},{"location":"reference/serialization/#casty.PickleSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Deserialize pickle bytes back to a Python object.</p> <p>Temporarily installs <code>ref_factory</code> as the module-level restore hook for the duration of <code>pickle.loads()</code>, then restores the previous value.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes produced by <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances from addresses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/serialization/#casty.CompressedSerializer","title":"<code>casty.CompressedSerializer</code>","text":"<p>Composable wrapper that adds compression to any <code>Serializer</code>.</p> <p>Applies compression on <code>serialize()</code> and decompression on <code>deserialize()</code>, delegating the actual serialization to the wrapped inner serializer.</p> <p>Parameters:</p> Name Type Description Default <code>inner</code> <code>Serializer</code> <p>The underlying serializer to wrap.</p> required <code>config</code> <code>CompressionConfig</code> <p>Compression algorithm and level settings.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty.config import CompressionConfig\n&gt;&gt;&gt; ser = CompressedSerializer(PickleSerializer(), CompressionConfig(level=9))\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.CompressedSerializer.__init__","title":"<code>__init__(inner, config)</code>","text":""},{"location":"reference/serialization/#casty.CompressedSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize and compress an object to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Compressed serialized bytes.</p>"},{"location":"reference/serialization/#casty.CompressedSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Decompress and deserialize bytes back to an object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Compressed bytes from <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/serialization/#casty.Serializer","title":"<code>casty.Serializer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for message serialization and deserialization.</p> <p>Implementations must handle <code>ActorRef</code> round-tripping via the <code>ref_factory</code> keyword argument passed to <code>deserialize()</code>.</p> <p>Examples:</p> <p>Minimal implementation:</p> <pre><code>&gt;&gt;&gt; class MySerializer:\n...     def serialize(self, obj: object) -&gt; bytes: ...\n...     def deserialize(self, data: bytes, *, ref_factory=None) -&gt; object: ...\n</code></pre>"},{"location":"reference/serialization/#casty.Serializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to bytes.</p>"},{"location":"reference/serialization/#casty.Serializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Deserialize bytes back to an object.</p>"},{"location":"reference/serialization/#casty.build_serializer","title":"<code>casty.build_serializer(config, *, registry=None)</code>","text":"<p>Build a <code>Serializer</code> chain from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SerializationConfig</code> <p>Serialization settings (serializer kind + optional compression).</p> required <code>registry</code> <code>TypeRegistry | None</code> <p>Type registry required when <code>config.serializer == \"json\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>PickleSerializer | JsonSerializer | CompressedSerializer</code> <p>Ready-to-use serializer, possibly wrapped with compression.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty.config import SerializationConfig, CompressionConfig\n&gt;&gt;&gt; config = SerializationConfig(compression=CompressionConfig(level=9))\n&gt;&gt;&gt; ser = build_serializer(config)\n</code></pre>"},{"location":"reference/serialization/#casty.MsgpackSerializer","title":"<code>casty.MsgpackSerializer</code>","text":"<p>MessagePack serializer with structured type handling.</p> <p>Reuses the same recursive dict-conversion as <code>JsonSerializer</code> but encodes to MessagePack binary format, giving compact output with fast encode/decode.</p> <p>Requires the optional <code>msgpack</code> package.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>TypeRegistry</code> <p>Registry for resolving type names.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; registry = TypeRegistry()\n&gt;&gt;&gt; ser = MsgpackSerializer(registry)\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.MsgpackSerializer.registry","title":"<code>registry</code>  <code>property</code>","text":"<p>Return the underlying type registry.</p>"},{"location":"reference/serialization/#casty.MsgpackSerializer.__init__","title":"<code>__init__(registry=None)</code>","text":""},{"location":"reference/serialization/#casty.MsgpackSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to MessagePack bytes.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code>"},{"location":"reference/serialization/#casty.MsgpackSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Deserialize MessagePack bytes back to a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances from addresses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/serialization/#casty.CloudPickleSerializer","title":"<code>casty.CloudPickleSerializer</code>","text":"<p>CloudPickle-based serializer for lambdas and closures.</p> <p>Drop-in replacement for <code>PickleSerializer</code> that handles lambdas, closures, and dynamically defined classes via <code>cloudpickle</code>. Deserialization uses standard <code>pickle.loads</code>.</p> <p>Requires the optional <code>cloudpickle</code> package.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ser = CloudPickleSerializer()\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.CloudPickleSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object using cloudpickle (protocol 5).</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code>"},{"location":"reference/serialization/#casty.CloudPickleSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Deserialize bytes back to a Python object.</p> <p>Uses standard <code>pickle.loads</code> since cloudpickle only customizes the dump side.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances from addresses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/serialization/#casty.Lz4CompressedSerializer","title":"<code>casty.Lz4CompressedSerializer</code>","text":"<p>Wraps any serializer with LZ4 frame compression.</p> <p>Applies LZ4 compression on <code>serialize()</code> and decompression on <code>deserialize()</code>, delegating actual serialization to the wrapped inner serializer.</p> <p>Requires the optional <code>lz4</code> package.</p> <p>Parameters:</p> Name Type Description Default <code>inner</code> <code>Serializer</code> <p>The underlying serializer to wrap.</p> required <code>level</code> <code>int</code> <p>LZ4 compression level (0 = default, higher = more compression).</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty.remote.serialization import PickleSerializer\n&gt;&gt;&gt; ser = Lz4CompressedSerializer(PickleSerializer())\n&gt;&gt;&gt; ser.deserialize(ser.serialize({\"key\": \"value\"}))\n{'key': 'value'}\n</code></pre>"},{"location":"reference/serialization/#casty.Lz4CompressedSerializer.__init__","title":"<code>__init__(inner, *, level=0)</code>","text":""},{"location":"reference/serialization/#casty.Lz4CompressedSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize and compress an object to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>M</code> <p>Object to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>LZ4-compressed serialized bytes.</p>"},{"location":"reference/serialization/#casty.Lz4CompressedSerializer.deserialize","title":"<code>deserialize(data, *, ref_factory=None)</code>","text":"<p>Decompress and deserialize bytes back to an object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>LZ4-compressed bytes from <code>serialize</code>.</p> required <code>ref_factory</code> <code>Callable or None</code> <p>Factory for reconstructing <code>ActorRef</code> instances.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code>"},{"location":"reference/sharding/","title":"Sharding","text":""},{"location":"reference/sharding/#casty.ReplicationConfig","title":"<code>casty.ReplicationConfig</code>  <code>dataclass</code>","text":"<p>Configuration for shard replication.</p> <p>Controls how many replica copies are maintained and how many acknowledgements are required before a write is considered committed.</p> <p>Parameters:</p> Name Type Description Default <code>replicas</code> <code>int</code> <p>Number of replica copies per shard (<code>0</code> disables replication).</p> <code>0</code> <code>min_acks</code> <code>int</code> <p>Minimum replica acknowledgements required before confirming a write.</p> <code>0</code> <code>ack_timeout</code> <code>float</code> <p>Seconds to wait for replica acknowledgements.</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ReplicationConfig(replicas=2, min_acks=1, ack_timeout=3.0)\nReplicationConfig(replicas=2, min_acks=1, ack_timeout=3.0)\n</code></pre>"},{"location":"reference/sharding/#casty.ReplicationConfig.replicas","title":"<code>replicas = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicationConfig.min_acks","title":"<code>min_acks = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicationConfig.ack_timeout","title":"<code>ack_timeout = 5.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicationConfig.__init__","title":"<code>__init__(replicas=0, min_acks=0, ack_timeout=5.0)</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEvents","title":"<code>casty.ReplicateEvents</code>  <code>dataclass</code>","text":"<p>Internal message: replicate persisted events to a replica node.</p> <p>Sent by the primary shard region to replica regions after persisting new events locally.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity whose events are being replicated.</p> required <code>shard_id</code> <code>int</code> <p>Shard that owns the entity.</p> required <code>events</code> <code>tuple[PersistedEvent[Any], ...]</code> <p>Events to replicate.</p> required <code>reply_to</code> <code>ActorRef[Any] | None</code> <p>Optional ref to acknowledge replication.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ReplicateEvents(\"user-1\", shard_id=7, events=(evt,))\nReplicateEvents(entity_id='user-1', shard_id=7, ...)\n</code></pre>"},{"location":"reference/sharding/#casty.ReplicateEvents.entity_id","title":"<code>entity_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEvents.shard_id","title":"<code>shard_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEvents.events","title":"<code>events</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEvents.reply_to","title":"<code>reply_to = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEvents.__init__","title":"<code>__init__(entity_id, shard_id, events, reply_to=None)</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEventsAck","title":"<code>casty.ReplicateEventsAck</code>  <code>dataclass</code>","text":"<p>Acknowledgement that a replica has persisted replicated events.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity whose events were replicated.</p> required <code>sequence_nr</code> <code>int</code> <p>Highest sequence number persisted by the replica.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ReplicateEventsAck(\"user-1\", sequence_nr=5)\nReplicateEventsAck(entity_id='user-1', sequence_nr=5)\n</code></pre>"},{"location":"reference/sharding/#casty.ReplicateEventsAck.entity_id","title":"<code>entity_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEventsAck.sequence_nr","title":"<code>sequence_nr</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ReplicateEventsAck.__init__","title":"<code>__init__(entity_id, sequence_nr)</code>","text":""},{"location":"reference/sharding/#casty.ClusteredActorSystem","title":"<code>casty.ClusteredActorSystem</code>","text":"<p>               Bases: <code>ActorSystem</code></p> <p>Actor system with cluster membership, sharding, and remote messaging.</p> <p>Extends <code>ActorSystem</code> to transparently distribute <code>ShardedBehavior</code> actors across cluster nodes.  Handles gossip-based membership, failure detection, shard coordination, and TCP transport.</p> <p>Use as an async context manager to start/stop the cluster lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logical system name (shared across cluster nodes).</p> required <code>host</code> <code>str</code> <p>Advertised hostname for this node.</p> required <code>port</code> <code>int</code> <p>Advertised port for this node (use <code>0</code> for auto-assignment).</p> required <code>seed_nodes</code> <code>tuple[tuple[str, int], ...] | None</code> <p>Initial contact points for cluster join.</p> <code>None</code> <code>roles</code> <code>frozenset[str]</code> <p>Roles assigned to this node (for role-aware shard placement).</p> <code>frozenset()</code> <code>bind_host</code> <code>str | None</code> <p>Network interface to bind to (defaults to host).</p> <code>None</code> <code>config</code> <code>CastyConfig | None</code> <p>Full configuration object.</p> <code>None</code> <code>tls</code> <code>Config | None</code> <p>TLS configuration for inter-node communication.  Use <code>TlsConfig.from_paths(...)</code> for file-based setup or pass pre-built <code>ssl.SSLContext</code> via <code>TlsConfig(server_context=..., client_context=...)</code>.</p> <code>None</code> <code>required_quorum</code> <code>int | None</code> <p>If set, <code>__aenter__</code> blocks until this many nodes are <code>up</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; async with ClusteredActorSystem(\n...     name=\"my-app\", host=\"127.0.0.1\", port=25520,\n...     seed_nodes=[(\"127.0.0.1\", 25520)],\n... ) as system:\n...     ref = system.spawn(Behaviors.sharded(my_entity, num_shards=50), \"things\")\n...     ref.tell(ShardEnvelope(\"abc\", DoSomething()))\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.receptionist","title":"<code>receptionist</code>  <code>property</code>","text":"<p>The cluster-wide receptionist for service discovery.</p>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.self_node","title":"<code>self_node</code>  <code>property</code>","text":"<p>The <code>NodeAddress</code> representing this cluster member.</p>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.__init__","title":"<code>__init__(*, name, host, port, node_id, seed_nodes=None, roles=frozenset(), bind_host=None, config=None, tls=None, required_quorum=None, serializer=None)</code>","text":""},{"location":"reference/sharding/#casty.ClusteredActorSystem.__make_ref__","title":"<code>__make_ref__(id, deliver)</code>","text":""},{"location":"reference/sharding/#casty.ClusteredActorSystem.from_config","title":"<code>from_config(config, *, host=None, port=None, node_id=None, seed_nodes=None, bind_host=None, tls=None, required_quorum=None)</code>  <code>classmethod</code>","text":"<p>Create a <code>ClusteredActorSystem</code> from a <code>CastyConfig</code>.</p> <p>Reads host, port, seed nodes, and roles from the <code>[cluster]</code> section of the config.  Keyword arguments override config values.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CastyConfig</code> <p>Parsed configuration (typically from <code>load_config</code>).</p> required <p>Returns:</p> Type Description <code>ClusteredActorSystem</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config has no <code>[cluster]</code> section.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"casty.toml\"))\n&gt;&gt;&gt; system = ClusteredActorSystem.from_config(config)\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":""},{"location":"reference/sharding/#casty.ClusteredActorSystem.spawn","title":"<code>spawn(behavior, name, *, mailbox=None)</code>","text":"<pre><code>spawn(\n    behavior: BroadcastedBehavior[M], name: str\n) -&gt; BroadcastRef[M]\n</code></pre><pre><code>spawn(\n    behavior: ShardedBehavior[M], name: str\n) -&gt; ActorRef[ShardEnvelope[M]]\n</code></pre><pre><code>spawn(\n    behavior: SingletonBehavior[M], name: str\n) -&gt; ActorRef[M]\n</code></pre><pre><code>spawn(\n    behavior: Behavior[M],\n    name: str,\n    *,\n    mailbox: Mailbox[M] | None = None,\n) -&gt; ActorRef[M]\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.ask","title":"<code>ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<pre><code>ask(\n    ref: BroadcastRef[M],\n    msg_factory: Callable[[ActorRef[R]], M],\n    *,\n    timeout: float,\n) -&gt; tuple[R, ...]\n</code></pre><pre><code>ask(\n    ref: ActorRef[M],\n    msg_factory: Callable[[ActorRef[R]], M],\n    *,\n    timeout: float,\n) -&gt; R\n</code></pre> <p>Ask with remote-addressable temp reply ref.</p> <p>When ref is a <code>BroadcastRef</code>, the message is fanned out to all cluster members and responses are collected into a <code>tuple[R, ...]</code>.</p>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.get_cluster_state","title":"<code>get_cluster_state(*, timeout=5.0)</code>  <code>async</code>","text":"<p>Query the current cluster membership state.</p> <p>Returns:</p> Type Description <code>ClusterState</code> <p>Snapshot of members, their statuses, and the vector clock.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; state = await system.get_cluster_state()\n&gt;&gt;&gt; len(state.members)\n3\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.wait_for","title":"<code>wait_for(n, *, timeout=60.0)</code>  <code>async</code>","text":"<p>Block until at least n members have status <code>up</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Minimum number of <code>up</code> members required.</p> required <code>timeout</code> <code>float</code> <p>Seconds to wait before raising <code>TimeoutError</code>.</p> <code>60.0</code> <p>Returns:</p> Type Description <code>ClusterState</code> <p>The cluster state once the quorum is reached.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; state = await system.wait_for(3, timeout=30.0)\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.barrier","title":"<code>barrier(name, n, *, timeout=60.0)</code>  <code>async</code>","text":"<p>Distributed barrier -- blocks until n nodes have reached this point.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Barrier name (shared across nodes).</p> required <code>n</code> <code>int</code> <p>Number of nodes that must arrive before all are released.</p> required <code>timeout</code> <code>float</code> <p>Seconds to wait before raising <code>TimeoutError</code>.</p> <code>60.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; await system.barrier(\"setup-done\", n=3)\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.lookup","title":"<code>lookup(path, *, node=None, timeout=5.0)</code>","text":"<pre><code>lookup(\n    path: ServiceKey[M], *, timeout: float = 5.0\n) -&gt; Coroutine[Any, Any, Listing[M]]\n</code></pre><pre><code>lookup(\n    path: str, *, node: NodeId | NodeAddress | None = None\n) -&gt; ActorRef[Any] | None\n</code></pre> <p>Look up an actor by path or find services by key.</p> <p>When path is a <code>str</code>, performs a path-based lookup (optionally on a remote node).  When it is a <code>ServiceKey</code>, queries the cluster receptionist and returns an awaitable <code>Listing</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | ServiceKey</code> <p>Actor path (e.g. <code>\"/my-actor\"</code>) or a typed service key.</p> required <code>node</code> <code>NodeId | NodeAddress | None</code> <p>Target node for path-based lookup.  <code>None</code> means local.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Seconds to wait for the receptionist (only for <code>ServiceKey</code>).</p> <code>5.0</code> <p>Returns:</p> Type Description <code>ActorRef[Any] | None | Coroutine[Any, Any, Listing]</code> <p>For path lookups, the actor reference or <code>None</code>. For service key lookups, an awaitable <code>Listing</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ref = system.lookup(\"/my-actor\")\n&gt;&gt;&gt; remote = system.lookup(\"/my-actor\", node=\"worker-1\")\n&gt;&gt;&gt; listing = await system.lookup(ServiceKey[PaymentMsg](\"payment\"))\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.region_ref","title":"<code>region_ref(key, factory, *, num_shards)</code>","text":"<p>Return a ref that routes <code>ShardEnvelope</code> to a sharded region.</p> <p>Spawns a new shard region if one with the given key does not already exist.</p>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.entity_ask","title":"<code>entity_ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Send a message to a sharded entity and wait for a reply.</p>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.distributed","title":"<code>distributed(*, journal=None)</code>","text":"<p>Create a <code>Distributed</code> facade for this system.</p> <p>Parameters:</p> Name Type Description Default <code>journal</code> <code>EventJournal | None</code> <p>If provided, data structures use event sourcing for persistence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Distributed</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = system.distributed()\n&gt;&gt;&gt; counter = d.counter(\"hits\")\n</code></pre>"},{"location":"reference/sharding/#casty.ClusteredActorSystem.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Shut down the cluster node, closing transport and all actors.</p>"},{"location":"reference/sharding/#casty.ShardEnvelope","title":"<code>casty.ShardEnvelope</code>  <code>dataclass</code>","text":"<p>Envelope that routes a message to a specific entity within a shard region.</p> <p>Wraps a message <code>M</code> together with an <code>entity_id</code> used for deterministic shard assignment.  The shard proxy computes the target shard from the <code>entity_id</code> and forwards the envelope to the owning node.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Logical identifier of the target entity.</p> required <code>message</code> <code>M</code> <p>The payload message delivered to the entity actor.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ref.tell(ShardEnvelope(\"user-42\", Deposit(amount=100)))\n</code></pre>"},{"location":"reference/sharding/#casty.ShardEnvelope.entity_id","title":"<code>entity_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ShardEnvelope.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/sharding/#casty.ShardEnvelope.__init__","title":"<code>__init__(entity_id, message)</code>","text":""},{"location":"reference/streams/","title":"Streams","text":""},{"location":"reference/streams/#casty.stream_producer","title":"<code>casty.stream_producer(*, buffer_size=0)</code>","text":"<p>Buffered, demand-gated element source.</p> <p>Parameters:</p> Name Type Description Default <code>buffer_size</code> <code>int</code> <p>Maximum number of buffered elements. <code>0</code> means unbounded (backward compatible with the original tuple-based buffer).</p> <code>0</code> <code>Story</code> required"},{"location":"reference/streams/#casty.stream_consumer","title":"<code>casty.stream_consumer(producer, *, timeout=30.0, initial_demand=16)</code>","text":"<p>Subscription mediator between a <code>stream_producer</code> and a <code>SourceRef</code>.</p> <p>Story: <code>waiting</code> \u2192 <code>active</code> \u2192 stopped.</p>"},{"location":"reference/streams/#casty.SinkRef","title":"<code>casty.SinkRef</code>","text":"<p>Input-side handle for pushing elements into a <code>stream_producer</code>.</p> <p>Obtained via <code>GetSink</code>. Backed by the producer's internal <code>asyncio.Queue</code> \u2014 <code>put</code> blocks when the buffer is full, providing input-side backpressure.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue[E]</code> <p>The producer's internal bounded queue.</p> required <code>producer</code> <code>ActorRef[StreamProducerMsg[E]]</code> <p>Ref to the producer actor, used to nudge it after each put.</p> required"},{"location":"reference/streams/#casty.SinkRef.__init__","title":"<code>__init__(queue, producer, closed)</code>","text":""},{"location":"reference/streams/#casty.SinkRef.put","title":"<code>put(element)</code>  <code>async</code>","text":"<p>Push an element, blocking if the buffer is full.</p> <p>Returns silently if the producer stops while waiting.</p>"},{"location":"reference/streams/#casty.SinkRef.complete","title":"<code>complete()</code>  <code>async</code>","text":"<p>Signal that no more elements will be pushed.</p>"},{"location":"reference/streams/#casty.SourceRef","title":"<code>casty.SourceRef</code>","text":"<p>Async iterator over a stream with automatic demand replenishment.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue[StreamElement[E] | StreamCompleted]</code> <p>Internal queue fed by the consumer actor's receive handler.</p> required <code>cancel</code> <code>Callable[[], None]</code> <p>Callback to send <code>StreamCancel</code> to the consumer actor.</p> required <code>timeout</code> <code>float</code> <p>Inactivity timeout in seconds before the stream ends.</p> required <code>request_demand</code> <code>Callable[[int], None]</code> <p>Callback to send <code>StreamDemand</code> to the consumer actor.</p> required"},{"location":"reference/streams/#casty.SourceRef.__init__","title":"<code>__init__(queue, cancel, timeout, request_demand)</code>","text":""},{"location":"reference/streams/#casty.SourceRef.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":""},{"location":"reference/streams/#casty.GetSink","title":"<code>casty.GetSink</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.GetSink.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.GetSink.__init__","title":"<code>__init__(reply_to)</code>","text":""},{"location":"reference/streams/#casty.GetSource","title":"<code>casty.GetSource</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.GetSource.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.GetSource.__init__","title":"<code>__init__(reply_to)</code>","text":""},{"location":"reference/streams/#casty.CompleteStream","title":"<code>casty.CompleteStream</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.CompleteStream.__init__","title":"<code>__init__()</code>","text":""},{"location":"reference/streams/#casty.StreamSubscribe","title":"<code>casty.StreamSubscribe</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.StreamSubscribe.consumer","title":"<code>consumer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.StreamSubscribe.demand","title":"<code>demand = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.StreamSubscribe.__init__","title":"<code>__init__(consumer, demand=0)</code>","text":""},{"location":"reference/streams/#casty.StreamDemand","title":"<code>casty.StreamDemand</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.StreamDemand.n","title":"<code>n</code>  <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.StreamDemand.__init__","title":"<code>__init__(n)</code>","text":""},{"location":"reference/streams/#casty.StreamCancel","title":"<code>casty.StreamCancel</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.StreamCancel.__init__","title":"<code>__init__()</code>","text":""},{"location":"reference/streams/#casty.StreamElement","title":"<code>casty.StreamElement</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.StreamElement.element","title":"<code>element</code>  <code>instance-attribute</code>","text":""},{"location":"reference/streams/#casty.StreamElement.__init__","title":"<code>__init__(element)</code>","text":""},{"location":"reference/streams/#casty.StreamCompleted","title":"<code>casty.StreamCompleted</code>  <code>dataclass</code>","text":""},{"location":"reference/streams/#casty.StreamCompleted.__init__","title":"<code>__init__()</code>","text":""},{"location":"reference/streams/#casty.StreamProducerMsg","title":"<code>casty.StreamProducerMsg = Push[E] | CompleteStream | Subscribe[E] | StreamDemand | StreamCancel | GetSink[E] | InputReady</code>","text":""},{"location":"reference/streams/#casty.StreamConsumerMsg","title":"<code>casty.StreamConsumerMsg = GetSource[E] | StreamDemand | StreamCancel | StreamElement[E] | StreamCompleted</code>","text":""},{"location":"reference/supervision/","title":"Supervision","text":""},{"location":"reference/supervision/#casty.SupervisionStrategy","title":"<code>casty.SupervisionStrategy</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for deciding how to handle actor failures.</p> <p>Implementations receive the exception raised by a child actor and return a <code>Directive</code> indicating the recovery action.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class AlwaysRestart:\n...     def decide(self, exception, *, child_id=\"\", **kw):\n...         return Directive.restart\n</code></pre>"},{"location":"reference/supervision/#casty.SupervisionStrategy.decide","title":"<code>decide(exception, *, child_id=..., **kwargs)</code>","text":""},{"location":"reference/supervision/#casty.OneForOneStrategy","title":"<code>casty.OneForOneStrategy</code>","text":"<p>               Bases: <code>SupervisionStrategy</code></p> <p>Supervision strategy that handles each child failure independently.</p> <p>Restarts a failing child up to <code>max_restarts</code> times within a sliding time window. If the limit is exceeded the child is stopped.</p> <p>Parameters:</p> Name Type Description Default <code>max_restarts</code> <code>int</code> <p>Maximum number of restarts allowed within the time window.</p> <code>3</code> <code>within</code> <code>float</code> <p>Length of the sliding time window in seconds.</p> <code>60.0</code> <code>decider</code> <code>Callable[[Exception], Directive] | None</code> <p>Optional function to override the directive for specific exceptions. If it returns <code>Directive.restart</code>, the rate-limit logic still applies.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import OneForOneStrategy, Directive\n&gt;&gt;&gt; strategy = OneForOneStrategy(max_restarts=5, within=30.0)\n</code></pre>"},{"location":"reference/supervision/#casty.OneForOneStrategy.__init__","title":"<code>__init__(max_restarts=3, within=60.0, decider=None)</code>","text":""},{"location":"reference/supervision/#casty.OneForOneStrategy.decide","title":"<code>decide(exception, *, child_id='__default__', **kwargs)</code>","text":"<p>Decide the recovery action for a failed child actor.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>Exception</code> <p>The exception that caused the child to fail.</p> required <code>child_id</code> <code>str</code> <p>Identifier for the child actor, used to track per-child restart frequency.</p> <code>'__default__'</code> <p>Returns:</p> Type Description <code>Directive</code> <p>The action to take: restart, stop, or escalate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; strategy = OneForOneStrategy(max_restarts=1, within=60.0)\n&gt;&gt;&gt; strategy.decide(ValueError(\"bad\"), child_id=\"a\")\n&lt;Directive.restart: 1&gt;\n</code></pre>"},{"location":"reference/supervision/#casty.Directive","title":"<code>casty.Directive</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Action to take when a supervised actor fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from casty import Directive\n&gt;&gt;&gt; Directive.restart\n&lt;Directive.restart: 1&gt;\n</code></pre>"},{"location":"reference/supervision/#casty.Directive.restart","title":"<code>restart = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/supervision/#casty.Directive.stop","title":"<code>stop = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/supervision/#casty.Directive.escalate","title":"<code>escalate = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/system/","title":"ActorSystem","text":""},{"location":"reference/system/#casty.ActorSystem","title":"<code>casty.ActorSystem</code>","text":"<p>Main entry point for creating and managing actors.</p> <p>Use as an async context manager for automatic shutdown.</p>"},{"location":"reference/system/#casty.ActorSystem.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"reference/system/#casty.ActorSystem.event_stream","title":"<code>event_stream</code>  <code>property</code>","text":""},{"location":"reference/system/#casty.ActorSystem.scheduler","title":"<code>scheduler</code>  <code>property</code>","text":""},{"location":"reference/system/#casty.ActorSystem.__init__","title":"<code>__init__(name='casty-system', *, config=None)</code>","text":""},{"location":"reference/system/#casty.ActorSystem.__make_ref__","title":"<code>__make_ref__(id, deliver)</code>","text":""},{"location":"reference/system/#casty.ActorSystem.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":""},{"location":"reference/system/#casty.ActorSystem.__aexit__","title":"<code>__aexit__(*exc)</code>  <code>async</code>","text":""},{"location":"reference/system/#casty.ActorSystem.spawn","title":"<code>spawn(behavior, name, *, mailbox=None)</code>","text":"<p>Spawn a root-level actor in this system.</p>"},{"location":"reference/system/#casty.ActorSystem.region_ref","title":"<code>region_ref(key, factory, *, num_shards)</code>","text":"<p>Not supported on plain ActorSystem \u2014 use ClusteredActorSystem.</p>"},{"location":"reference/system/#casty.ActorSystem.entity_ask","title":"<code>entity_ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Send a message and wait for a reply (EntityGateway protocol).</p>"},{"location":"reference/system/#casty.ActorSystem.ask","title":"<code>ask(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Send a message and wait for a reply (request-reply pattern).</p>"},{"location":"reference/system/#casty.ActorSystem.ask_or_none","title":"<code>ask_or_none(ref, msg_factory, *, timeout)</code>  <code>async</code>","text":"<p>Like <code>ask()</code>, but returns <code>None</code> on timeout instead of raising.</p>"},{"location":"reference/system/#casty.ActorSystem.lookup","title":"<code>lookup(path)</code>","text":"<p>Look up an actor by its path in the actor tree.</p>"},{"location":"reference/system/#casty.ActorSystem.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Shut down the actor system, stopping all root actors.</p>"},{"location":"reference/task-runner/","title":"Task Runner","text":""},{"location":"reference/task-runner/#casty.RunTask","title":"<code>casty.RunTask</code>  <code>dataclass</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.fn","title":"<code>fn</code>  <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.args","title":"<code>args = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.kwargs","title":"<code>kwargs = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.reply_to","title":"<code>reply_to = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.key","title":"<code>key = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.RunTask.__init__","title":"<code>__init__(fn, args=(), kwargs=(), reply_to=None, key='')</code>","text":""},{"location":"reference/task-runner/#casty.TaskCancelled","title":"<code>casty.TaskCancelled</code>  <code>dataclass</code>","text":""},{"location":"reference/task-runner/#casty.TaskCancelled.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.TaskCancelled.__init__","title":"<code>__init__(key)</code>","text":""},{"location":"reference/task-runner/#casty.TaskCompleted","title":"<code>casty.TaskCompleted</code>  <code>dataclass</code>","text":""},{"location":"reference/task-runner/#casty.TaskCompleted.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.TaskCompleted.__init__","title":"<code>__init__(key)</code>","text":""},{"location":"reference/task-runner/#casty.TaskFailed","title":"<code>casty.TaskFailed</code>  <code>dataclass</code>","text":""},{"location":"reference/task-runner/#casty.TaskFailed.key","title":"<code>key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.TaskFailed.exception","title":"<code>exception</code>  <code>instance-attribute</code>","text":""},{"location":"reference/task-runner/#casty.TaskFailed.__init__","title":"<code>__init__(key, exception)</code>","text":""},{"location":"reference/task-runner/#casty.TaskResult","title":"<code>casty.TaskResult = TaskCompleted | TaskFailed | TaskCancelled</code>","text":""},{"location":"reference/task-runner/#casty.TaskRunnerMsg","title":"<code>casty.TaskRunnerMsg = RunTask</code>","text":""},{"location":"reference/transport/","title":"Transport","text":""},{"location":"reference/transport/#casty.MessageTransport","title":"<code>casty.MessageTransport</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for delivering messages to actors by address.</p> <p>Any object with a <code>deliver(address, msg)</code> method satisfies this protocol.</p> <p>Examples:</p> <p>Minimal implementation:</p> <pre><code>&gt;&gt;&gt; class NullTransport:\n...     def deliver(self, address: ActorAddress, msg: object) -&gt; None:\n...         pass  # discard all messages\n</code></pre>"},{"location":"reference/transport/#casty.MessageTransport.deliver","title":"<code>deliver(address, msg)</code>","text":"<p>Deliver a message to the actor at the given address.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>ActorAddress</code> <p>Target actor address.</p> required <code>msg</code> <code>Any</code> <p>The message to deliver.</p> required"},{"location":"reference/transport/#casty.LocalTransport","title":"<code>casty.LocalTransport</code>","text":"<p>In-process message transport that routes by actor path.</p> <p>Messages to unregistered paths are buffered (up to <code>max_pending_per_path</code>) until a handler registers. Messages to paths that have been unregistered (dead actors) are dropped.</p> <p>Parameters:</p> Name Type Description Default <code>max_pending_per_path</code> <code>int</code> <p>Maximum buffered messages per path before dropping. Default is 64.</p> <code>64</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; transport = LocalTransport()\n&gt;&gt;&gt; received = []\n&gt;&gt;&gt; transport.register(\"/user/actor\", received.append)\n&gt;&gt;&gt; transport.deliver(ActorAddress(system=\"sys\", path=\"/user/actor\"), \"hello\")\n&gt;&gt;&gt; received\n['hello']\n</code></pre>"},{"location":"reference/transport/#casty.LocalTransport.__init__","title":"<code>__init__(*, max_pending_per_path=64)</code>","text":""},{"location":"reference/transport/#casty.LocalTransport.register_path_factory","title":"<code>register_path_factory(prefix, factory)</code>","text":"<p>Register a factory that lazily spawns handlers for paths with a given prefix.</p> <p>When <code>deliver()</code> encounters an unregistered path starting with prefix, it calls <code>factory(path)</code> which should register a handler for that path.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Path prefix to match (e.g. <code>\"/_coord-\"</code>).</p> required <code>factory</code> <code>Callable[[str], None]</code> <p>Called with the full path; must call <code>register()</code> for that path.</p> required"},{"location":"reference/transport/#casty.LocalTransport.register","title":"<code>register(path, handler)</code>","text":"<p>Register a message handler for the given path.</p> <p>Any messages buffered for this path are flushed to the handler immediately.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Actor path (e.g. <code>/user/counter</code>).</p> required <code>handler</code> <code>Callable[[Any], None]</code> <p>Callback invoked with each delivered message.</p> required"},{"location":"reference/transport/#casty.LocalTransport.unregister","title":"<code>unregister(path)</code>","text":"<p>Remove the handler for the given path and mark it dead.</p> <p>Future messages to this path are dropped rather than buffered.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Actor path to unregister.</p> required"},{"location":"reference/transport/#casty.LocalTransport.wait_for_path","title":"<code>wait_for_path(path)</code>  <code>async</code>","text":"<p>Wait until a handler is registered for the given path.</p>"},{"location":"reference/transport/#casty.LocalTransport.deliver","title":"<code>deliver(address, msg)</code>","text":"<p>Deliver a message to the handler registered for the address path.</p> <p>If no handler is registered and the path is not dead, the message is buffered. If the buffer is full, the message is dropped.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>ActorAddress</code> <p>Target actor address.</p> required <code>msg</code> <code>Any</code> <p>The message to deliver.</p> required"},{"location":"reference/transport/#casty.MessageEnvelope","title":"<code>casty.MessageEnvelope</code>  <code>dataclass</code>","text":"<p>Wire-format envelope for messages sent over TCP.</p> <p>Serializes to binary as <code>[header_len:4][header_json][payload]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>URI of the target actor.</p> required <code>sender</code> <code>str</code> <p>URI of the sending actor or system.</p> required <code>payload</code> <code>bytes</code> <p>Serialized message body.</p> required <code>type_hint</code> <code>str</code> <p>Fully-qualified type name of the message.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; env = MessageEnvelope(target=\"casty://sys/user/a\", sender=\"casty://sys/\",\n...                       payload=b'{\"x\":1}', type_hint=\"mymod.Msg\")\n&gt;&gt;&gt; data = env.to_bytes()\n&gt;&gt;&gt; MessageEnvelope.from_bytes(data).target\n'casty://sys/user/a'\n</code></pre>"},{"location":"reference/transport/#casty.MessageEnvelope.target","title":"<code>target</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.MessageEnvelope.sender","title":"<code>sender</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.MessageEnvelope.payload","title":"<code>payload</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.MessageEnvelope.type_hint","title":"<code>type_hint</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.MessageEnvelope.to_bytes","title":"<code>to_bytes()</code>","text":"<p>Serialize the envelope to binary wire format.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Binary data: <code>[header_len:4][header_json][payload]</code>.</p>"},{"location":"reference/transport/#casty.MessageEnvelope.from_bytes","title":"<code>from_bytes(data)</code>  <code>staticmethod</code>","text":"<p>Deserialize a <code>MessageEnvelope</code> from binary wire format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data produced by <code>to_bytes()</code>.</p> required <p>Returns:</p> Type Description <code>MessageEnvelope</code>"},{"location":"reference/transport/#casty.MessageEnvelope.__init__","title":"<code>__init__(target, sender, payload, type_hint)</code>","text":""},{"location":"reference/transport/#casty.RemoteTransport","title":"<code>casty.RemoteTransport</code>","text":"<p>Bridge between <code>MessageTransport</code> and the TCP transport actor.</p> <p>Routes local messages through <code>LocalTransport</code> and remote messages through TCP serialization via <code>tell(SendToNode(...))</code>.</p>"},{"location":"reference/transport/#casty.RemoteTransport.sender_host","title":"<code>sender_host</code>  <code>property</code>","text":"<p>Host used in outbound sender URIs (advertised or local).</p>"},{"location":"reference/transport/#casty.RemoteTransport.sender_port","title":"<code>sender_port</code>  <code>property</code>","text":"<p>Port used in outbound sender URIs (advertised or local).</p>"},{"location":"reference/transport/#casty.RemoteTransport.__init__","title":"<code>__init__(*, local, tcp, local_host, local_port, system_name, advertised_host=None, advertised_port=None, task_runner=None, on_send_failure=None, local_node_id=None)</code>","text":""},{"location":"reference/transport/#casty.RemoteTransport.update_node_index","title":"<code>update_node_index(index)</code>","text":"<p>Replace the node index used for <code>node_id</code> resolution.</p>"},{"location":"reference/transport/#casty.RemoteTransport.deliver","title":"<code>deliver(address, msg)</code>","text":"<p>Deliver a message to the given address.</p> <p>Local addresses are routed through <code>LocalTransport</code>; remote addresses are serialized and sent via TCP.</p>"},{"location":"reference/transport/#casty.RemoteTransport.make_ref","title":"<code>make_ref(address)</code>","text":"<p>Create an <code>ActorRef</code> with the appropriate transport for the address.</p>"},{"location":"reference/transport/#casty.RemoteTransport.clear_blacklist","title":"<code>clear_blacklist(host, port)</code>","text":"<p>Clear the TCP circuit breaker for a specific endpoint.</p>"},{"location":"reference/transport/#casty.TcpTransportConfig","title":"<code>casty.TcpTransportConfig</code>  <code>dataclass</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.self_address","title":"<code>self_address = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.client_only","title":"<code>client_only = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.server_ssl","title":"<code>server_ssl = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.client_ssl","title":"<code>client_ssl = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.connect_timeout","title":"<code>connect_timeout = 2.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.blacklist_duration","title":"<code>blacklist_duration = 5.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.address_map","title":"<code>address_map = field(default_factory=(lambda: _identity_address))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.TcpTransportConfig.__init__","title":"<code>__init__(host, port, self_address=None, client_only=False, server_ssl=None, client_ssl=None, connect_timeout=2.0, blacklist_duration=5.0, address_map=(lambda: _identity_address)())</code>","text":""},{"location":"reference/transport/#casty.TcpTransportMsg","title":"<code>casty.TcpTransportMsg = SendToNode | SendFrameToNode | InboundAccepted | PeerDown | ClearNodeBlacklist | GetPort | DeliverToNode | LogSerializationFailure</code>","text":""},{"location":"reference/transport/#casty.tcp_transport","title":"<code>casty.tcp_transport(config, handler, logger=None, serializer=None)</code>","text":"<p>Actor managing TCP connections to all peers.</p> <p>On setup, starts a TCP server (unless <code>client_only</code>).  The accept callback performs the handshake outside the actor, then tells itself <code>InboundAccepted</code>.  Outbound connections are established inside the receive handler on first <code>SendToNode</code> to a new peer.</p>"},{"location":"reference/transport/#casty.SendToNode","title":"<code>casty.SendToNode</code>  <code>dataclass</code>","text":""},{"location":"reference/transport/#casty.SendToNode.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.SendToNode.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.SendToNode.data","title":"<code>data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.SendToNode.__init__","title":"<code>__init__(host, port, data)</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode","title":"<code>casty.DeliverToNode</code>  <code>dataclass</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.msg","title":"<code>msg</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.target","title":"<code>target</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.sender","title":"<code>sender</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.DeliverToNode.__init__","title":"<code>__init__(host, port, msg, target, sender)</code>","text":""},{"location":"reference/transport/#casty.ClearNodeBlacklist","title":"<code>casty.ClearNodeBlacklist</code>  <code>dataclass</code>","text":""},{"location":"reference/transport/#casty.ClearNodeBlacklist.host","title":"<code>host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.ClearNodeBlacklist.port","title":"<code>port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.ClearNodeBlacklist.__init__","title":"<code>__init__(host, port)</code>","text":""},{"location":"reference/transport/#casty.GetPort","title":"<code>casty.GetPort</code>  <code>dataclass</code>","text":""},{"location":"reference/transport/#casty.GetPort.reply_to","title":"<code>reply_to</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.GetPort.__init__","title":"<code>__init__(reply_to)</code>","text":""},{"location":"reference/transport/#casty.InboundMessageHandler","title":"<code>casty.InboundMessageHandler</code>","text":"<p>Deserializes inbound TCP frames and delivers them locally.</p> <p>Satisfies <code>InboundHandler</code> structurally.  Passed directly to <code>tcp_transport</code> at spawn time \u2014 no placeholder or delegate wiring needed.</p>"},{"location":"reference/transport/#casty.InboundMessageHandler.ref_factory","title":"<code>ref_factory = ref_factory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transport/#casty.InboundMessageHandler.__init__","title":"<code>__init__(*, local, serializer, system_name, ref_factory=None)</code>","text":""},{"location":"reference/transport/#casty.InboundMessageHandler.on_message","title":"<code>on_message(data)</code>  <code>async</code>","text":""}]}